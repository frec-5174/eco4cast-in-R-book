[
  {
    "objectID": "visualizing.html#scoring-metric-rmse",
    "href": "visualizing.html#scoring-metric-rmse",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.1 Scoring Metric: RMSE",
    "text": "10.1 Scoring Metric: RMSE",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing and evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "visualizing.html#scoring-metric-continuous-ranked-probability-score",
    "href": "visualizing.html#scoring-metric-continuous-ranked-probability-score",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.2 Scoring Metric: Continuous Ranked Probability Score",
    "text": "10.2 Scoring Metric: Continuous Ranked Probability Score\nForecasts will be scored using the continuous ranked probability score (CRPS), a proper scoring rule for evaluating forecasts presented as distributions or ensembles (Gneiting & Raftery 2007). The CRPS compares the forecast probability distribution to that of the validation observation and assigns a score based on both the accuracy and precision of the forecast. We will use the ‘crps_sample’ function from the scoringRules package in R to calculate the CRPS for each forecast.\nWe will generate a combined score for all locations and forecast horizons. Forecasts will also be evaluated using the CRPS at each time-step in the forecast horizon and each location included in the forecasts.\nImportantly, we use the convention for CRPS where zero is lowest and best possible score, therefore teams want to achieve the lowest score. CPRS can be also expressed as a negative number with zero as highest and best possible score (Gneiting & Raftery 2007). The scoringRules package that we use follows the 0 or greater convention.\n\nlibrary(scoringRules)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n10.2.1 Example of a CRPS calculation from an ensemble forecast\nThe following uses Equation 2 in Jordan, Kruger, and Lerch 2018\n\n\n\nEquation 1 from Jordan, Kruger, and Lerch 2018.\n\n\nFirst, create a random sample from a probability distribution. This is the “forecast” for a particular point in time. For simplicity, we will use a normal distribution with a mean of 8 and standard deviation of 1\n\nx &lt;- rnorm(1000, mean = 8, sd = 1.0)\n\nSecond, we have our data point (i.e., the target). We will set it to zero as well\n\ny &lt;- 8\n\nNow calculate CRPS using Equation 2\n\ns &lt;- 0\nfor(i in 1:length(x)){\n  for(j in 1:length(x)){\n    s &lt;- s + abs(x[i] - x[j])\n  }\n}\ncrps_equation_2 &lt;- mean(abs(x - y)) - s / (2 * length(x)^2)\ncrps_equation_2\n\n[1] 0.2375868\n\n\nNow calculate using the crps_sample() function in the scoringRules package\n\ncrps_sample(y = y, dat = x)\n\n[1] 0.2375868\n\n\n\n\n10.2.2 Exploring the scoring surface\nNow lets see how the CRPS changes as the mean and standard deviation of the forecasted distribution change\nFirst, set vectors for the different mean and SD values we want to explore\n\nsample_mean &lt;- seq(4, 12, 0.1)\nsample_sd &lt;- seq(0.1, 10, 0.1)\n\nSecond, set our observed value to 8 for simplicity\n\ny &lt;- 8\n\nNow calculate the CRPS at each combination of forest mean and SD\n\ncombined &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd)))\nfor(i in 1:length(sample_mean)){\n  for(j in 1:length(sample_sd)){\n    sample &lt;- rnorm(10000, sample_mean[i], sample_sd[j])\n    combined[i, j] &lt;- crps_sample(y = y, dat = sample)\n  }\n}\n\nFinally, visualize the scoring surface with the observed value represented by the red line\n\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined),nlevels = 20, xlab = \"Mean\", ylab = \"SD\")\nabline(v = y, col = \"red\")\n\n\n\n\nThe contour surface highlights the trade-off between the mean and standard deviation.\n\n\n10.2.3 CRPS from the Normal Distribution\nIf the distributional forecast is a normal distribution represented by a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), an ensemble of predictions is not needed to evaluate CRPS because we can take advantage of the analytic solution to CRPS under the normal assumption (Equation 4 from Gneiting et al. 2005)\nEquation 5 from Gneiting et al. (2005) gives\n\\[\\begin{align*}\nCRPS(N(\\mu, \\sigma^2) | y) = \\sigma \\left( \\frac{y - \\mu}{\\sigma} \\left( 2 \\Phi\\left(  \\frac{y - \\mu}{\\sigma} \\right) - 1 \\right)  + 2 \\phi \\left(  \\frac{y - \\mu}{\\sigma} \\right) - \\frac{1}{\\sqrt{\\pi}} \\right)\n\\end{align*}\\]\nfor \\(\\Phi(\\cdot)\\) and \\(\\phi(\\cdot)\\) the standard normal CDF and PDF, respectively. Therefore, if the forecast distribution is truly a normal distribution (often this isn’t true in forecasts that only report a mean and sd) a simplified score can be applied as follows:\n\nsample_mean &lt;- seq(4, 12, 0.1)\nsample_sd &lt;- seq(0.1, 10, 0.1)\n\ncombined_norm &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd)))\nfor(i in 1:length(sample_mean)){\n  for(j in 1:length(sample_sd)){\n    combined_norm[i, j] &lt;- crps_norm(y = y, mean = sample_mean[i], sd = sample_sd[j])\n  }\n}\n\nFinally, visualize the scoring surface with the observed value represented by the red line.\n\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\")\nabline(v = y, col = \"red\")\n\n\n\n\nNote that at a given value of the sd, the lowest score is achieved at \\(\\mu = y\\) as shown for each of the blue lines where the minimum value of the score across each blue line is at the red line. This behavior makes sense because the CRPS is a score that rewards accuracy and precision. Thus, for any given level of precision (represented by the standard deviation), CRPS is optimized by producing the most accurate prediction of the distribution’s location.\n\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\")\nabline(v = y, col = \"red\")\nabline(h = 2.5, col = \"blue\")\nabline(h = 4.3, col = \"blue\")\nabline(h = 6.8, col = \"blue\")\n\n\n\n\nInterestingly, for a given mean \\(\\mu \\neq y\\) we find a pattern that makes intuitive sense given the goal of CRPS to produce forecasts that are both accurate and precise. For a given amount of bias in the prediction (i.e., given a \\(\\mu \\neq y\\)), the optimal score is achieved by a standard deviation that slightly larger than the bias.\n\nlayout(matrix(1:4, 2, 2, byrow = TRUE))\n## plots for mu = 7\nmu &lt;- 7\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\", main = paste0(\"CRPS contour given mu = \", mu))\nabline(v = mu, col = \"red\")\nmin_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))]\nabline(h = min_sd, col = \"blue\")\nplot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = 'l', main = paste0(\"CRPS profile given mu = \", mu))\nabline(v = min_sd, col = \"blue\")\n## plots for mu = 11\nmu &lt;- 11\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\", main = paste0(\"CRPS contour given mu = \", mu))\nabline(v = mu, col = \"red\")\nmin_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))]\nabline(h = min_sd, col = \"blue\")\nplot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = 'l', main = paste0(\"CRPS profile given mu = \", mu))\nabline(v = min_sd, col = \"blue\")\n\n\n\n\nNext, we plot the relationship between a given value of \\(\\mu\\) and the \\(\\sigma\\) that produces the optimal CRPS. This looks like a linear relationship.\n\noptimal_sd &lt;- rep(0, length(sample_mean))\nfor (i in 1:length(sample_mean)) {\n  optimal_sd[i] &lt;- sample_sd[which.min(crps_norm(y, mean = sample_mean[i], sd = sample_sd))]\n}\nplot(sample_mean, optimal_sd, type = 'l')\n\n\n\n\nLet’s estimate the slope of the relationship. It looks like the optimal \\(sd\\) for a normal distribution forecast that is biased by \\(|y - \\mu|\\) is \\(sd = 1.2|y - \\mu|\\) which makes sense as this would put the true value in a region of high probability.\n\ncoef(lm(optimal_sd[sample_mean &gt; 0] ~ sample_mean[sample_mean &gt; 0]))\n\n                 (Intercept) sample_mean[sample_mean &gt; 0] \n                2.430864e+00                -1.688326e-16",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing and evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "visualizing.html#appendix-crps-tutorial",
    "href": "visualizing.html#appendix-crps-tutorial",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.3 Appendix: CRPS tutorial",
    "text": "10.3 Appendix: CRPS tutorial\nThis provide a tutorial for the Continuous Ranked Probability Score (CRPS), a proper scoring rule used to evaluate probabilistic forecasts.\nAt its core, CRPS presents the difference between a cumulative probability distribution generated from the forecast and the data.\n\n10.3.1 Set up data\nWe will start with the calculation of CRPS for a single forecast and observation pair.\n\nobs &lt;- 0\n\nOur forecast is a sample (n = 100) from a normal distribution with mean of 0 and standard deviation of 1\n\nforecast &lt;- rnorm(100, mean = 0, sd = 1)\n\n\ndy &lt;- 0.5\ny &lt;- seq(-3,3, dy)\n\n\nforecast_cdf_function &lt;- ecdf(forecast)\nforecast_cdf &lt;- forecast_cdf_function(y)\nobs_cdf &lt;- as.numeric((y &gt; obs[i]))\n\n\ndf &lt;- tibble(y = y,\n           cdf = forecast_cdf,\n           variable = \"forecast\")\n\ndf &lt;- bind_rows(df, \n                tibble(y = y,\n           cdf = obs_cdf,\n           variable = \"observation\"))\n\n\nggplot(df, aes(x = y, y = cdf, color = variable)) + geom_line()\n\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nerror &lt;- (forecast_cdf - obs_cdf)^2\n\n\ntibble(y = y,\n       error = error) |&gt; \n  ggplot(aes(x = y, y = error)) + geom_line()\n\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncdf_diff_area &lt;- error * dy\n\n\ntibble(y = y,\n       area = cdf_diff_area) |&gt; \n  ggplot(aes(x = y, y = area)) + geom_line()\n\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncrps &lt;- sum(cdf_diff_area)\n\n\n\n10.3.2 Comparing forecasts\n\ncalculate_crps &lt;- function(obs, forecast, dy = 0.01) {\n  \n  y &lt;- seq(-10,10, dy)\n  forecast_cdf &lt;- ecdf(forecast)\n  cdf_diff &lt;- (forecast_cdf(y) - as.numeric((y &gt; obs)))^2 * dy\n  crps &lt;- sum(cdf_diff)\n  \n  return(crps)\n}\n\n\nforecast1 &lt;- rnorm(100, mean = 0, sd = 4)\nforecast2 &lt;- rnorm(100, mean = 1, sd = 2)\nforecast3&lt;- rnorm(100, mean = 0, sd = 0.1)\nforecast4&lt;- rnorm(100, mean = 1, sd = 0.1)\n\n\ndy &lt;- 0.001\ny &lt;- seq(-10,10, dy)\n\nforecast1_cdf_function &lt;- ecdf(forecast1)\nforecast2_cdf_function &lt;- ecdf(forecast2)\nforecast3_cdf_function &lt;- ecdf(forecast3)\nforecast4_cdf_function &lt;- ecdf(forecast4)\n\ndf &lt;- tibble(y1 = c(y,y,y,y),\n             cdf = c(forecast1_cdf_function(y),\n                     forecast2_cdf_function(y),\n                     forecast3_cdf_function(y),\n                     forecast4_cdf_function(y)),\n             variable = c(rep(paste0(\"mean = 0, sd = 4, CRPS: \",calculate_crps(obs, forecast1)), length(y)),\n                          rep(paste0(\"mean = 1, sd = 1 CRPS: \",calculate_crps(obs, forecast2)), length(y)), \n                          rep(paste0(\"mean = 0, sd = 0.1 CRPS: \",calculate_crps(obs, forecast3)), length(y)),\n                          rep(paste0(\"mean = 1, sd = 0.1 CRPS: \",calculate_crps(obs, forecast4)), length(y))))\n\ndf &lt;- bind_rows(df, \n                tibble(y1 = y,\n           cdf = as.numeric((y &gt; obs)),\n           variable = \"observation\"))\n\n\nggplot(df, aes(x = y1, y = cdf, color = variable)) + geom_line()\n\n\n\n\n\nc(scoringRules::crps_sample(y = obs, dat = forecast1),scoringRules::logs_sample(y = obs, dat = forecast1)) \n\n[1] 0.8815944 2.3501224\n\nc(scoringRules::crps_sample(y = obs, dat = forecast2),scoringRules::logs_sample(y = obs, dat = forecast2)) \n\n[1] 0.707551 1.791146\n\nc(scoringRules::crps_sample(y = obs, dat = forecast3),scoringRules::logs_sample(y = obs, dat = forecast3)) \n\n[1]  0.0248817 -1.2621717\n\nc(scoringRules::crps_sample(y = obs, dat = forecast4),scoringRules::logs_sample(y = obs, dat = forecast4)) \n\n[1]   0.9533151 314.2223698\n\n\n\ncalculate_crps &lt;- function(obs, forecast, dy) {\n  n &lt;- nrow(forecast)\n  crps &lt;- rep(NA, n)\n  y &lt;- seq(-5,5, dy)\n  \n  for (i in 1:n) {\n    forecast_cdf &lt;- ecdf(forecast[i, ])\n    cdf_diff &lt;- (forecast_cdf(y) - as.numeric((y &gt; obs[i])))^2 * dy\n    crps[i] &lt;- sum(cdf_diff)\n  }\n  \n  return(crps)\n}",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing and evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "intro-ecoforecast.html",
    "href": "intro-ecoforecast.html",
    "title": "3  Introduction to Ecological Forecasting",
    "section": "",
    "text": "This module was developed by Moore, T.N., Lofton, M.E., C.C. Carey, and R.Q. Thomas. 03 July 2023. Macrosystems EDDIE: Introduction to Ecological Forecasting. Macrosystems EDDIE Module 5, Version 2. http://module5.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nhttps://serc.carleton.edu/eddie/teaching_materials/modules/module5.html\nEcological forecasting is a tool that can be used for understanding and predicting changes in populations, communities, and ecosystems. Ecological forecasting is an emerging approach which provides an estimate of the future state of an ecological system with uncertainty, allowing society to prepare for changes in important ecosystem services. Ecological forecasters develop and update forecasts using the iterative forecasting cycle, in which they make a hypothesis of how an ecological system works; embed their hypothesis in a model; and use the model to make a forecast of future conditions. When observations become available, they can assess the accuracy of their forecast, which indicates if their hypothesis is supported or needs to be updated before the next forecast is generated. In this module, students will apply the iterative forecasting cycle to develop an ecological forecast for a National Ecological Observation Network (NEON) site. Students will use NEON data to build an ecological model that predicts primary productivity. Using their calibrated model, they will learn about the different components of a forecast with uncertainty and compare productivity forecasts among NEON sites. The overarching goal of this module is for students to learn fundamental concepts about ecological forecasting and build a forecast for a NEON site. Students will work with an R Shiny interface to visualize data, build a model, generate a forecast with uncertainty, and then compare the forecast with observations. The A-B-C structure of this module makes it flexible and adaptable to a range of student levels and course structures.\nReference\nMoore, T.N., R.Q. Thomas, W.M. Woelmer, C.C Carey. 2022. Integrating ecological forecasting into undergraduate ecology curricula with an R Shiny application-based teaching module. Forecasting 4:604-633. https://doi.org/10.3390/forecast4030033",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Ecological Forecasting</span>"
    ]
  },
  {
    "objectID": "first-forecast.html",
    "href": "first-forecast.html",
    "title": "4  First forecast: Introduction to NEON Ecological Forecasting Challenge",
    "section": "",
    "text": "This module was developed by Olsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas\nhttps://github.com/OlssonF/NEON-forecast-challenge-workshop\nThis tutorial introduces participants to key concepts in ecological forecasting and provides hands-on materials for submitting forecasts to the Ecological Forecasting Initiative (EFI) - National Ecological Observatory Network (NEON) Forecasting Challenge (hereafter, Challenge). The tutorial has been developed and used with more than 300 participants in both classrooms and workshops and provides the ecological understanding, workflows, and tools to enable ecologists with minimal forecasting experience to participate in the Challenge via a hands-on code-based tutorial. This R-based tutorial introduces participants to a near-term, iterative forecasting workflow that includes obtaining observations from NEON, developing a simple forecasting model, generating a forecast, and submitting the forecast to the Challenge, as well as analyzing forecast performance once new observations become available for evaluation. The overarching aim of this tutorial is to lower the barrier to ecological forecasting and empower participants to continue to innovate and develop their own ecological forecasts for the Challenge into the future.\nReference\nOlsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas. Can you predict the future? A tutorial for the National Ecological Observatory Network Ecological Forecasting Challenge. In review at Journal of Open Source Education.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First forecast: Introduction to NEON Ecological Forecasting Challenge</span>"
    ]
  },
  {
    "objectID": "understand-uncertainty.html",
    "href": "understand-uncertainty.html",
    "title": "5  Understanding Uncertainty in Ecological Forecasts",
    "section": "",
    "text": "https://serc.carleton.edu/eddie/teaching_materials/modules/module6.html\nEcological forecasting is a tool that can be used for understanding and predicting changes in populations, communities, and ecosystems. Ecological forecasting is an emerging approach which provides an estimate of the future state of an ecological system with uncertainty, allowing society to prepare for changes in important ecosystem services. Forecast uncertainty is derived from multiple sources, including model parameters and driver data, among others. Knowing the uncertainty associated with a forecast enables forecast users to evaluate the forecast and make more informed decisions. Ecological forecasters develop and update forecasts using the iterative forecasting cycle, in which they make a hypothesis of how an ecological system works; embed their hypothesis in a model; and use the model to make a forecast of future conditions and quantify forecast uncertainty. There are a number of approaches that forecasters can use to reduce uncertainty, which will be explored in this module.\nThis module was developed by Moore, T. N., Lofton, M.E., Carey, C.C. and Thomas, R. Q. 24 July 2023. Macrosystems EDDIE: Understanding Uncertainty in Ecological Forecasts. Macrosystems EDDIE Module 6, Version 2. http://module6.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Understanding Uncertainty in Ecological Forecasts</span>"
    ]
  },
  {
    "objectID": "second-forecast.html",
    "href": "second-forecast.html",
    "title": "6  Second forecast: Adding uncertainty to first forecast",
    "section": "",
    "text": "This module builds on https://github.com/OlssonF/NEON-forecast-challenge-workshop",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Second forecast: Adding uncertainty to first forecast</span>"
    ]
  },
  {
    "objectID": "using-data.html",
    "href": "using-data.html",
    "title": "7  Using data to improve ecological forecasts",
    "section": "",
    "text": "This module was initially developed by: Lofton, M.E., T.N. Moore, Thomas, R.Q., and C.C. Carey. 20 September 2022. Macrosystems EDDIE: Using Data to Improve Ecological Forecasts. Macrosystems EDDIE Module 7, Version 1. https://macrosystemseddie.shinyapps.io/module7. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nhttps://serc.carleton.edu/eddie/teaching_materials/modules/module7.html\nTo be useful for management, ecological forecasts need to be both accurate enough for managers to be able to rely on them for decision-making andinclude a representation of forecast uncertainty, so managers can properly interpret the probability of future events. To improve forecast accuracy, we can update forecasts with observational data once they become available, a process known asdata assimilation. Recent improvements in environmental sensor technology and an increase in the number of sensors deployed in ecosystems have resulted in an increase in the availability of data for assimilation to help develop and improve forecasts for natural resource management. In this module, students will develop an ecosystem model of primary productivity, use the model to generate forecasts, and then explore how assimilating different types of data at different temporal frequencies (e.g., daily, weekly) affects forecast accuracy. Finally, students will assimilate different types of data into forecasts and examine how data assimilation affects water resource management decisions.\nPart of the “On the Cutting Edge Exemplary Teaching Activities” collection",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Using data to improve ecological forecasts</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#installing-docker",
    "href": "third-forecast.html#installing-docker",
    "title": "8  Third forecast: automatically updating second forecast",
    "section": "8.1 Installing Docker",
    "text": "8.1 Installing Docker\nGo to https://docs.docker.com/get-docker/ to install the relevant install for your platform (available for PC, Mac and Linux). Also see https://docs.docker.com/desktop/.\nNOTE: * If you’re running Windows, you will need WSL (Windows Subsystem for Linux) * If you’re running a Linux distribution, you may have to enable Viritualization on your computer (see here)",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Third forecast: automatically updating second forecast</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#running-a-docker-container",
    "href": "third-forecast.html#running-a-docker-container",
    "title": "8  Third forecast: automatically updating second forecast",
    "section": "8.2 Running a docker container",
    "text": "8.2 Running a docker container\n\nLaunch Docker Desktop (either from the Command Line or by starting the GUI)\nAt the command line run the following command which tells docker to run the container with the name eco4cast/rocker-neon4cast that has all the packages and libraries installed already. The PASSWORD=yourpassword sets a simple password that you will use to open the container. The -ti option starts both a terminal and an interactive session.\n\ndocker run --rm -ti -e PASSWORD=yourpassword -p 8787:8787 eco4cast/rocker-neon4cast\nThis can take a few minutes to download and install. It will be quicker the next time you launch it.\n\nOpen up a web browser and navigate to http://localhost:8787/\nEnter the username: rstudio and password: yourpassword\nYou should see a R Studio interface with all the packages etc. pre-installed and ready to go.\n\nYou can close this localhost window (and then come back to it) but if you close the container from Docker (turn off your computer etc.) any changes will be lost unless you push them to Github or exported to your local environment.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Third forecast: automatically updating second forecast</span>"
    ]
  },
  {
    "objectID": "decision-making.html",
    "href": "decision-making.html",
    "title": "9  Using Ecological Foreccasts to Guide Decision Making",
    "section": "",
    "text": "This module was developed by W.M. Woelmer, R.Q. Thomas, T.N. Moore and C.C. Carey. 21 January 2021. Macrosystems EDDIE: Using Ecological Forecasts to Guide Decision-Making. Macrosystems EDDIE Module 8, Version 1. http://module8.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nhttps://serc.carleton.edu/eddie/teaching_materials/modules/module8.html\nBecause of increased variability in populations, communities, and ecosystems due to land use and climate change, there is a pressing need to know the future state of ecological systems across space and time. Ecological forecasting is an emerging approach which provides an estimate of the future state of an ecological system with uncertainty, allowing society to preemptively prepare for fluctuations in important ecosystem services. However, forecasts must be effectively designed and communicated to those who need them to realize their potential for protecting natural resources. In this module, students will explore real ecological forecast visualizations, identify ways to represent uncertainty, make management decisions using forecast visualizations and learn decision support techniques. Lastly, students will then customize a forecast visualization for a specific forecast user’s decision needs. The overarching goal of this module is for students to understand how forecasts are connected to decision-making of forecast users, or the managers, policy-makers, and other members of society who use forecasts to inform decision-making.\nPart of the “On the Cutting Edge Exemplary Teaching Activities” collection\nReference\nWoelmer, W.M., T.N. Moore, M.E. Lofton, R.Q. Thomas, and C.C. Carey. 2023. Embedding communication concepts in forecasting training increases students’ understanding of ecological uncertainty Ecosphere 14: e4628 https://doi.org/10.1002/ecs2.4628",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Ecological Foreccasts to Guide Decision Making</span>"
    ]
  },
  {
    "objectID": "project1.html#intro-to-tidymodels",
    "href": "project1.html#intro-to-tidymodels",
    "title": "Project 1: Create model and submit",
    "section": "Intro to Tidymodels",
    "text": "Intro to Tidymodels",
    "crumbs": [
      "Project 1: Create model and submit"
    ]
  },
  {
    "objectID": "project1.html#intro-to-fable-models",
    "href": "project1.html#intro-to-fable-models",
    "title": "Project 1: Create model and submit",
    "section": "Intro to Fable Models",
    "text": "Intro to Fable Models",
    "crumbs": [
      "Project 1: Create model and submit"
    ]
  },
  {
    "objectID": "modeling-approaches.html#intro-to-tidymodels",
    "href": "modeling-approaches.html#intro-to-tidymodels",
    "title": "11  Modeling approaches",
    "section": "11.1 Intro to Tidymodels",
    "text": "11.1 Intro to Tidymodels",
    "crumbs": [
      "Project 1: Create model and submit",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling approaches</span>"
    ]
  },
  {
    "objectID": "modeling-approaches.html#intro-to-fable-models",
    "href": "modeling-approaches.html#intro-to-fable-models",
    "title": "11  Modeling approaches",
    "section": "11.2 Intro to Fable Models",
    "text": "11.2 Intro to Fable Models",
    "crumbs": [
      "Project 1: Create model and submit",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling approaches</span>"
    ]
  },
  {
    "objectID": "advanced-forecasting.html",
    "href": "advanced-forecasting.html",
    "title": "Advanced forecasting",
    "section": "",
    "text": "introduction to the ecosystem and data\nterrestrial flux data\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nurl &lt;- \"https://sdsc.osn.xsede.org/bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=P1D/terrestrial_daily-targets.csv.gz\"\nflux &lt;- read_csv(url, show_col_types = FALSE) |&gt; \n  filter(site_id == \"ORNL\", \n         variable == \"nee\")\n\nflux |&gt; \n  #filter(datetime &gt; as_date(\"2023-01-01\"),\n  #       datetime &lt; as_date(\"2023-06-05\")) |&gt; \nggplot(aes(x = datetime, y = observation)) + geom_point()",
    "crumbs": [
      "Advanced forecasting"
    ]
  },
  {
    "objectID": "process-model.html",
    "href": "process-model.html",
    "title": "13  Process model",
    "section": "",
    "text": "SSEM.orig &lt;- function(X, params, inputs, timestep = 86400){\n\n  ne &lt;- nrow(X)  ## ne = number of ensemble members\n\n  ##Unit Converstion: umol/m2/sec to Mg/ha/timestep\n  k &lt;- 1e-6 * 12 * 1e-6 * 10000 * timestep #mol/umol*gC/mol*Mg/g*m2/ha*sec/timestep\n\n  ## photosynthesis\n  LAI &lt;- X[, 1] * params$SLA * 0.1  #0.1 is conversion from Mg/ha to kg/m2\n  GPP &lt;- pmax(0, params$alpha * (1 - exp(-0.5 * LAI)) * inputs$PAR)\n  GPP[inputs$PAR &lt; 1e-20] = 0 ## night\n\n  ## respiration & allocation\n  NPP &lt;- GPP * (1 - params$Ra_frac)\n\n  leaf_alloc &lt;- NPP * params$leaf_frac\n  wood_alloc &lt;- NPP * (1 - params$leaf_frac)\n\n  Rh &lt;- pmax(params$Rbasal * X[, 3] * params$Q10 ^ (inputs$temp / 10), 0) ## pmax ensures SOM never goes negative\n\n  ## turnover\n  litterfall &lt;- X[, 1] * params$litterfall\n  mortality &lt;- X[, 2] * params$mortality\n\n  ## update states\n  leaves &lt;- pmax(rnorm(ne, X[, 1] + leaf_alloc * k - litterfall, params$sigma.leaf), 0)\n  wood &lt;- pmax(rnorm(ne, X[, 2] + wood_alloc * k - mortality, params$sigma.stem), 0)\n  SOM &lt;- pmax(rnorm(ne, X[, 3] + litterfall + mortality - Rh * k, params$sigma.soil), 0)\n\n  return(cbind(X1 = leaves, \n               X2 = wood, \n               X3 = SOM,\n               LAI = leaves * params$SLA * 0.1,\n               GPP = GPP,\n               NEP = NPP - Rh,\n               Ra = GPP*params$Ra_frac,\n               NPPw = wood_alloc,\n               NPPl = leaf_alloc,\n               Rh = Rh,\n               litterfall = litterfall,\n               mortality = mortality))\n\n}\n\n\nparams &lt;- list()\nparams$alpha &lt;- 0.02\nparams$SLA &lt;- 4.74\nparams$leaf_frac &lt;- 0.315\nparams$Ra_frac &lt;- 0.5\nparams$Rbasal &lt;- 0.002\nparams$Q10 &lt;- 2.1\nparams$litterfall &lt;- 0.0012\nparams$mortality &lt;- 0.00015\nparams$sigma.leaf &lt;- 0.0 #0.01 \nparams$sigma.stem &lt;- 0.0 #0.01 ## wood biomass\nparams$sigma.soil &lt;- 0.0# 0.01\nparams &lt;- as.data.frame(params)\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nsite &lt;- \"TALL\"\nmet_s3 &lt;- arrow::s3_bucket(paste0(\"bio230014-bucket01/neon4cast-drivers/noaa/gefs-v12/stage3/site_id=\", site),\n                           endpoint_override = \"sdsc.osn.xsede.org\",\n                           anonymous = TRUE)\ninputs &lt;- arrow::open_dataset(met_s3) |&gt; \n  filter(variable %in% c(\"air_temperature\", \"surface_downwelling_shortwave_flux_in_air\")) |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  summarise(prediction = mean(prediction,  na.rm = TRUE), .by =c(\"variable\", \"datetime\")) |&gt;\n  collect() |&gt; \n  pivot_wider(names_from = variable, values_from = prediction) |&gt; \n  rename(temp = air_temperature,\n        SW = surface_downwelling_shortwave_flux_in_air) |&gt; \n  mutate(PAR = SW/0.486,\n         temp = temp - 273.15) |&gt; \n  select(-SW)\n\n\nX &lt;- list()\nX$leaf &lt;- 10\nX$wood &lt;- 140\nX$SOM &lt;- 140\nX_init &lt;- as.data.frame(X)\n\n\ndates &lt;- seq(as_date(\"2020-09-30\"), as_date(\"2023-12-01\"), by = \"1 day\")\n\nX &lt;- array(NA, dim = c(length(dates), 12))\nX[1,1:3 ] &lt;- unlist(X_init)\nfor(i in 2:length(dates)){\n  curr_input &lt;- inputs |&gt; \n    filter(datetime == dates[i])\n  \n  output &lt;- SSEM.orig(matrix(X[i-1,1:3], ncol = 3), params = params, inputs = curr_input)\n  \n  X[i, ] &lt;- output\n}\n\nout &lt;- as.data.frame(X)\nnames(out) &lt;- c(\"leaves\",\"wood\",\"SOM\", \"LAI\", \"GPP\", \"NEP\", \"Ra\", \"NPPw\", \"NPPl\", \"Rh\", \"litterfall\",\"mortality\")\n\nout &lt;- bind_cols(tibble(datetime = dates), out)\n\nout |&gt; \n  pivot_longer(-datetime, names_to = \"variable\", values_to = \"prediction\") |&gt; \n  ggplot(aes(x = datetime, y = prediction)) +\n  geom_line() +\n  facet_wrap(~variable, scales = \"free\")\n\nWarning: Removed 1 row containing missing values (`geom_line()`).",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#download-data",
    "href": "neon-data.html#download-data",
    "title": "15  NEON data",
    "section": "15.1 Download data",
    "text": "15.1 Download data\nFirst, we are going to want to define the site ID. The four letter site code for Oak Ridge is ORNL.\n\nsite &lt;- \"TALL\"\nelevation &lt;- 166    \nlatitude &lt;- 32.95047\nlongitude &lt;- -87.393259",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#wood-carbon",
    "href": "neon-data.html#wood-carbon",
    "title": "15  NEON data",
    "section": "15.2 Wood carbon",
    "text": "15.2 Wood carbon\nWe will also define the data product ID that we are going to extract first (Woody plant vegetation structure).\n\ndata_product &lt;- \"DP1.10098.001\"\n\nSecond, we will use the neon_download() function from neonstore to create a database of files for our data product and site.\n\nneon_download(product = data_product, site = site)\n\n  comparing hashes against local file index...\n\n\n  omitting 73 files previously downloaded\n\nprint(unique(neon_index(site = site)$table))\n\n [1] \"categoricalCodes\"             \"EML\"                         \n [3] \"mgp_perarchivesample-basic\"   \"mgp_perbiogeosample-basic\"   \n [5] \"mgp_perbulksample-basic\"      \"mgp_perhorizon-basic\"        \n [7] \"mgp_permegapit-basic\"         \"readme\"                      \n [9] \"cdw_fieldtally-basic\"         \"cdw_densitydisk-basic\"       \n[11] \"cdw_densitylog-basic\"         \"bbc_chemistryPooling-basic\"  \n[13] \"bbc_dilution-basic\"           \"bbc_percore-basic\"           \n[15] \"bbc_rootChemistry-basic\"      \"bbc_rootmass-basic\"          \n[17] \"vst_apparentindividual-basic\" \"vst_mappingandtagging-basic\" \n[19] \"vst_perplotperyear-basic\"     \"vst_shrubgroup-basic\"        \n\n\n\n15.2.1 Calculate carbon in trees\nIn this section, we will be calculating carbon in live and dead trees at Talladega. At the end we will have a site-level mean carbon stock in live trees for each year that was sampled from the plots that are sampling the ecosystem under the flux tower (e.g., tower plots).\nRead in the separate tables\n\n## use neon_read to extract the desired data\nind_table &lt;- neon_read('vst_apparentindividual', site = site)\n\n  Some raw data files have changed.\n  Using only most updated file to avoid duplicates.\n  see ?neonstore::show_deprecated_data() for details.\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nmap_tag_table &lt;- neon_read('vst_mappingandtagging', site = site)\n\nplot_table &lt;- neon_read('vst_perplotperyear', site = site)\n\n  Some raw data files have changed.\n  Using only most updated file to avoid duplicates.\n  see ?neonstore::show_deprecated_data() for details.\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n#allometrics &lt;- read_csv(\"Allometrics.csv\") %&gt;% \n#  mutate(SPECIES = ifelse(SPECIES == \"spp.\", \"sp.\", SPECIES))\n\n## link allodb\n\ngenus_species &lt;- unlist(str_split_fixed(map_tag_table$scientificName, \" \", 3))\n\nmap_tag_table &lt;- map_tag_table %&gt;% \n  mutate(GENUS = genus_species[,1], \n         SPECIES = genus_species[,2]) \n\nSelect the key variables in each table and join into the individual table, making sure that we have GENUS, SPECIES, and measurementHeight. These will be important when we use allodb to estimate the amount of carbon.\n\nselect_map_tag_table &lt;- map_tag_table %&gt;% \n  select(individualID, scientificName, GENUS, SPECIES) |&gt; \n  distinct()\n\nselect_plot_table &lt;- plot_table %&gt;% \n  select(plotID,totalSampledAreaTrees,plotType) %&gt;% \n  distinct(plotID, .keep_all = TRUE)\n\nselect_ind_table &lt;- ind_table %&gt;% \n  select(individualID, plotID, date, stemDiameter,plantStatus, measurementHeight) \n\ncombined_table &lt;- left_join(select_ind_table, select_map_tag_table, by = \"individualID\") %&gt;% \n  arrange(plotID,individualID)\n\ncombined_table &lt;- inner_join(combined_table, select_plot_table, by = c(\"plotID\")) %&gt;% \n  arrange(individualID)\n\ncombined_table_dbh &lt;- combined_table[which(combined_table$measurementHeight == 130),]\n\ncombined_table_dbh &lt;- combined_table_dbh[which(!is.na(combined_table_dbh$stemDiameter)),]\n\nTidy up the individual tree data to include only live trees from the tower plots. Also create a variable that is the year of the sample date.\n\ncombined_table_live_tower &lt;- combined_table_dbh %&gt;% \n  mutate(year = year(date)) %&gt;%\n  filter(str_detect(plantStatus,\"Live\"),\n         plotType == \"tower\") \n\nCalculate the biomass of each tree in the table. We will do this using get_biomass in the allodb package. This function takes as arguments: dbh, genus, species, coords. We have already extracted genus and species, and in fact we have already extracted dbh as well. The steps where we were subsetting based on measurement heights of 130 cm was actually subsetting to include only data that had dbh measurements.\nIn this next section, as well as a future one where we calculate dead tree carbon, we are going to make a simplfying assumption. We will assume that the below ground biomass of a tree is some fixed proportion of its above ground biomass. In our analysis, we will assume this value is \\(.3\\), but it is a parameter that can be changed.\n\nag_bg_prop &lt;- 0.3\n\n\nlibrary(allodb)\n\ntree_live_carbon &lt;- combined_table_live_tower %&gt;%\n        mutate(ag_tree_kg = get_biomass(\n          dbh = combined_table_live_tower$stemDiameter,\n          genus = combined_table_live_tower$GENUS,\n          species = combined_table_live_tower$SPECIES,\n          coords = c(longitude, latitude)\n          ),\n         bg_tree_kg = ag_tree_kg * ag_bg_prop, ## assumption about ag to bg biomass\n         tree_kgC = (ag_tree_kg + bg_tree_kg) * 0.5) ## convert biomass to carbon\n\nCalculate the plot level biomass\n\n plot_live_carbon &lt;-  tree_live_carbon %&gt;%\n    mutate(treeC_kgCm2 = (tree_kgC)/(totalSampledAreaTrees)) |&gt; \n    summarise(plot_kgCm2 = sum(treeC_kgCm2, na.rm = TRUE), .by = c(\"plotID\", \"year\"))\n\n\nggplot(plot_live_carbon, aes(x = year, y = plot_kgCm2, color = plotID)) + \n  geom_point() +\n  geom_line()\n\n\n\n\n\n site_live_carbon &lt;- plot_live_carbon %&gt;%\n    group_by(year) %&gt;%\n    summarize(mean_kgCperm2 = mean(plot_kgCm2, na.rm = TRUE),\n              sd_kgCperm2 = sd(plot_kgCm2))\n\n\nggplot(site_live_carbon, aes(x = year, y = mean_kgCperm2)) + \n  geom_point() + \n  geom_errorbar(aes(ymin=mean_kgCperm2-sd_kgCperm2, ymax=mean_kgCperm2+sd_kgCperm2), width=.2,\n                 position=position_dodge(0.05))\n\n\n\n\n\n\n15.2.2 Calculate carbon in dead trees\nWe will now use allodb to extract the carbon in dead trees.\n\ncombined_table_dead_tower &lt;- combined_table_dbh %&gt;% \n  mutate(year = year(date)) %&gt;%\n  filter(grepl(\"Standing dead\",plantStatus),\n         plotType == \"tower\") \n\nCalculate the biomass of each tree in the table\n\ntree_dead_carbon &lt;- combined_table_dead_tower %&gt;%\n    mutate(ag_tree_kg = get_biomass(\n          dbh = combined_table_dead_tower$stemDiameter,\n          genus = combined_table_dead_tower$GENUS,\n          species = combined_table_dead_tower$SPECIES,\n          coords = c(longitude, latitude)\n          ),\n         bg_tree_kg = ag_tree_kg * ag_bg_prop,\n         tree_kgC = (ag_tree_kg + bg_tree_kg) * 0.5)\n\nCalculate the plot level biomass\n\n plot_dead_carbon &lt;-  tree_dead_carbon %&gt;%\n    mutate(treeC_kgCm2 = (tree_kgC)/(totalSampledAreaTrees))  %&gt;%\n    group_by(plotID, year) %&gt;%\n    summarise(plot_kgCm2 = sum(treeC_kgCm2, na.rm = TRUE))\n\n`summarise()` has grouped output by 'plotID'. You can override using the\n`.groups` argument.\n\n\n\n site_dead_carbon &lt;- plot_dead_carbon %&gt;%\n    group_by(year) %&gt;%\n    summarize(mean_kgCperm2 = mean(plot_kgCm2, na.rm = TRUE),\n              sd_kgCperm2 = sd(plot_kgCm2))\n\n\nggplot(plot_dead_carbon, aes(x = year, y = plot_kgCm2, color = plotID)) + \n  geom_point() +\n  geom_line()",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#calculate-soil-carbon",
    "href": "neon-data.html#calculate-soil-carbon",
    "title": "15  NEON data",
    "section": "15.3 Calculate soil carbon",
    "text": "15.3 Calculate soil carbon\n\ndata_product1 &lt;- \"DP1.10014.001\"\nneon_download(product = data_product1, site = site)\n\n  comparing hashes against local file index...\n\n\n  omitting 21 files previously downloaded\n\nprint(unique(neon_index(product = data_product1, site = site)$table))\n\n[1] \"cdw_densitydisk-basic\" \"cdw_densitylog-basic\"  \"EML\"                  \n[4] \"readme\"               \n\ndata_product2 &lt;- 'DP1.10010.001'\nneon_download(product = data_product2, site = site)\n\n  comparing hashes against local file index...\n\n\n  omitting 14 files previously downloaded\n\nprint(unique(neon_index(product = data_product2, site = site)$table))\n\n[1] \"cdw_fieldtally-basic\" \"EML\"                  \"readme\"              \n\n\n\ncdw_tally &lt;- neon_read(table = 'cdw_fieldtally-basic', site = site)\n\ncdw_density &lt;- neon_read(table = 'cdw_densitydisk-basic', site = site)\n\nlog_table &lt;- neon_read(table = 'cdw_densitylog-basic', site = site)\n\n## filter by tower plot for log table\nlog_table_filter &lt;- log_table %&gt;% \n  filter(plotType == \"tower\")\n\n## filter by tower plot for cdw table\ncdw_tally &lt;- cdw_tally %&gt;%\n  filter(plotType == 'tower')\n\n## create \nlog_table_filter$gcm3 &lt;- rep(NA, nrow(log_table_filter))\n\n## set site specific volume factor\nsite_volume_factor &lt;- 5\n\nfor (i in 1:nrow(log_table_filter)){\n  ## match log table sampleID to cdw density table sample ID\n  ind &lt;- which(cdw_density$sampleID == log_table_filter$sampleID[i])\n  ## produce g/cm^3 by multiplying bulk density of disk by site volume factor\n  log_table_filter$gcm3[i] &lt;- mean(cdw_density$bulkDensDisk[ind]) * site_volume_factor\n}\n\n## table of coarse wood\nsite_cwd_carbon &lt;- log_table_filter %&gt;%\n  summarize(mean_kgCperm2 = .5 * sum(gcm3, na.rm = TRUE) * .1) |&gt; \n  mutate(year = unique(log_table_filter$yearBoutBegan))\n\nsite_cwd_carbon\n\n# A tibble: 1 × 2\n  mean_kgCperm2  year\n          &lt;dbl&gt; &lt;dbl&gt;\n1          7.17  2018",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-fine-roots",
    "href": "neon-data.html#calculate-carbon-in-fine-roots",
    "title": "15  NEON data",
    "section": "15.4 Calculate carbon in fine roots",
    "text": "15.4 Calculate carbon in fine roots\nHere we are going to calculate the carbon stored in fine roots using the root chemistry data product. We will calculate the carbon in both dead and alive roots. Though we are interested mostly in live roots, at the time of writing this, there the 2021 NEON data for our site does not have rootStatus data available. Thus we will use historical data to compute an estimate of the ratio, so that we don’t have to throw away perfectly good information.\n\n## root chemistry data product\nroots_DP &lt;- 'DP1.10067.001'\n## use neon_download from neonstore to read in data\nneon_download(product = roots_DP, site = site)\n\n  comparing hashes against local file index...\n\n\n  omitting 31 files previously downloaded\n\n## print available datasets from product\nprint(unique(neon_index(site = site, product = roots_DP)$table))\n\n[1] \"bbc_chemistryPooling-basic\" \"bbc_dilution-basic\"        \n[3] \"bbc_percore-basic\"          \"bbc_rootChemistry-basic\"   \n[5] \"bbc_rootmass-basic\"         \"EML\"                       \n[7] \"readme\"                    \n\n\n\n## read in bbc_percore\nbbc_percore &lt;- neon_read('bbc_percore-basic', site = site, product = roots_DP)\n## read in rootmass data\nrootmass &lt;- neon_read('bbc_rootmass-basic', site = site, product = roots_DP)\n## extract year\nrootmass$year = year(rootmass$collectDate)\n\n## set variables for liveDryMass, deadDryMass, unkDryMass, area\nrootmass$liveDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$deadDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$unkDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$area &lt;- rep(NA, nrow(rootmass))\n\n\nfor (i in 1:nrow(rootmass)){\n  ## match by sample ID\n  ind &lt;- which(bbc_percore$sampleID == rootmass$sampleID[i])\n  ## extract core sample area\n  rootmass$area[i] &lt;- bbc_percore$rootSampleArea[ind]\n  ## categorize mass as live, dead, or unknown\n  if (is.na(rootmass$rootStatus[i])){\n    rootmass$unkDryMass[i] &lt;- rootmass$dryMass[i]\n  } else if (rootmass$rootStatus[i] == 'live'){\n    rootmass$liveDryMass[i] &lt;- rootmass$dryMass[i]\n  } else if (rootmass$rootStatus[i] == 'dead'){\n    rootmass$deadDryMass[i] &lt;- rootmass$dryMass[i]\n  } else{\n    rootmass$unkDryMass[i] &lt;- rootmass$dryMass[i]\n  }\n}\n\n\n## aggregate at plot and year level\n## convert to kg (/1000) and carbon (.5)\nsite_roots &lt;- rootmass %&gt;%\n  group_by(plotID, year) %&gt;%\n  summarize(mean_kgCperm2_live = .5*sum(liveDryMass/area, na.rm = TRUE)/1000,\n            mean_kgCperm2_dead = .5*sum(deadDryMass/area, na.rm = TRUE)/1000,\n            mean_kgCperm2_unk = .5*sum(unkDryMass/area, na.rm = TRUE)/1000)\n\n`summarise()` has grouped output by 'plotID'. You can override using the\n`.groups` argument.\n\nmean(site_roots$mean_kgCperm2_live[which(site_roots$year == 2017)]) \n\n[1] 1.34645\n\nmean(site_roots$mean_kgCperm2_dead[which(site_roots$year == 2017)]) \n\n[1] 0.4293219\n\nmean(site_roots$mean_kgCperm2_unk[which(site_roots$year == 2017)]) \n\n[1] 0\n\nmean(site_roots$mean_kgCperm2_live[which(site_roots$year == 2021)]) \n\n[1] 0\n\nmean(site_roots$mean_kgCperm2_dead[which(site_roots$year == 2021)]) \n\n[1] 0\n\nmean(site_roots$mean_kgCperm2_unk[which(site_roots$year == 2021)]) \n\n[1] 1.687774\n\nsite_roots &lt;- site_roots |&gt; \n  ungroup() |&gt; \n  summarise(mean_kgCperm2 = mean(mean_kgCperm2_live + mean_kgCperm2_unk, na.rm = TRUE), .by = \"year\")\n\nThen, we can look at the ratio of the carbon densities: \\(\\rho_{live} = \\frac{C_{live}}{C_{live} + C_{dead}}\\) for 2017 and apply that to the data from 2021.\n\nc_dens_live2017 &lt;- mean(site_roots$mean_kgCperm2_live[which(site_roots$year == 2017)]) \n\nWarning: Unknown or uninitialised column: `mean_kgCperm2_live`.\n\n\nWarning in mean.default(site_roots$mean_kgCperm2_live[which(site_roots$year ==\n: argument is not numeric or logical: returning NA\n\nc_dens_dead2017 &lt;- mean(site_roots$mean_kgCperm2_dead[which(site_roots$year == 2017)]) \n\nWarning: Unknown or uninitialised column: `mean_kgCperm2_dead`.\n\n\nWarning in mean.default(site_roots$mean_kgCperm2_dead[which(site_roots$year ==\n: argument is not numeric or logical: returning NA\n\nrho &lt;- c_dens_live2017 / (c_dens_live2017 + c_dens_dead2017)\n\napprox_live_cdens_2021 &lt;- rho * mean(site_roots$mean_kgCperm2_unk[which(site_roots$year == 2021)]) \n\nWarning: Unknown or uninitialised column: `mean_kgCperm2_unk`.\n\n\nWarning in mean.default(site_roots$mean_kgCperm2_unk[which(site_roots$year == :\nargument is not numeric or logical: returning NA\n\nprint(approx_live_cdens_2021)\n\n[1] NA",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-soils",
    "href": "neon-data.html#calculate-carbon-in-soils",
    "title": "15  NEON data",
    "section": "15.5 Calculate carbon in soils",
    "text": "15.5 Calculate carbon in soils\n\n#Download bieogeochemistry soil data to get carbon concentration\n#data_product1 &lt;- \"DP1.00097.001\"\n#Download physical soil data to get bulk density\nsoil_dp &lt;- \"DP1.00096.001\"\n\n\n## use neon_download to extract data product\nneon_download(product = soil_dp, site = site)\n\n  comparing hashes against local file index...\n\n\n  omitting 9 files previously downloaded\n\nprint(unique(neon_index(site = site)$table))\n\n [1] \"categoricalCodes\"             \"EML\"                         \n [3] \"mgp_perarchivesample-basic\"   \"mgp_perbiogeosample-basic\"   \n [5] \"mgp_perbulksample-basic\"      \"mgp_perhorizon-basic\"        \n [7] \"mgp_permegapit-basic\"         \"readme\"                      \n [9] \"cdw_fieldtally-basic\"         \"cdw_densitydisk-basic\"       \n[11] \"cdw_densitylog-basic\"         \"bbc_chemistryPooling-basic\"  \n[13] \"bbc_dilution-basic\"           \"bbc_percore-basic\"           \n[15] \"bbc_rootChemistry-basic\"      \"bbc_rootmass-basic\"          \n[17] \"vst_apparentindividual-basic\" \"vst_mappingandtagging-basic\" \n[19] \"vst_perplotperyear-basic\"     \"vst_shrubgroup-basic\"        \n\n\n\nmgc_perbiogeosample &lt;- neon_read('mgp_perbiogeosample', site = site)\nmgp_perbulksample &lt;- neon_read('mgp_perbulksample', site = site)\n\n\nbulk_density &lt;- mgp_perbulksample %&gt;% \n    filter(bulkDensSampleType == \"Regular\") %&gt;% \n    select(horizonName,bulkDensExclCoarseFrag) \n\n  #gramsPerCubicCentimeter\nhorizon_carbon &lt;- mgc_perbiogeosample %&gt;% \n    filter(biogeoSampleType == \"Regular\") %&gt;% \n    select(horizonName,biogeoTopDepth,biogeoBottomDepth,carbonTot) \n\nyear &lt;- year(as_date(mgp_perbulksample$collectDate[1]))\n\n\n  #Unit notes\n  #bulkDensExclCoarseFrag = gramsPerCubicCentimeter\n  #carbonTot = gramsPerKilogram\n  \n  #Combine and calculate the carbon of each horizon\nhorizon_combined &lt;- inner_join(horizon_carbon,bulk_density, by = \"horizonName\") %&gt;%\n    #Convert volume in g per cm3 to mass per area in g per cm2 by multiplying by layer thickness\n    mutate(horizon_soil_g_per_cm2 = (biogeoBottomDepth - biogeoTopDepth) * bulkDensExclCoarseFrag) %&gt;% \n    #Units of carbon are g per Kg soil but we have bulk density in g per cm2 so convert Kg soil to g soil\n    mutate(CTot_g_per_g_soil = carbonTot*(1/1000),  #Units are g C per g soil\n           horizon_C_g_percm2 = CTot_g_per_g_soil*horizon_soil_g_per_cm2, #Units are g C per cm2\n           horizon_C_kg_per_m2 = horizon_C_g_percm2 * 10000 / 1000) %&gt;% #Units are g C per m2\n    select(-CTot_g_per_g_soil,-horizon_C_g_percm2) %&gt;%\n    arrange(biogeoTopDepth)\n  \nsite_soil_carbon &lt;- horizon_combined %&gt;% \n    summarize(soilC_gC_m2 = sum(horizon_C_kg_per_m2)) |&gt; \n  mutate(year = year)\n\n\nggplot(horizon_combined, map = aes(-biogeoTopDepth,horizon_C_kg_per_m2)) +\n  geom_line() +\n  geom_point() +\n  labs(y = \"Carbon\", x = \"Depth\", title = \"Soil carbon by depth\") +\n  coord_flip()",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#combine-together",
    "href": "neon-data.html#combine-together",
    "title": "15  NEON data",
    "section": "15.6 Combine together",
    "text": "15.6 Combine together\n\ntotal_carbon_components &lt;- unlist(c(site_live_carbon$mean_kgCperm2[1],site_dead_carbon$mean_kgCperm2[1],mean(site_cwd_carbon$mean_kgCperm2),site_soil_carbon)) \n\nsite_live_carbon &lt;- site_live_carbon |&gt; \n  mutate(variable = \"live_tree\") |&gt; \n  select(year, variable, mean_kgCperm2)\n\nsite_dead_carbon &lt;- site_dead_carbon |&gt; \n  mutate(variable = \"dead_trees\") |&gt; \n  select(year, variable, mean_kgCperm2)\n\nsite_cwd_carbon &lt;- site_cwd_carbon |&gt; \n  mutate(variable = \"down_wood\") |&gt; \n  select(year, variable, mean_kgCperm2)\n\nsite_roots &lt;- site_roots |&gt; \n  mutate(variable = \"fine_roots\") |&gt; \n  select(year, variable, mean_kgCperm2)\n\nsite_soil_carbon &lt;- site_soil_carbon |&gt; \n  mutate(variable = \"soil_carbon\") |&gt; \n  rename(mean_kgCperm2 = soilC_gC_m2) |&gt; \n  select(year, variable, mean_kgCperm2)\n\ntotal_carbon_components &lt;- bind_rows(site_live_carbon, site_dead_carbon, site_cwd_carbon, site_roots, site_soil_carbon)\n\ntotal_carbon_components |&gt; \n  ggplot(aes(x = year, y = mean_kgCperm2, color = variable)) + \n  geom_point()\n\n\n\ntotal_carbon_simple &lt;- total_carbon_components |&gt; \n  pivot_wider(names_from = variable, values_from = mean_kgCperm2) |&gt; \n  mutate(live = live_tree + mean(fine_roots, na.rm = TRUE),\n         SOM = mean(dead_trees, na.rm = TRUE) + mean(down_wood, na.rm = TRUE) + mean(soil_carbon, na.rm = TRUE)) |&gt; \n  select(year, live, SOM) |&gt; \n  pivot_longer(-year, names_to = \"variable\", values_to = \"observation\")",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#modis-lai",
    "href": "neon-data.html#modis-lai",
    "title": "15  NEON data",
    "section": "15.7 MODIS LAI",
    "text": "15.7 MODIS LAI\n\nlai &lt;- MODISTools::mt_subset(product = \"MCD15A2H\",\n                  lat = latitude,\n                  lon =  longitude,\n                  band = c(\"Lai_500m\", \"FparLai_QC\"),\n                  start = as_date(paste0(min(total_carbon_simple$year),\"-01-01\")),\n                  end = Sys.Date(),\n                  site_name = site,\n                  progress = FALSE)\n\n\nlai_cleaned &lt;- lai |&gt; \n  mutate(scale = ifelse(band == \"FparLai_QC\", 1, scale),\n         scale = as.numeric(scale),\n         value = scale * value,\n         datetime = lubridate::as_date(calendar_date)) |&gt; \n  select(band, value, datetime) |&gt; \n  pivot_wider(names_from = band, values_from = value) |&gt; \n  filter(FparLai_QC == 0) |&gt; \n  rename(observation = Lai_500m) |&gt; \n  mutate(variable = \"LAI\") |&gt; \n  select(datetime, variable, observation)\n\n\nlai_cleaned |&gt; \n  ggplot(aes(x = datetime, y = observation)) +\n  geom_point()",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#flux-contraints",
    "href": "neon-data.html#flux-contraints",
    "title": "15  NEON data",
    "section": "15.8 Flux contraints",
    "text": "15.8 Flux contraints\n\nurl &lt;- \"https://sdsc.osn.xsede.org/bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=P1D/terrestrial_daily-targets.csv.gz\"\nflux &lt;- read_csv(url, show_col_types = FALSE) |&gt; \n  filter(site_id %in% c(\"TALL\"), \n         variable == \"nee\") |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  select(datetime, variable, site_id, observation)\n\nflux |&gt; \n  filter(month(datetime) %in% c(2, 3,4,5, 6)) |&gt; \nggplot(aes(x = datetime, y = observation)) + geom_line() + facet_wrap(~site_id)",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "neon-data.html#combine-together-to-create-data-contraints",
    "href": "neon-data.html#combine-together-to-create-data-contraints",
    "title": "15  NEON data",
    "section": "15.9 Combine together to create data contraints",
    "text": "15.9 Combine together to create data contraints\n\nobs &lt;- total_carbon_simple |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  bind_rows(lai_cleaned, flux)\n\n\nobs |&gt; \n  ggplot(aes(x = datetime, y = observation)) + \n  geom_point() +\n  facet_wrap(~variable, scale = \"free\")\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>NEON data</span>"
    ]
  },
  {
    "objectID": "parameter-calibration1.html#problem-set",
    "href": "parameter-calibration1.html#problem-set",
    "title": "16  Parameter calibration: Intro to probablity and likelihood",
    "section": "16.1 Problem set",
    "text": "16.1 Problem set\n\n16.1.1 Part 1\nLoad dataset\n\nd &lt;- read_csv(file = \"https://data.ecoforecast.org/neon4cast-targets/phenology/phenology-targets.csv.gz\")\n\nRows: 269498 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): site_id, variable\ndbl  (1): observation\ndate (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFilter the dataset to only include the site_id BART (Bartlett Experimental Forest in the White Mountains of New Hampshire) and the dates between 2019-01-01 and 2019-07-01. Convert the date to Day of Year (hint: use lubridate:: yday() function). Remove rows with gcc_90 equal to NA or gcc_sd equal to 0.\n\nbart_2019 &lt;- d  %&gt;%\n  filter(site_id == \"BART\",\n         datetime &gt; as_date(\"2019-01-01\"),\n         datetime &lt; as_date(\"2019-07-01\"),\n         variable == \"gcc_90\") %&gt;%\n  mutate(doy = yday(datetime)) %&gt;% \n  filter(!is.na(observation),\n         observation &gt; 0)\n\nQuestion 1: How is gcc_90 related to day of year?\nAnswer 1:\n\n#Add Answer\n\nQuestion 2: Use a histogram to examine the distribution of the gcc_90\nAnswer 2:\n\n#Add Answer\n\nFirst create a function called `pred_logistic’ that is your process model. The model is the the logistic curve which ish the equation \\[P_1 + P_2 {{exp(P_3 + P_4 x)}\\over{1+exp(P_3 + P_4 x)}}\\] Question 3: Is this process model a dynamic model? Why or why not?\nAnswer 3:\nQuestion 4: Based on the equation above, write a function that predicts the gcc_90 as a function of the parameters (\\(P\\)) and x where x is the DOY. Name that function “pred_logistic”\nAnswer 4:\n\n#Add Answer\n\nQuestion 5: Write a function that calculates the negative log-likelihood of the data given a set of parameters governing the process and data models. Assume a normal distribution and be sure to estimate the sd in the data model.\nAnswer 5:\n\n#Add Answer\n\nQuestion 6: Use the optim function to find the most likely parameter values. Use the following as starting values par = c(0.34,0.11,-15,0.11, 1) where the first four are the theta parameters from your process model and the fifth is the sd of your data model.\nAnswer 6:\n\n#Add Answer\n\nQuestion 7: Use your optimal parameters in the pred_logistic function to predict the data. Save this as the object predicted\nAnswer 7:\n\n#Add Answer\n\nQuestion 8: Calculate the residuals and plot a histogram of the residuals\nAnswer 8:\n\n#Add Answer\n\nQuestion 9: How does the distribution of the data (Question 2) compare to the distribution of the residuals?\nAnswer 9:\nQuestion 10: Predict 2020 using the process model parameters from the 2019 fit.\n\n#Add Answer\n\nAnswer 10:\nQuestion 11: Plot the forecast from Question 10 over the data from 2020 (I give the code for getting the 2020 data)\nAnswer 11:\n\nbart_2020 &lt;- d  %&gt;%\n  filter(site_id == \"BART\",\n         datetime &gt; as_date(\"2020-01-01\"),\n         datetime &lt; as_date(\"2020-07-01\"),\n         variable == \"gcc_90\") %&gt;%\n  mutate(doy = yday(datetime)) %&gt;% \n  filter(!is.na(observation),\n         observation &gt; 0)\n\nQuestion 12: Do you think your model from 2019 is reasonable for predicting 2020?\nAnswer 12:\n\n\n16.1.2 Part 2 (worth 8 points)\nDownload the following data from Files/Exercises folder Canvas\n“soil_respiration_module_data.csv”\nIt is a dataset that reports soil respiration, soil temperature, and soil moisture over a year at the University of Michigan Biological Station (from Nave, L.E., N. Bader, and J.L. Klug)\nThe columns correspond to the following\ndoy = Day of Year soil_resp: Soil respiration (micromoles CO2 per m2 per second) soil_temp: Soil Temp (deg C) soil_moisture: Soil Moisture (%)\nModel the relationship between soil temperature and soil respiration using the Q10 function below\n\\[\\theta_1 * \\theta_2 ^{{(T - 20)}\\over{10}}\\]\nShow all the steps to determine the most likely parameter values, report the parameter values, and plot the data and predictions on the same plot",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Parameter calibration: Intro to probablity and likelihood</span>"
    ]
  },
  {
    "objectID": "parameter-calibration2.html#starting-with-likelihood",
    "href": "parameter-calibration2.html#starting-with-likelihood",
    "title": "17  Parameter calibration: Intro to Bayesian statistics",
    "section": "17.1 Starting with likelihood",
    "text": "17.1 Starting with likelihood\nHere we start where we left off with the likelihood lecture. Imagine the following dataset with five data points drawn from a normal distribution.\n\nnum_data_points &lt;- 5\n\nmean_data &lt;- 3.0\nsd_data &lt;- 1.0\n\nnew_data &lt;- rnorm(num_data_points, mean = mean_data, sd = sd_data)\nhist(new_data)\n\n\n\n\nWe can calculate the likelihood for a set of different means using the manual likelihood estimation that we did in the likelihood lecture\n\n#DATA MODEL\ndelta_x &lt;- 0.1\nx &lt;- seq(-10,10, delta_x)\nnegative_log_likelihood &lt;- rep(NA,length(x))\nfor(i in 1:length(x)){\n  negative_log_likelihood[i] &lt;- -sum(dnorm(new_data, mean = x[i], sd = 1, log = TRUE))\n}\n\nplot(x, negative_log_likelihood)\n\n\n\n\nThe negative log likelihood is useful for using finding the maximum likelihood values with a optimizing function. However, here we want to convert back to density\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nplot(x, density_likelihood, type = \"l\")\n\n\n\n\nThe y-axis are tiny numbers because we multiplied probability densities together. This is OK for our illustrative purposes. To help visualize we can rescale so that area under the curve is 1. The area is approximated where the density is the height and the width is the distance between points on the x axis (delta_x) that we evaluated (height x width is the area of a bar under the curve). If we sum the bars together, we get the area under the curve (a value way less than 1). Dividing the likelihood by the area rescales the densities so the area under the curve is 1. Rescaling the likelihood is not a formal part of the the analysis - just used here for visualizing.\n\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_x) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\nThe rescaled likelihood looks like this\n\nd &lt;- tibble(x = x,\n            likelihood = density_likelihood_rescaled)\nggplot(d, aes(x = x, y = likelihood)) +\n  geom_line()\n\n\n\n\nRemember that this curve is the P(data | mean = x). We want the P(mean = x | data).\nFollowing Bayes rule we can multiply the likelihood x the prior\nOur prior is the following normal distribution\n\nmean_prior &lt;- 0\nsd_prior &lt;- 1.0\n\n#Priors\ndensity_prior &lt;- dnorm(x, mean = mean_prior, sd = sd_prior)\nd &lt;- tibble(x = x,\n            density_prior = density_prior)\nggplot(d, aes(x = x, y = density_prior)) +\n  geom_line()\n\n\n\n\nPutting the rescaled likelihood on the same plot as the prior results in\n\ntibble(x = x,\n            prior = density_prior,\n            likelihood_rescaled = density_likelihood_rescaled) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line() \n\n\n\n\nMultiplying the prior x the likelihood (not rescaled)\n\nprior_times_likelihood &lt;- density_prior * density_likelihood\nd &lt;- tibble(x = x,\n            prior_times_likelihood = prior_times_likelihood)\nggplot(d, aes(x = x, y = prior_times_likelihood)) +\n  geom_line()\n\n\n\n\nBut from Bayes rule we can rescale the prior x likelihood by the area under the curve to convert to a probability density function\n\narea_under_curve &lt;- sum(prior_times_likelihood * delta_x)  #sum(Width * Height) = total probability of the data given all possible values of the parameter\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\npaste0(\"the probablity of the data is (i.e., area under likelihood x prior curve): \", area_under_curve)\n\n[1] \"the probablity of the data is (i.e., area under likelihood x prior curve): 7.42475962358318e-06\"\n\n\nNow we can visualize the rescaled likelihood (remember that the un-scaled likelihood was actually used in the calculations), the prior, and the normalized posterior.\nSee how the how the posterior is a blend of the prior and the likelihood\n\ntibble(x = x,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line()\n\n\n\n\nHere is a function that will allow you to explore how the posterior is sensitive to the likelihood and the prior\n\nexplore_senstivity &lt;- function(num_data_points, mean_data, sd_data, mean_prior, sd_prior, title){\n\nnew_data &lt;- rnorm(num_data_points, mean_data, sd_data)\ndelta_x &lt;- 0.1\nx &lt;- seq(-5,5, delta_x)\nnegative_log_likelihood &lt;- rep(NA,length(x))\n\n#DATA MODEL\nfor(i in 1:length(x)){\n  #Process model is that mean = x\n  negative_log_likelihood[i] &lt;- -sum(dnorm(new_data, mean = x[i], sd = 1, log = TRUE))\n}\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_x) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\n#Prior\ndensity_prior &lt;- dnorm(x, mean = mean_prior, sd = sd_prior)\n\n#Prior x Likelihood\nprior_times_likelihood &lt;- density_prior * density_likelihood\narea_under_curve &lt;- sum(prior_times_likelihood * delta_x) #Width * Height\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\np &lt;- tibble(x = x,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line() +\n  labs(title = title)\nreturn(p)\n}\n\nNow you can explore how the posterior is sensitive to 1) prior sd (i.e., confidence in the prior) 2) the number of data points used in the likelihood (how does increasing the number of data points influence the posterior) the prior mean 3) the mean of the data used in the likelihood (how different is it than the prior?)\n\n#Baseline\np1 &lt;- explore_senstivity(num_data_points = 5,\n                        mean_data = 3,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 1.0,\n                        title = \"Baseline\")\n\n#Increase confidence in prior\np2 &lt;- explore_senstivity(num_data_points = 5,\n                        mean_data = 3,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 0.1,\n                        title = \"Increase confidence in prior\")\n\n#Increase number of data points\np3 &lt;- explore_senstivity(num_data_points = 50,\n                        mean_data = 3,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 1.0,\n                        title = \"Increase data\")\n\n#Make likelihood mean closer to prior\np4 &lt;- explore_senstivity(num_data_points = 5,\n                        mean_data = 0,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 1.0,\n                        title = \"Make likelihood mean closer to prior\")\n(p1 / p2) | (p3 / p4) \n\n\n\n\nJust like we extended the likelihood analysis to the non-linear example, we can do the same for the Bayesian analysis.\nFirst create a data set using the Michaelis-Menten function from the likelihood exercise. Here, instead of fitting both parameters, we only fit one parameter (the maximum or saturating value) called par1. In the chunk below we set the number of data points, the true value for par1, and the standard deviation of the data.\n\nset.seed(100)\nnum_data_points &lt;- 10\npar1_true &lt;- 3\nsd_data &lt;- 0.5\nx &lt;- runif(num_data_points, 0, 10)\npar_true &lt;- c(par1_true, 0.5)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\nplot(x, y, ylim = c(0, par1_true + 2))\n\n\n\n\nNow we can define the prior. We think the prior is normally distributed with a mean and sd defined below\n\nmean_prior &lt;- 1.0\nsd_prior &lt;- 0.5\n\nHere is the manual calculation of the likelihood and the prior. We combine the results into a data frame for visualization.\n\ndelta_par1 &lt;- 0.1\npar1 &lt;- seq(-3,10, delta_par1)\nnegative_log_likelihood &lt;- rep(NA,length(par1))\nfor(i in 1:length(par1)){\n  #Process model\n  pred &lt;- par1[i] * (x / (x + par_true[2]))\n  #Data model\n  negative_log_likelihood[i] &lt;- -sum(dnorm(y, mean = pred, sd = sd_data, log = TRUE))\n}\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_par1) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\n#Priors\ndensity_prior &lt;- dnorm(par1, mean = mean_prior, sd = sd_prior)\nprior_times_likelihood &lt;- density_prior * density_likelihood\narea_under_curve &lt;- sum(prior_times_likelihood * delta_par1) #Width * Height\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\ntibble(par1 = par1,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -par1, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = par1, y = density, color = distribution)) +\n  geom_line()\n\n\n\n\nNow we can look at how our the prior and posterior distributions influence the shape of the process model curve (M-M). For illustration, the figure below shows the M-M curve using the most likely value from the prior, most likely value if we just looked at the likelihood, and most likely value from the posterior.\n\npar1_mle &lt;- par1[which.max(density_likelihood)]\npar1_post &lt;- par1[which.max(normalized_posterior)]\n\nd &lt;- tibble(x = seq(0,10, 0.1),\n            prior = mean_prior * (x / (x + par_true[2])),\n            likelihood = par1_mle * (x / (x + par_true[2])),\n            posterior = par1_post * (x / (x + par_true[2]))) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"prediction\") %&gt;% \n  mutate(distribution = factor(distribution))\nggplot(d, aes(x = x, y = prediction, col = distribution)) +\n  geom_line() +\n  labs(y = \"M-M model prediction (process model)\")",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Parameter calibration: Intro to Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "parameter-calibration2.html#solving-a-bayesian-model",
    "href": "parameter-calibration2.html#solving-a-bayesian-model",
    "title": "17  Parameter calibration: Intro to Bayesian statistics",
    "section": "17.2 Solving a bayesian model",
    "text": "17.2 Solving a bayesian model\nThe example above is designed to build an intuition for how a Bayesian analysis works but is not how the parameters in a Bayesian model are estimated in practice. For one thing, if there are multiple parameters being estimated it is very hard to estimate the area under the curve. Second, the area is very very same if there are many data points (so small that the computer can’t hold it well)\nThere are two common methods for estimating posterior distributions that involve numerical computation. Both methods involve randomly sampling from an unknown posterior distribution and saving the samples. The sequence of saved sample for the parameters is called a Markov chain Monte Carlo (MCMC). The distribution of parameter values in the MCMC chain is your posterior distribution.\nThe first is called Gibbs sampling and is used when you know the probability distribution type for the posterior but not the parameter values of the distribution. For example, if your prior is normally distributed and your likelihood is normally distributed then you know (from math that other have already done for you) that the posterior is normally distributed. Therefore, you can randomly draw from that distribution to build your MCMC chain that generates your posterior distribution. We will not go over this in detail but know that this method will give you an answer quicker but requires you (or the software you are using) to know that your prior and likelihood are conjunct (i.e., someone else has worked out that the posterior always has a certain PDF if the prior and likelihood are of certain PDF - see pages 86-89 and Table A3 in Hobbs and Hooten).\nThe second is called MCMC Metropolis–Hastings (MCMC-MH) and is a rejection sampling method. In this case you don’t know the form of the posterior. Basically the MCMC-MH is the following\n\nCreate vector that has a length equal to the total number of iterations you want to have in your MCMC chain. Set the first value of the vector to your parameter starting point.\nrandomly choose a new parameter value based on the previous parameter value in the MCMC chain (called a proposal). For example (where i is the current iteration in your MCMC chain): par_proposed &lt;- rnorm(1, mean = par[i - 1], sd = jump), where jump is the standard deviation that governs how far you want the proposed parameter to potentially be from the previous parameter.\nuse this proposed parameter in your likelihood and prior calculations and multiply the likelihood * prior (i.e., the numerator in Bayes formula). Save this as the proposed probability(prob_proposed)\nTake the ratio of the proposed probability to the previous probability (also called the current probability; prob_current). call this prob_proposed\nRandomly select a number between 0 and 1. Call this z\nIf prob_proposed from Step 3 is greater than z from Step 4 then save the proposed parameter for that iteration of MCMC chain. If it is less, then assign the previous parameter value for that iteration of the MCMC chain. As a result of Step 5:\n\n\nAll parameters that improve the probability will have prob_proposed/prob_current &gt; 1. Therefore all improvements will be accepted since z (by definition in Step 4) can never be greater than 1.\nWorse parameters where prob_proposed/prob_current &lt; 1, will be accepted in proportion to how worse they are. For example if prob_proposed/prob_current = 0.9, 90% of the time z will be less the .90 so the worse parameters than are worse by 10% will be accepted 90% of the time. If prob_proposed/prob_current = 0.01 (i.e., the new parameters are much worse), only 1% of the time will z be less then 0.01. Therefore it is possible but not common to save these worse parameters. As a result, the MCMC-MH approach explores the full distribution by spending more time at more likely parameter values. This is different than maximum likelihood optimization methods like optim' that only save parameters that are better than previous (thus finds the peak of the mountain rather than the shape of the mountain). The MCMC-HM approach requires taking a lot samples so that it spends some time at very unlikely values - a necessity for estimating the tails of a distribution.\nYou want to accept ~40% percent of all proposed parameter values. If your jump parameter from #1 is too large, you won’t be able two explore the area around the most likely values (i.e., you won’t get a lot of prob_proposed/prob_current values near 1). If your jump parameter from #1 is too small, you won’t be able to explore the tails of the distribution (i.e., you won’t get a lot of prob_proposed/prob_current values near 0 that have a random chance of being accepted).\n\nNote: there is a step that is ignored here that just confuses at this stage. Your proposal distribution in #1 doesn’t have to be normal, which is symmetric (i.e., your probability of jumping from a value of X to Y is the same as jumping from Y to X). There is adjustment for non-symmetric proposals that is on page page 71 in Dietze and page 158 in Hobbs and Hooten.\nHere is an example of the MCMC-MH method:\n(note: the example below should use logged probability densities for numerical reasons but uses the non-logged densities so that the method is more clear. The example in assignment uses logged densities)\nSet up data (same as above)\n\nnum_data_points &lt;- 10\npar1_true &lt;- 3\nsd_data &lt;- 0.5\nx &lt;- runif(num_data_points, 0, 10)\npar_true &lt;- c(par1_true, 0.5)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\nplot(x, y, ylim = c(0, par1_true + 2))\n\n\n\n\nRun MCMC-MH\n\n#Initialize chain\nnum_iter &lt;- 5000\npars &lt;- array(NA, dim = c(num_iter))\npars[1] &lt;- 2\nlog_prob_current &lt;- -10000000000\nprob_current &lt;- exp(log_prob_current)\njump &lt;- 0.1\n\nmean_prior &lt;- 1.0\nsd_prior &lt;- 0.5\n\nfor(i in 2:num_iter){\n  \n  #Randomly select new parameter values\n  proposed_pars &lt;- rnorm(1, pars[i - 1], jump)\n  \n    \n  #PRIORS: how likely is the proposed value given the prior distribution?\n  prior &lt;- dnorm(proposed_pars, mean = mean_prior, sd = sd_prior)\n\n  #PROCESS MODEL: Use new parameter values in the process model\n  pred &lt;- proposed_pars * (x / (x + par_true[2]))\n\n  #DATA MODEL: how likely is the data given the proposed parameter?\n  #We are multiplying here\n  likelihood &lt;- prod(dnorm(y, mean = pred, sd = sd_data))\n  \n  #Combine the prior and likelihood\n  #remember that you multiply probabilities which mean you can add log(probability)\n  prob_proposed &lt;- prior * likelihood\n  \n  z &lt;- (prob_proposed/prob_current)\n  \n  #Now pick a random number between 0 and 1\n  r &lt;- runif(1, 0, 1)\n  \n  #If z &gt; r then accept the new parameters\n  #Note: this will always happen if the new parameters are more likely than\n  #the old parameters z &gt; 1 means than z is always &gt; r no matter what value of\n  #r is chosen.  However it will accept worse parameter sets (P_new is less\n  #likely then P_old - i.e., z &lt; 1) in proportion to how much worse it is\n  #For example: if z = 0.9 and then any random number drawn by runif that is\n  #less than 0.90 will result in accepting the worse values (i.e., the slightly\n  #worse values will be accepted a lot of the time).  In contrast, if z = 0.01\n  #(i.e., the new parameters are much much worse), then they can still be accepted\n  #but much more rarely because random r values of &lt; 0.1 occur more rarely\n  if(z &gt; r){\n    pars[i] &lt;- proposed_pars\n    prob_current &lt;- prob_proposed\n  }else{\n    pars[i] &lt;- pars[i - 1]\n    prob_current &lt;- prob_current #this calculation isn't necessary but is here to show you the logic\n  }\n}\n\nThe pars variable is our MCMC chain estimating the posterior distribution . We can visualize it in two ways. The first is with iteration number on the axis. The second is as a histogram. A chain is that ready for analysis will have a constant mean and variance. The variance is important because it is the exploration of the posterior distribution. The histogram shows the posterior distribution.\n\nd &lt;- tibble(iter = 1:num_iter,\n       par1 = pars)\n\np1 &lt;-  ggplot(d, aes(x = iter, y = par1)) +\n  geom_line()\n\np2 &lt;- ggplot(d, aes(x = par1)) +\n  geom_histogram()\n\np1 | p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYou should notice that the chain starts at 2 before moving to a mean of 3. The starting value of 2 was arbitrary. Since it was far from 3, the proposed new parameter values often resulted in improvements and accepting values that were more likely. As a result the chain moves to the part with a mean of 3 and constant variance (i.e., where the chain has converged). This transition from the starting value to the point where the chain has converged should be discard. We call this the “burn-in”. Here are the same plots with the burn-in removed\n\nnburn &lt;- 100\nd_burn &lt;- tibble(iter = nburn:num_iter,\n       par1 = pars[nburn:num_iter])\n\np1 &lt;-  ggplot(d_burn, aes(x = iter, y = par1)) +\n  geom_line()\n\np2 &lt;- ggplot(d_burn, aes(x = par1)) +\n  geom_histogram()\n\np1 | p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow your can analyze the chain to explore the posterior distribution\n\npar_post_burn &lt;- pars[nburn:num_iter]\n\n#Mean\nmean(par_post_burn)\n\n[1] 2.703547\n\n#sd\nsd(par_post_burn)\n\n[1] 0.1801216\n\n#Quantiles\nquantile(par_post_burn, c(0.025, 0.5,0.975))\n\n    2.5%      50%    97.5% \n2.359081 2.700100 3.065872 \n\n\nFinally, you can sample from the posterior distribution just like your would sample from a random variable using the rnorm, rexp, rlnorm, etc. function. The key is to randomly select an iteration (num_sample = 1) or a set of samples (if num_sample &gt; 0) with replacement (replace = TRUE; i.e., a iteration could be randomly selected multiple times).\n\nnum_samples &lt;- 100\nsample_index &lt;- sample(x = 1:length(par_post_burn), size = num_samples, replace = TRUE)\nrandom_draws &lt;- par_post_burn[sample_index]\nhist(random_draws)",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Parameter calibration: Intro to Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "parameter-calibration2.html#predictive-posterior-distributions",
    "href": "parameter-calibration2.html#predictive-posterior-distributions",
    "title": "17  Parameter calibration: Intro to Bayesian statistics",
    "section": "17.3 Predictive posterior distributions",
    "text": "17.3 Predictive posterior distributions\nFinally we can use the idea of randomly drawing from the posterior to develop predictions from the posterior. This is just like the Logistic growth module where you randomly sampled from the parameter uncertainty and used the random samples in the logistic equation.\nHere we are calculating two things 1) pred_posterior_mean just has uncertainty in the M-M parameter. Think about this as generating uncertainty around the mean prediction at each value of x. 2) y_posterior is the prediction of a observation. Therefore you take the value from #1 and add the uncertainty in the observations from sd_data that was set above. This is your predictive or forecast uncertainty.\nImportant: If you have multiple parameters your MCMC chain, you randomly draw posterior_sample_indices and use values for all the parameters at that iteration. By doing this you are representing the joint distribution of the parameter - e.g., if parameter 1 is high, parameter 2 is always low. If you select posterior_sample_indices for each parameter, you break the correlations of parameters that the MCMC method estimated and, as a result, overestimate the uncertainty.\n\nnum_samples &lt;- 1000\nx_new = x\npred_posterior_mean &lt;- matrix(NA, num_samples, length(x_new))   # storage for all simulations\ny_posterior &lt;- matrix(NA, num_samples, length(x_new)) \n\nfor(i in 1:num_samples){\n  sample_index &lt;- sample(x = 1:length(pars), size = 1, replace = TRUE)\n  pred_posterior_mean[i, ] &lt;- pars[sample_index] * (x_new / (x_new + par_true[2]))\n  y_posterior[i, ] &lt;- rnorm(length(x_new), pred_posterior_mean[i, ], sd = sd_data)\n  \n}\nn.stats.y &lt;- apply(y_posterior, 2, quantile, c(0.025, 0.5, 0.975))\nn.stats.y.mean &lt;- apply(y_posterior, 2, mean)\n\nn.stats.mean &lt;- apply(pred_posterior_mean, 2, quantile, c(0.025, 0.5, 0.975))\n\nd &lt;- tibble(x = x_new,\n            median = n.stats.y.mean,\n            lower95_y = n.stats.y[1, ],\n            upper95_y = n.stats.y[3, ],\n            lower95_mean = n.stats.mean[1, ],\n            upper95_mean = n.stats.mean[3, ],\n            obs = y)\n\nggplot(d, aes(x = x)) +\n  geom_ribbon(aes(ymin = lower95_y, ymax = upper95_y), fill = \"lightblue\", alpha = 0.5) +\n    geom_ribbon(aes(ymin = lower95_mean, ymax = upper95_mean), fill = \"pink\", alpha = 0.5) +\n  geom_line(aes(y = median)) +\n  geom_point(aes(y = obs)) +\n  labs(y = \"M-M Prediction\")",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Parameter calibration: Intro to Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "parameter-calibration2.html#problem-set",
    "href": "parameter-calibration2.html#problem-set",
    "title": "17  Parameter calibration: Intro to Bayesian statistics",
    "section": "17.4 Problem set",
    "text": "17.4 Problem set\nUsing the following as a guide\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n#Build fake dataset\nset.seed(100)\nnum_data_points &lt;- 200\nsd_data &lt;- 0.25\npar_true &lt;- c(3, 0.5)\nx &lt;- runif(num_data_points, 0, 10)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\nplot(x, y, ylim = c(0, par_true[1] + 2))\n\n\n\n#Set MCMC Configuration\nnum_iter &lt;- 2000\nnum_pars &lt;- 2\njump &lt;- c(0.05, 0.05)\n\n#Initialize chain\npars &lt;- array(NA, dim = c(num_pars, num_iter))\npars[1, 1] &lt;- 2\npars[2, 1] &lt;- 1\nlog_likelihood_prior_current &lt;- -10000000000\n\nfor(i in 2:num_iter){\n  \n  #Loop through parameter value\n  \n  for(j in 1:num_pars){\n      #Randomly select new parameter values\n    proposed_pars &lt;- pars[, i - 1]\n    proposed_pars[j] &lt;- rnorm(1, mean = pars[j, i - 1], sd = jump[j])\n    \n    ##########################\n    # PRIORS\n    #########################\n    #(remember that you multiply probabilities which mean you can add log(probability))\n    log_prior &lt;- dunif(proposed_pars[1], min = 0, max = 10, log = TRUE) + \n      dunif(proposed_pars[2], min = 0, max = 100, log = TRUE)\n    \n    #Likelihood.  \n    #You could use:\n    # pred &lt;- process_model(x, pars = proposed_pars)\n    # log_likelihood &lt;- sum(dnorm(new_data, mean = pred, sd = sd_data, log = TRUE)\n    # but we are looping here because it transitions well to the next section of the course\n    log_likelihood &lt;- rep(NA, length(x))\n    pred &lt;- rep(NA, length(x))\n    for(m in 1:length(x)){\n      ##########################\n      # PROCESS MODEL\n      #########################\n      pred[m] &lt;- proposed_pars[1] * (x[m] / (x[m] + proposed_pars[2]))\n      ##########################\n      # DATA MODEL\n      #########################\n      log_likelihood[m] &lt;- dnorm(y[m], mean = pred[m], sd = sd_data, log = TRUE)\n    }\n    #Remember that you multiply probabilities which mean you can add log(probability)\n    #Hence the use of sum\n    log_likelihood &lt;- sum(log_likelihood)\n    \n    ############################\n    ###  PRIOR x LIKELIHOOD\n    ############################\n    #Combine the prior and likelihood\n    #remember that you multiply probabilities which means you can add log(probability)\n    log_likelihood_prior_proposed &lt;- log_prior + log_likelihood\n    \n    #We want the ratio of new / old but since it is in log space we first\n    #take the difference of the logs: log(new/old) = log(new) - log(old) \n    # and then take out of log space exp(log(new) - log(old))\n    z &lt;- exp(log_likelihood_prior_proposed - log_likelihood_prior_current)\n    \n    #Now pick a random number between 0 and 1\n    r &lt;- runif(1, min = 0, max = 1)\n    #If z &gt; r then accept the new parameters\n    #Note: this will always happen if the new parameters are more likely than\n    #the old parameters z &gt; 1 means than z is always &gt; r no matter what value of\n    #r is chosen.  However it will accept worse parameter sets (P_new is less\n    #likely then P_old - i.e., z &lt; 1) in proportion to how much worse it is\n    #For example: if z = 0.9 and then any random number drawn by runif that is\n    #less than 0.90 will result in accepting the worse values (i.e., the slightly\n    #worse values will be accepted a lot of the time).  In contrast, if z = 0.01\n    #(i.e., the new parameters are much much worse), then they can still be accepted\n    #but much more rarely because random r values of &lt; 0.1 occur more rarely\n    if(log(z) &gt; log(r)){\n      pars[j, i] &lt;- proposed_pars[j]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_proposed\n    }else{\n      pars[j, i] &lt;- pars[j, i - 1]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_current #this calculation isn't necessary but is here to show you the logic\n    }\n  }\n}\n\nd &lt;- tibble(iter = 1:num_iter,\n            par1 = pars[1, ],\n            par2 = pars[2, ]) %&gt;%\n  pivot_longer(-iter, values_to = \"value\", names_to = \"parameter\")\n\np1 &lt;- ggplot(d, aes(x = iter, y = value)) +\n  geom_line() +\n  facet_wrap(~parameter, scales = \"free\")\n\np2 &lt;- ggplot(d, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~parameter, scales = \"free\")\n\np1 / p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nYour task is to modify the code above to estimate the posterior distribution of parameters in Q10 function that was in the likelihood analysis exercise. Use the same data as used in the Q10 likelihood exercise.\nQuestion 1: Provide the distribution and parameters describing the distribution for your prior distributions. Justify why you chose the distribution and parameters. (do not spend time looking at the literature for values to use to build prior distribution - just give plausible priors and say why their plausible)\nAnswer 1:\nQuestion 2: Provide plots of your prior distributions.\nAnswer 2:\nQuestion 3: Modify the code above to estimate the posterior distribution of your parameters. Put your modified code below.\nAnswer 3:\nQuestion 4: Plot the your MCMC chain for all parameters (iteration # will be the x-axis)\nAnswer 4:\nQuestion 5: Approximately how many iterations did it take your chain to converge to a straight line with constant variation around the line (i.e., a fuzzy caterpillar). This is the burn-in. If your chain did not converge, modify the jump variable for each parameters and/or increase your iterations. You should not need more than 10000 iterations for convergence so running the chain for a long period of time will not fix issues that could be fixed by modifying the jump variable. Also, pay attention to the sd_data parameter. You should estimate it as a parameter or set it to a reasonable value. If it is too small your chain will fail because the probability of the some of parameters that are explored functionally zero.\nAnswer 5:\nQuestion 6: Remove the iterations between 1 and your burn-in number and plot the histograms for your parameters.\nAnswer 6:\nQuestion 7: Provide the mean and 95% Credible Intervals for each parameter\nAnswer 7:\nQuestion 8: Random select 1000 values from the parameters in your posterior distribution. Show the randomly selected values for each parameter as a histogram.\nAnswer 8:\nQuestion 9: Use the samples from Question 8 to generate posterior predictions of soil respiration at the observed temperature values (i.e., the same temperature data used in your model fit). Provide a plot with temperature on the x-axis and respiration on the y-axis. The plot should have the mean and 95% predictive uncertainty bounds (i.e., include uncertainty in parameters and in the data model)\nAnswer 9:",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Parameter calibration: Intro to Bayesian statistics</span>"
    ]
  },
  {
    "objectID": "parameter-calibration3.html",
    "href": "parameter-calibration3.html",
    "title": "18  Parameter calibration: Applying to process model",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Parameter calibration: Applying to process model</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#review-of-batch-vs-sequential-methods",
    "href": "particle-filter.html#review-of-batch-vs-sequential-methods",
    "title": "19  Data assimilation: particle filter",
    "section": "19.1 Review of Batch vs Sequential Methods",
    "text": "19.1 Review of Batch vs Sequential Methods\nBefore beginning the introduction of the particle filter it is important to revisit the MCMC-MH approach that we used to estimate the parameters of a Bayesian model. Here is the general algorithm again:\nfor(i in 1:num_iterations){\n\n    Choose new parameters based on previous parameters\n\n    for(t in 1:length_of_time_series){\n    \n      Make predictions over full time series\n    \n    }\n    \n    Calculate likelihood of data given model and current parameters\n    Calculate probability of priors given current parameters\n    \n    Accept or reject parameters\n\n}\nAbove you see that the outermost for-loop is looping over the number of iterations (num_iterations). The inner loop is looping over the length of the time series (length_of_time_series). Therefore this approach tests each parameter value choose in an iteration (and latent state if using a state space model) over ALL time points in the time series. As a result, we all this a batch method because it considers all data as a single “batch” of data.\nStrengths of the batch method are:\n- The parameters are consistent with all data - Straightforward to estimate parameters, uncertainty parameters, and latent states\nWeaknesses:\n- Can be computationally slow\n- Require re-fitting model if there is even a single new data point\nAlternatively, sequential methods only analyze data one time point as a time. Here is general code for a sequential method - notice that the for-loop order is reversed\n\nCalculate prior distribution of parameters\n\nfor(t in 1:length_of_time_series){\n\n    for(i in 1:num_of_particles){\n    \n      Make predictions for each particle based on previous value for particle\n    \n    }\n    \n    Compare particle to data (likelihood or other technique)\n\n    Adjust particles based on the comparsion to the data\n\n}\nIn the sequential method we can restart at any time point, as long as we have the values for the states and parameters that are associated with each particle. When doing an iterative forecast, these values are what you would save to wait for new data to arrive.",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data assimilation: particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#introduction-to-particle-filter",
    "href": "particle-filter.html#introduction-to-particle-filter",
    "title": "19  Data assimilation: particle filter",
    "section": "19.2 Introduction to Particle Filter",
    "text": "19.2 Introduction to Particle Filter\nThere are many sequential data assimilation methods that make different assumptions about data and model distributions in order to simplify the analysis. Many of the methods emerged before the our massive computational resources or were developed for HUGE problems like assimilating terabytes of data into a global weather model that simulates the physics of the atmosphere at 25 km vertical resolution. Examples include the Kalman Filter, Extending Kalman Filter, Ensemble Kalman Filter, and 4D-var. These methods all heavily use matrix algebra which I have not introduced in this class. Since these methods commonly assume that the data and model errors are normally distributed, the numerical version (Ensemble Kalman Filter) can run fewer particles (in this case: ensemble members) because it doesn’t take as many samples to estimate the mean and variance of a distribution than it does to estimate the full distribution\nHere I introduce the Particle Filter. The particle filter is a sequential method that is familiar to folks that have learned the Bayesian methods that we have covered in the class. It has the concept of a likelihood, of which you are well versed. The particle filter does not assume a specific distribution of the data and model error so it requires more particles to estimate the full distributions. As a result, it is more appropriate for “smaller” problems like the ones we are tackling in this class.\nThe particle filter is quite simple\n\nInitialize a set of particles: Set the initial distribution of the states for each particle and, if estimating parameters, initial distribution of parameters that you want to estimate. Think of this as your initial priors.\nPredict the next time step using your process model for each particle. Add process uncertainty to each particle (i.e., rnorm)\nIf there are observations at the time-step, calculate the likelihood of the data given the particle just like we calculated the likelihood in the likelihood and Bayesian exercises. For example: LL &lt;- dnorm(obs, mean = pred, sd_obs). You will typically use the uncertainty of the observations in the likelihood because you have already included the process uncertainty in #2. If observations are not available at the time step, then continue to the time step.\nIf there are observations at the time-step, resample the particles using the likelihood as the weights (don’t forget to exponentiate the likelihood if you logged it in #3, the weights must be probabilities rather than log probabilities). The weighted sampling, with replacement, will randomly pick the more likely particles more often. You will keep the same number of particles but the values for each particle will change. Less likely particles will be replaced with more likely particles (though the less likelihood particles can still be selected). Be sure to resample all states together and, if also estimating parameters, the parameters as well. The key to resampling is the following:\n\n  ## calculate likelihood (weight) for one state with an observation at that time-step\n  ## The dimensiosn of x\n  wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_data)\n    \n  ## resample ensemble members in proportion to their weight.  \n  ## Since the total number of samples = the number of particles then you will preserve the \n  ## Same number of particles\n  resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt) \n  \n  ##Use the index to resample\n  x[t, ] &lt;- x[t, resample_index]\n\nContinue to next time step.\n\nFundamentally, the particle filter depends on two concepts that you have already been exposed to: likelihood and sampling from a set of particles (you sampled iterations from MCMC in previous exercises).\nSpecifically we refer to the particle filter described above as a bootstrap particle filter",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data assimilation: particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#example-of-particle-filter",
    "href": "particle-filter.html#example-of-particle-filter",
    "title": "19  Data assimilation: particle filter",
    "section": "19.3 Example of Particle Filter",
    "text": "19.3 Example of Particle Filter\nHere is an example of the particle filter applied to the google flu data\nFirst load in data. We are only going to focus on the first 15 weeks of the data so that you can see the particles more easily\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ngflu &lt;- read_csv(\"data/gflu_data.txt\", skip = 11) %&gt;% \n  select(Date, Virginia)\n\nRows: 620 Columns: 160\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (159): United States, Alabama, Alaska, Arizona, Arkansas, California, C...\ndate   (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngflu$Date &lt;- as.Date(gflu$Date)\ngflu &lt;- gflu[1:15, ]\nggplot(data = gflu, aes(x = Date, y = log(Virginia))) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Date\", y = \"Log(flu cases)\", title = \"Virginia Google Flu Trends\")\n\n\n\n\n\n19.3.1 PF with no-observations\nFirst we are going to run the particle filter with all data missing. This is equal to the random walk.\nThe sd_add would have been determined from a state-space Bayesian MCMC chain that used previously available data.\nThe sd_obs is the observation uncertainty.\nThe sd_init sets the initial uncertainty in the model states (you could use the distribution of the last latent state from a state-space Bayesian MCMC chain)\nThe key decision is the num_particles. More is always better but it comes at a computational and computer memory cost.\n\nnum_particles &lt;- 25\n\ny &lt;- log(gflu$Virginia)\n\nnt &lt;- length(y)\n\n#This sets all the observations to NA after the first\ny[2:nt] &lt;- NA\n\nsd_init &lt;- 0.5\nsd_add &lt;- 0.2\nsd_obs &lt;- 0.2\n\nx &lt;- array(NA, dim = c(nt, num_particles))\n\nx[1, ] &lt;- rnorm(num_particles, mean = y[1], sd = sd_obs)\n\n### resampling bootstrap particle filter\n\nfor(t in 2:nt){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    x[t, m ] &lt;- x[t - 1, m  ] + rnorm(1, mean = 0, sd = sd_add)\n  }\n \n  ## analysis step\n  if(!is.na(y[t])){ \n\n      ## calculate Likelihood (weights)\n      wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n    }\n}\n\nNow plot the particles individually\n\ntibble(time = 1:nt,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -time, names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  ggplot(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_line() +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"State of Virginia\")\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\n\n\n\n\nLets save the output as a different name so we can compare to a PF with observations\n\nx_no_obs &lt;- x\n\n\n\n19.3.2 PF with observations\nNow we can examine how the PF uses observations to update the model and modify the trajectory. The following is the same as above except that there are data at week 1, 5, and 10.\n\nnum_particles &lt;- 25\n\ny &lt;- log(gflu$Virginia)\n\nnt &lt;- length(y)\n\n#This sets all the observations to NA after the first\ny[2:nt] &lt;- NA\ny[c(5, 10)] &lt;- log(gflu$Virginia)[c(5, 10)]\n\nsd_init &lt;- 0.5\nsd_add &lt;- 0.2\nsd_obs &lt;- 0.2\n\nx &lt;- array(NA, dim = c(nt, num_particles))\n\nx[1, ] &lt;- rnorm(num_particles, mean = y[1], sd = sd_init)\n\n### resampling bootstrap particle filter\n\nfor(t in 2:nt){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    x[t, m ] &lt;- x[t - 1, m  ] + rnorm(1, 0, sd_add)\n  }\n \n  ## analysis step\n  if(!is.na(y[t])){\n\n      ## calculate Likelihood (weights)\n      wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n    }\n}\n\nNow plot the particles with the observations. You can see how the particles are adjusted when data are present.\n\ntibble(time = 1:nt,\n       as_tibble(x),\n       obs = y) %&gt;% \n  pivot_longer(cols = -c(\"time\",\"obs\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(x = exp(x),\n         obs = exp(obs)) %&gt;% \n  ggplot(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_line() +\n  geom_point(aes(y = obs), color = \"red\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"State of Virginia\")\n\nWarning: Removed 300 rows containing missing values (`geom_point()`).\n\n\n\n\n\nSave the output as a different object to compare to other PF simulations\n\nx_with_obs &lt;- x\n\nNow we can compare the influence of data assimilation on the last 5 weeks of the time-series (think of this as a 5-week forecast)\n\nno_obs &lt;- tibble(time = gflu$Date,\n       as_tibble(x_no_obs)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"no obs\")\n\nwith_obs &lt;- tibble(time = gflu$Date,\n       as_tibble(x_with_obs)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"with obs\")\n\ncombined &lt;- bind_rows(no_obs, with_obs)\n\ngflu$obs_in_fit &lt;- exp(y)\n\ncombined %&gt;% \n  group_by(time, type) %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = gflu, aes(x = Date, y = Virginia), color = \"red\") +\n  geom_point(data = gflu, aes(x = Date, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"Google Flu Trends for Virginia\")\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n19.3.3 PF with parameter estimation\nThe estimate of parameters using PF is also straightforward. Here we will estimate the b1 parameter in the Dynamic Linear Model from the state-space exercise. The DLM uses minimum temperature as a covariant\nFirst, create the observed data that includes the minimum temperature. (same as above and using the daymetr package from the previous exercise)\n\ngflu &lt;- read_csv(\"data/gflu_data.txt\", skip = 11)  %&gt;% \n  select(Date, Virginia)\n\nRows: 620 Columns: 160\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (159): United States, Alabama, Alaska, Arizona, Arkansas, California, C...\ndate   (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngflu$Date &lt;- as.Date(gflu$Date)\n\ndf &lt;- daymetr::download_daymet(site = \"Blacksburg\",\n                               lat = 37.22,\n                               lon = -80.41,\n                               start = 2003,\n                               end = 2016,\n                               internal = TRUE)$data\n\nDownloading DAYMET data for: Blacksburg at 37.22/-80.41 latitude/longitude !\n\nDone !\n\ndf$date &lt;- as.Date(paste(df$year,df$yday,sep = \"-\"),\"%Y-%j\")\nTmean &lt;- mean(df$tmin..deg.c.)\ngflu$Tmin = df$tmin..deg.c.[match(gflu$Date,df$date)] - Tmean\n\ngflu &lt;- gflu[1:15, ]\nggplot(data = gflu, aes(x = Date, y = log(Virginia))) +\n  geom_line()\n\n\n\n\nNow modify the PF with the DLM model instead of the random walk. This should be familiar to you.\nThe values for b0, b2, and sd_add are the mean values from the MCMC chain in the previous assignment.\nWe are estimating b1 (the sensitivity to minimum temperature). Just like we need to initialize the states at the first time step, we will initialize the distribution of the b1 at the first time step using a normal distribution with a mean = b1_mean and sd = b1_sd. This were the mean an sd of b1 from the MCMC chain in the previous assignment.\n\ny &lt;- log(gflu$Virginia)\nnum_particles &lt;- 25\nnt &lt;- length(y)\n\ny[2:nt] &lt;- NA\ny[c(5, 10)] &lt;- log(gflu$Virginia)[c(5, 10)]\n\nsd_init &lt;- 0.5\nsd_add &lt;- 0.130144\nsd_obs &lt;- 0.2\n\nb0 &lt;- 0.482462\nb2 &lt;- -0.063887\nb1_mean &lt;- -0.002479\nb1_sd &lt;- 0.01\n\nx &lt;- array(NA, dim = c(nt, num_particles))\nx[1, ] &lt;- rnorm(num_particles, y[1], sd = sd_init)\n\nb1 &lt;- array(NA, dim = c(nt, num_particles))\nb1[1, ] &lt;- rnorm(num_particles, mean = b1_mean, sd = b1_sd)\n\nTmin &lt;- gflu$Tmin\n\nNow run the PF with the DLM model. You need to also carry through the values for b1.\nImportantly, the distribution of b1 carries through from the previous time-step. When there is an observation, b1 is resampled using the same index that the states are resampled. This ensures that the parameters match the states from the same particle.\n\nfor(t in 2:nt){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    \n    pred &lt;- x[t-1, m] + b0 + b1[t-1, m] * Tmin[t] + b2*x[t-1, m]\n    \n    x[t, m ] &lt;- pred + rnorm(1, mean = 0, sd = sd_add)\n    b1[t, m] &lt;- b1[t-1, m]\n  }\n  \n  ## analysis step\n  if(!is.na(y[t])){\n    \n    ## calculate Likelihood (weights)\n    wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n    \n    ## resample ensemble members in proportion to their weight\n    resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt) \n\n    x[t, ] &lt;- x[t, resample_index]  ## update state\n    b1[t, ] &lt;- b1[t, resample_index] ## Parameter update\n  }\n}\n\nNow visualize the states from the PF\n\ntibble(time = 1:nt,\n       obs = y,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -c(time,obs), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  ggplot() +\n  geom_line(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_point(aes(x = time, y = exp(obs)), color = \"red\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"State of Virginia\")\n\nWarning: Removed 300 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAnd visualize the time-evolution of the parameter. There are two new concepts illustrated below:\n\nparameters distributions evolve through time. As a result the distribution of parameters is strongly influenced by the most recent observations. The distributions produced by a PF are not the same as the distributions produced by MCMC chain.\n\nparticles can have degeneracy, whereby the values of the parameters collapse down to one or a few values. This occurs because the PF does not propose new parameter values, it only selects (through resampling) parameter values from the initial set that you started with. Over the time-step the PF weeds out bad parameters and only a few ones are left. This is a major issue with a PF. Degeneracy can also occur in the states but since we are adding process uncertainty (sd_add) the particles are able to separate through time. There are a couple of ways to solve degeneracy - increase the number of particles so you sample more initial parameter values or propose new parameter values as part of the PF (how do to this is beyond the scope of this lecture)\n\n\ntibble(time = 1:nt,\n       as_tibble(b1)) %&gt;% \n  pivot_longer(cols = -c(time), names_to = \"ensemble\", values_to = \"b1\") %&gt;% \n  ggplot() +\n  geom_line(aes(x = time, y = b1, group = factor(ensemble))) +\n  labs(x = \"Date\", y = \"B1\", title = \"State of Virginia\")\n\n\n\n\n\n\n19.3.4 Senstivity to number of particles\nThe number of particles is a key decision when using a PF. To explore the sensitivity of the PF to the number of particles, here is a function that can be reused with different numbers of particles. It is the same as the DLM above and returns the x and b1 for the particles in a list\n\nbootstrap_pf &lt;- function(num_particles, sd_add = 0.2, sd_obs = 0.2){\n  y &lt;- log(gflu$Virginia)\n  nt &lt;- length(y)\n  \n  y[2:nt] &lt;- NA\n  y[c(5, 10)] &lt;- log(gflu$Virginia)[c(5, 10)]\n  \n  sd_init &lt;- 0.5\n\n  b0 &lt;- 0.482462\n  b2 &lt;- -0.063887\n  b1_mean &lt;- -0.002479\n  b1_sd &lt;- 0.01\n  sd_add &lt;- 0.130144\n  \n  x &lt;- array(NA, dim = c(nt, num_particles))\n  \n  x[1, ] &lt;- rnorm(num_particles, mean = y[1], sd = sd_init)\n  \n  b1 &lt;- array(NA, dim = c(nt, num_particles))\n  b1[1, ] &lt;- rnorm(num_particles, b1_mean, sd = b1_sd)\n  \n  Tmin &lt;- gflu$Tmin\n  \n  ### resampling bootstrap particle filter\n  \n  for(t in 2:nt){\n    \n    ## forward step\n    for(m in 1:num_particles){\n      \n      pred &lt;- x[t-1, m] + b0 + b1[t-1, m] * Tmin[t] + b2*x[t-1, m]\n      \n      x[t, m ] &lt;- pred + rnorm(1, mean = 0, sd = sd_add)\n      b1[t, m] &lt;- b1[t-1, m]\n    }\n    \n    ## analysis step\n    if(!is.na(y[t])){\n      \n      ## calulate Likelihood (weights)\n      wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n      b1[t, ] &lt;- b1[t, resample_index] ## Parameter update\n    }\n  }\n  return(list(x = x, b1 = b1))\n}\n\nFirst, run the PF using 10, 100, 1000, and 10000 particles\n\npf_10 &lt;- bootstrap_pf(10)\npf_100 &lt;- bootstrap_pf(100)\npf_1000 &lt;- bootstrap_pf(1000)\npf_10000 &lt;- bootstrap_pf(10000)\n\nAnd combine to a single plot. You see the width of the 95 % confidence interval increases substantially from 10 to 1000 particles but then is similar from 1000 to 10000. This reflects what we learned from the very first exercise where you drew random samples from a distribution and found that between 1000 and 10000 random samples were required to approximate the distribution well.\n\np10 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_10$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_10\")\n\np100 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_100$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_100\")\n\np1000 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_1000$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_1000\")\n\np10000 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_10000$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_10000\")\n\ngflu$obs_in_fit &lt;- exp(y)\n\nbind_rows(p10, p100, p1000, p10000) %&gt;% \n  group_by(time, type) %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = gflu, aes(x = Date, y = Virginia), color = \"red\") +\n  geom_point(data = gflu, aes(x = Date, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"Google Flu Trends for Virginia\")\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n19.3.5 Senstivity to observation uncertainity\nWe can also explore the sensitivity of the PF updating to our observation uncertainty. Intuitively, we should have stronger update (i.e., the particles are adjusted to be closer to the observation) when there is less uncertainty in the observations. The code below explores whether this intuition is correct.\n\npf_low_obs &lt;- bootstrap_pf(5000, sd_obs = 0.1)\n\npf_high_obs &lt;- bootstrap_pf(5000, sd_obs = 0.5)\n\npf_low &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_low_obs$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"low obs uncertainity\")\n\npf_high &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_high_obs$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"high obs uncertainity\")\n\ngflu$obs_in_fit &lt;- exp(y)\n\nbind_rows(pf_low, pf_high) %&gt;% \n  group_by(time, type) %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = gflu, aes(x = Date, y = Virginia), color = \"red\") +\n  geom_point(data = gflu, aes(x = Date, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"Google Flu Trends for Virginia\")\n\nWarning: Removed 12 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data assimilation: particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#problem-set",
    "href": "particle-filter.html#problem-set",
    "title": "19  Data assimilation: particle filter",
    "section": "19.4 Problem set",
    "text": "19.4 Problem set\n\n19.4.1 Overview\nThis exercise involves the following objectives\n\nModify the particle filter examples to apply to a new model. The model is defined as NIMBLE code so you have to convert to a particle filter.\nUse out put from a state-space model fit to initialize the particle filter\nRun the particle filter without observations to forecast\nRun the particle filter to assimilate new observations and forecast\nEvaluate how the forecast depends on data assimilation\n\n\n19.4.1.1 Data\nThe data for this exercise is above biomass vs. age for a single forest plot. The data has two columns: age (in years) and biomass (in gC/m2)\n\n\n19.4.1.2 Model\nWe are predicting the aboveground biomass using the following model\nbiomass = previous biomass + constant growth - mortality rate * previous biomass\nThe constant growth is the parameter g below (in units of gC/m2/yr) and mortality rate is the parameter ubelow (proportion of biomass per year). We fit the model as a state-space model.\n\n\n\n19.4.2 Part 1: Fit model to historical data (Already done for you!)\nThis step is already done for you.\nHere is the data for ages 1 through 50 for the plot. It was measured every 5 years.\n\nplot_data &lt;- read_csv(\"data/PF_data1.csv\")\n\nRows: 50 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): age, biomass\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(plot_data, aes(x = age, y = biomass)) +\n  geom_point() +\n  labs(x = \"age\", y = \"aboveground biomass (gC/m2)\")\n\nWarning: Removed 40 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe following model was used to estimate the posterior distributions of the parameters using a Bayesian state-space framework. sd_data is defined in the constants\nstate_space &lt;- nimbleCode({\n  \n  #Priors\n  g ~ dunif(300, 1000)\n  u ~ dunif(0, 1)\n  sd_add ~ dunif(0, 800)\n  \n  biomass_latent[1] &lt;- biomass_init\n  y[1] ~ dnorm(biomass_latent[1], sd = sd_data)  \n  \n  for(i in 2:n){\n    pred[i] &lt;- biomass_latent[i - 1] + g - u * biomass_latent[i - 1]\n    biomass_latent[i] ~ dnorm(pred[i], sd = sd_add)        ## process model\n    y[i]  ~ dnorm(biomass_latent[i], sd = sd_data)       ## data model\n  }\n})\n\nconstants &lt;- list(n = length(plot_data$biomass),\n                  biomass_init = plot_data$biomass[1],\n                  sd_data = 200)\nThe NIMBLE run produces an MCMC object. The MCMC output with the burn-in removed is in the following R data.\nThe MCMC chain (nimble_burn) has posterior distributions for the parameters (g, u, sd_add) and latent states (biomass_latent)\n\nload(\"data/PF_MCMC.Rdata\")\n\n\n\n19.4.3 Part 2: Forecast using PF\nNow you will use the nimble_burn MCMC to determine the mean parameter values and the initial condition at age 50 for the particle filter.\nUsing the lecture material create a particle filter that uses the forest growth model to simulate the aboveground biomass of the forest for age 50 through 70.\n\n19.4.3.1 Step 1: Set up PF\nFollowing the code in the PF lecture set up the particle filter.\nInstead of using data from a file you will use the following for your data: y &lt;- rep(NA, 21)\nBe sure to:\n\nuse the mean values for g, u, and sd_add from the MCMC chain as the parameter values\nuse the distribution of the latent state from age 50 in the MCMC chain as your initial state for the PF\n\n\ny &lt;- rep(NA, 21) #number of years from age 50 to 70\nsd_data &lt;- 200\n#ADD CODE TO SET UP PF HERE\n\n\n\n19.4.3.2 Step 2: Run particle filter\nWrite the code and run the particle filter based on the examples from the lecture. You will need to include the process model that is in the NIMBLE code above.\n\n#ADD PF CODE HERE\n\n\n\n19.4.3.3 Step 3: Visualize particle filter output\nGenerate a plot that visualizes the output of the PF (see examples from the lecture). Your plot must have age on the x-axis and biomass on the y-axis with different lines for the particles.\n\n# ADD VISUALIZATION CODE HERE\n\n\n\n19.4.3.4 Step 4: Save PF output\nuse this code to save your PF output as the object initial_forecast\n\ninitial_forecast &lt;- x\n\n\n\n\n19.4.4 Part 3:\nNow we have new data!\n\nnew_data &lt;- read_csv(\"data/PF_data2.csv\")\n\nRows: 21 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): age, biomass\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(new_data, aes(x = age, y = biomass)) +\n  geom_point() +\n  labs(x = \"age\", y = \"aboveground biomass (gC/m2)\")\n\nWarning: Removed 19 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n19.4.4.1 Step 1: Repeat the PF setup\nUsing the new data as y, repeat the PF set up in Part 2 Step 1. You will be starting at age 50 just like above.\n\ny &lt;- new_data$biomass\nsd_data &lt;- 200\n#ADD CODE TO SET UP PF HERE\n\n\n\n19.4.4.2 Step 2: Run particle filter using the new data\nUsing the new data as y, run the the PF again. This will be the same code as in Part 2 Step 2 (just copy and paste)\n\n#COPY AND PASTE PF CODE FROM ABOVE\n\n\n\n19.4.4.3 Step 3: Visualize PF output\nGenerate a plot that visualizes the output of the PF (see examples from the lecture). Your plot must have age on the x-axis and biomass on the y-axis with different lines for the particles. Your observations from the new data must be on the plot.\n\n#ADD VISUALIZATION CODE HERE\n\n\n\n19.4.4.4 Step 4: Save output\n\nassimilated_forecast &lt;- x\n\n\n\n\n19.4.5 Part 4:\nCombine the two PF forecast and evaluate how data assimilation influence the forecast of the last 10 years (age 60 to 70). Produce a plot with the mean and 95% CI for the initial_forecast and assimilated_forecast on the same plot. Include the observations from the new data set.\n\n#ADD CODE TO COMPARE THE TWO PF OUTPUTS\n\n\n\n19.4.6 Part 5:\nAnswer the follow question\nHow did assimilating data influence your forecast for ages 60 to 70? Consider both the mean and uncertainty in your answer.",
    "crumbs": [
      "Advanced forecasting",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Data assimilation: particle filter</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "22  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "background-skills.html#r-skills",
    "href": "background-skills.html#r-skills",
    "title": "2  Background skills",
    "section": "2.1 R skills",
    "text": "2.1 R skills\nR, Rstudio, Tidyverse (read_csv, write_csv, mutate, filter, group_by, summarize, ggplot, select, pipes (|&gt; or %&gt;), pivot_wider, pivot_longer, arrange, left_join)\nhttps://r4ds.hadley.nz](https://r4ds.hadley.nz\nhttps://github.com/frec-3044/tidyvere-intro-template/blob/main/assignment/tidyverse-intro.qmd",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "background-skills.html#git-skills",
    "href": "background-skills.html#git-skills",
    "title": "2  Background skills",
    "section": "2.2 Git Skills",
    "text": "2.2 Git Skills\n\nCreate a GitHub user account at https://github.com, if you don’t already have one. Here is advice about choosing a user name, because choosing a good user name is critical.\nGo to Rstudio and install the usethis package.\n\ninstall.packages(\"usethis\")\n\nRun the following command where you replace the user.email and user.name with the email used for GitHub and your GitHub user name. You can learn more about the command here\n\nlibrary(usethis)\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\nIf you get an error at this step it is likely due to your computer not having Git. Follow the instructions here about installing Git\n\nSet up your GitHub credentials on your computer. Follow the instructions here about using usethis::create_github_token() and gitcreds::gitcreds_set() functions. Also, save your GitHub PAT to a password manager so that you can find it in the future (in case you need to interact with GitHub from a different computer).\nGo to Canvas and get the link to accept the assignment. Copy and paste the link in a web browser. Accept the assignment.\nGo to your assignment at https://github.com/frec-3044-Spring2024. Click on repository.\nUnder the green “Code” button, select the local tab, and copy the https link.\nOpen Rstudio on your computer and create a new project. First, File -&gt; New Project -&gt; Version Control -&gt; Git. Paste the URL from you repo in the first box, hit tab to fill in the repo name in the second, and then use Browse to select where you want the project on your computer (I recommend having a directory on your computer where you keep all repositories we use in the class).\nYour project will load. Then go to File -&gt; New -&gt; New File -&gt; Quarto Document\nIn the prompt use Title = “Assignment 1” and Author = [Your name]\nSave file as “assignment1.qmd” in the assignment subdirectory of the Project.\nCommit your assignment1.qmd file using the Git tab at the top right pane using a useful commit message. You will need to check the box for the files that you want to commit. A useful message helps you broadly remember what you did to the files that are included in the commit. The Git tab may not be in the top right panel if you have moved the panels around.\nFind the Sources / Visual buttons right above the document. Select Source (which is the code view).\nCopy the code chunk on lines 21-24 and paste at end of document. Change to echo: TRUE.\nFind the following code at the top\n\nformat: html:\nand change to so that all the necessary files are saved in a single html file.\nformat:   \n  html:\n    embed-resources: true\n\nFind the Render (found above the document) button and click it to render the document a to an html document. You will see a file named “assignment1.html” appear. The html is like webpage version of your code. If your have a directory called assignment1_files then you did not do step 15 correctly.\nClick on the “assignment1.html” in your “Files” pane and select “View in Web Browser”. Confirm that it looks as expected.\nCommit the updated .qmd and new .html files to git.\nPush to your repository on GitHub.\nGo to https://github.com/frec-3044-Spring2024 and click on your repository. You should also see three commits: 2 were committed by you and 1 was committed by the github classroom bot. The Github classroom bot commit is the automatic commit that occurred when you accepted the assignment.\nGo to the course on Canvas and upload the .html file to the assignment.\n\nIf you are having issues (i.e., your computer does not seem to have Git installed), here is an excellent resource to help you debug your git + Rstudio issues.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "background-skills.html#docker-skills",
    "href": "background-skills.html#docker-skills",
    "title": "2  Background skills",
    "section": "2.3 Docker Skills",
    "text": "2.3 Docker Skills\nGo to https://docs.docker.com/get-docker/ to install the relevant install for your platform (available for PC, Mac and Linux). Also see https://docs.docker.com/desktop/.\nNOTE: * If you’re running Windows, you will need WSL (Windows Subsystem for Linux) * If you’re running a Linux distribution, you may have to enable Viritualization on your computer (see here)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "background-skills.html#running-a-docker-container",
    "href": "background-skills.html#running-a-docker-container",
    "title": "2  Background skills",
    "section": "2.4 Running a docker container",
    "text": "2.4 Running a docker container\n\nLaunch Docker Desktop (either from the Command Line or by starting the GUI)\nAt the command line run the following command which tells docker to run the container with the name eco4cast/rocker-neon4cast that has all the packages and libraries installed already. The PASSWORD=yourpassword sets a simple password that you will use to open the container. The -ti option starts both a terminal and an interactive session.\n\ndocker run --rm -ti -e PASSWORD=yourpassword -p 8787:8787 eco4cast/rocker-neon4cast\nThis can take a few minutes to download and install. It will be quicker the next time you launch it.\n\nOpen up a web browser and navigate to http://localhost:8787/\nEnter the username: rstudio and password: yourpassword\nYou should see a R Studio interface with all the packages etc. pre-installed and ready to go.\n\nYou can close this localhost window (and then come back to it) but if you close the container from Docker (turn off your computer etc.) any changes will be lost unless you push them to Github or exported to your local environment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "third-forecast.html",
    "href": "third-forecast.html",
    "title": "8  Third forecast: automatically updating second forecast",
    "section": "",
    "text": "https://github.com/OlssonF/NEON-forecast-challenge-workshop/blob/main/Automate_forecasts/automation_tutorial.Rmd",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Third forecast: automatically updating second forecast</span>"
    ]
  }
]