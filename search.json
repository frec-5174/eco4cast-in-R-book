[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A practical guide to ecological forecasting",
    "section": "",
    "text": "Preface\nThis book is designed for Virginia Tech graduate course FREC 5174: Ecological Modeling and Forecasting. It leverages work through the NSF-funded Macrosystems EDDIE (DEB-1926050) and Ecological Forecasting Initiative Research Coordination Network (DEB-1926388) projects. It includes training materials developed by the Macrosystems EDDIE forecasting team at Virginia Tech (Cayelan Carey, Mary Lofton, Tadhg Moore, Whitney Woelmer), Freya Olsson (NEON Ecological Forecasting Challenge workshop), Mike Dietze (Terrestrial carbon flux tutorial and modified activities from his Ecological Forecasting book), and John Smith (NEON carbon data processing code). The imprint of Carl Boettiger on the materials is throughout the book.\nThe book is a work in progress that involves harmonizing materials developed for many different use-cases (undergraduate courses, tutorials, graduate classes). As a result there are likely to differences among chapters in style and approaches to assignments.\nI welcome feedback on the book through GitHub issues and/or Pull Requests."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Welcome!. The overarching goal of the course (and this book) is to teach concepts and skills in ecological modeling and forecasting and contribute forecasts to an international forecasting challenge run by the Ecological Forecasting Initiative and led out of Virginia Tech (Thomas et al. 2023). The NEON (National Ecological Observatory Network) Ecological Forecasting Challenge empowers teams to submit forecasts of NEON data before it is collected as a test of our capacity to predict ecological processes in the future. More information about the NEON Ecological Forecasting Challenge can be found here: neon4cast.org\nThe first half of the book will build the foundations of ecological forecasting using a set of modules, developed at Virginia Tech, that have been widely used and tested across the globe (Moore et al. 2022, Woelmer et al. 2023). Topics include an introduction to the iterative, near-term forecasting cycle, understanding uncertainty in ecological forecasts, using data to improve ecological forecasts, and using ecological forecasting to guide decision making. By the end of the first section, you will be automatically submitting forecasts that you developed to the NEON Ecological Forecasting Challenge. The first half will explicitly focus on forecasting using statistical/empirical models. In the process, you will gain skills in the use of GitHub, GitHub Actions, Docker, Tidymodels https://www.tidymodels.org), and Fable https://fable.tidyverts.org).\nThe second half of the book will introduce more advanced concepts in ecological modeling and forecasting by focusing on the use of process models (e.g., models that represent ecological mechanisms). We will learn how to build a process model, estimate the parameters of process models using likelihood and Bayesian techniques, and update the model using data assimilation.\nThroughout the class you will work with multiple NEON data products that include water temperature, phenocam, chlorophyll-a and terrestrial carbon.\nLearning objectives\nHaving completed the course a student will be able to:\n\nCreate computer models that mathematically represent an ecological system,\nApply maximum likelihood methods to estimate parameters in ecological models using data,\nApply Bayesian methods to estimate parameter distributions in ecological models using data,\nApply sequential data assimilation to improve ecological model predictions, and\nCreate, evaluate, and interpret an ecological forecast of the future that includes uncertainty."
  },
  {
    "objectID": "background-skills.html#r-skills",
    "href": "background-skills.html#r-skills",
    "title": "2  Background skills",
    "section": "2.1 R skills",
    "text": "2.1 R skills\nThe book uses R as the focal programming language and generate uses the Tidyverse approach when working with and visualizing data. The following functions are commonly used: read_csv, write_csv, mutate, filter, group_by, summarize, ggplot, select, pipes (|&gt; or %&gt;), pivot_wider, pivot_longer, arrange, left_join). If you are new R and Tidyverse there are many great materials on the internet. The Data Carpentry “Data Analysis and Visualization in R for Ecologists” is an excellent starting point for learning. The R for Data Science book is an especially useful reference for learning the Tidyverse commands. Finally, I have created an introductory to tidyverse module for my undergraduate Environmental Data Science class. You can use the module as a “test” of your Tidyverse skills"
  },
  {
    "objectID": "background-skills.html#sec-github",
    "href": "background-skills.html#sec-github",
    "title": "2  Background skills",
    "section": "2.2 Git Skills",
    "text": "2.2 Git Skills\nYou will be required to use Git and GitHub to complete the assignments in the book. In particular, Git and GitHub are uses for the generation and submission of forecasts to the NEON Ecological Forecasting Challenge. Below are instructions for setting up Git and GitHub on your computer.\n\nCreate a GitHub user account at https://github.com, if you don’t already have one. Here is advice about choosing a user name, because choosing a good user name is critical.\nGo to Rstudio and install the usethis package.\n\ninstall.packages(\"usethis\")\n\nRun the following command where you replace the user.email and user.name with the email used for GitHub and your GitHub user name. You can learn more about the command here\n\nlibrary(usethis)\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\nIf you get an error at this step it is likely due to your computer not having Git. Follow the instructions here about installing Git\n\nSet up your GitHub credentials on your computer. Follow the instructions here about using usethis::create_github_token() and gitcreds::gitcreds_set() functions. Also, save your GitHub PAT to a password manager so that you can find it in the future (in case you need to interact with GitHub from a different computer).\nGo to Canvas and get the link to accept the assignment. Copy and paste the link in a web browser. Accept the assignment.\nGo to your assignment at https://github.com/frec-3044-Spring2024. Click on repository.\nUnder the green “Code” button, select the local tab, and copy the https link.\nOpen Rstudio on your computer and create a new project. First, File -&gt; New Project -&gt; Version Control -&gt; Git. Paste the URL from you repo in the first box, hit tab to fill in the repo name in the second, and then use Browse to select where you want the project on your computer (I recommend having a directory on your computer where you keep all repositories we use in the class).\nYour project will load. Then go to File -&gt; New -&gt; New File -&gt; Quarto Document\nIn the prompt use Title = “Assignment 1” and Author = [Your name]\nSave file as “assignment1.qmd” in the assignment subdirectory of the Project.\nCommit your assignment1.qmd file using the Git tab at the top right pane using a useful commit message. You will need to check the box for the files that you want to commit. A useful message helps you broadly remember what you did to the files that are included in the commit. The Git tab may not be in the top right panel if you have moved the panels around.\nFind the Sources / Visual buttons right above the document. Select Source (which is the code view).\nCopy the code chunk on lines 21-24 and paste at end of document. Change to echo: TRUE.\nFind the following code at the top\n\nformat: html:\nand change to so that all the necessary files are saved in a single html file.\nformat:   \n  html:\n    embed-resources: true\n\nFind the Render (found above the document) button and click it to render the document a to an html document. You will see a file named “assignment1.html” appear. The html is like webpage version of your code. If your have a directory called assignment1_files then you did not do step 15 correctly.\nClick on the “assignment1.html” in your “Files” pane and select “View in Web Browser”. Confirm that it looks as expected.\nCommit the updated .qmd and new .html files to git.\nPush to your repository on GitHub.\nGo to https://github.com/frec-3044-Spring2024 and click on your repository. You should also see three commits: 2 were committed by you and 1 was committed by the github classroom bot. The Github classroom bot commit is the automatic commit that occurred when you accepted the assignment.\nGo to the course on Canvas and upload the .html file to the assignment.\n\nIf you are having issues (i.e., your computer does not seem to have Git installed), here is an excellent resource to help you debug your git + Rstudio issues."
  },
  {
    "objectID": "background-skills.html#docker-skills",
    "href": "background-skills.html#docker-skills",
    "title": "2  Background skills",
    "section": "2.3 Docker Skills",
    "text": "2.3 Docker Skills\nDocker is tool that the activities in the book use for improving the reproduciblity and automation of data analysis and forecasting workflows. Below are the instructions for setting up and interacting with a Docker container (instructions are from Freya Olsson’s workshop)\nGo to https://docs.docker.com/get-docker/ to install the relevant install for your platform (available for PC, Mac and Linux). Also see https://docs.docker.com/desktop/.\nNOTE: * If you’re running Windows, you will need WSL (Windows Subsystem for Linux) * If you’re running a Linux distribution, you may have to enable Viritualization on your computer (see here)"
  },
  {
    "objectID": "background-skills.html#running-a-docker-container",
    "href": "background-skills.html#running-a-docker-container",
    "title": "2  Background skills",
    "section": "2.4 Running a docker container",
    "text": "2.4 Running a docker container\n\nLaunch Docker Desktop (either from the Command Line or by starting the GUI)\nAt the command line run the following command which tells docker to run the container with the name eco4cast/rocker-neon4cast that has all the packages and libraries installed already. The PASSWORD=yourpassword sets a simple password that you will use to open the container. The -ti option starts both a terminal and an interactive session.\n\ndocker run --rm -ti -e PASSWORD=yourpassword -p 8787:8787 eco4cast/rocker-neon4cast\nThis can take a few minutes to download and install. It will be quicker the next time you launch it.\n\nOpen up a web browser and navigate to http://localhost:8787/\nEnter the username: rstudio and password: yourpassword\nYou should see a R Studio interface with all the packages etc. pre-installed and ready to go.\n\nYou can close this localhost window (and then come back to it) but if you close the container from Docker (turn off your computer etc.) any changes will be lost unless you push them to Github or exported to your local environment."
  },
  {
    "objectID": "setting-the-stage.html",
    "href": "setting-the-stage.html",
    "title": "Introduction to forecasting",
    "section": "",
    "text": "The first section of the book is focused on introducing key concepts in ecological forecasting and empowering you to generate your own forecast. It uses a combination of Rshiny applications and R coding assignments."
  },
  {
    "objectID": "intro-ecoforecast.html",
    "href": "intro-ecoforecast.html",
    "title": "3  Introduction to Ecological Forecasting",
    "section": "",
    "text": "Ecological forecasting is a tool that can be used for understanding and predicting changes in populations, communities, and ecosystems. Ecological forecasting is an emerging approach which provides an estimate of the future state of an ecological system with uncertainty, allowing society to prepare for changes in important ecosystem services. Ecological forecasters develop and update forecasts using the iterative forecasting cycle, in which they make a hypothesis of how an ecological system works; embed their hypothesis in a model; and use the model to make a forecast of future conditions. When observations become available, they can assess the accuracy of their forecast, which indicates if their hypothesis is supported or needs to be updated before the next forecast is generated. In this module, students will apply the iterative forecasting cycle to develop an ecological forecast for a National Ecological Observation Network (NEON) site. Students will use NEON data to build an ecological model that predicts primary productivity. Using their calibrated model, they will learn about the different components of a forecast with uncertainty and compare productivity forecasts among NEON sites. The overarching goal of this module is for students to learn fundamental concepts about ecological forecasting and build a forecast for a NEON site. Students will work with an R Shiny interface to visualize data, build a model, generate a forecast with uncertainty, and then compare the forecast with observations. The A-B-C structure of this module makes it flexible and adaptable to a range of student levels and course structures.\nBackground presentation https://cdn.serc.carleton.edu/files/eddie/teaching_materials/modules/instructor_powerpoint_module5.pptx\nRshiny App https://macrosystemseddie.shinyapps.io/module5/\nAssignment https://cdn.serc.carleton.edu/files/eddie/teaching_materials/modules/student_handout_module5.docx\nThis module was developed by Moore, T.N., Lofton, M.E., C.C. Carey, and R.Q. Thomas. 03 July 2023. Macrosystems EDDIE: Introduction to Ecological Forecasting. Macrosystems EDDIE Module 5, Version 2. http://module5.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nReference describing the effectiveness of the module:\nMoore, T.N., R.Q. Thomas, W.M. Woelmer, C.C Carey. 2022. Integrating ecological forecasting into undergraduate ecology curricula with an R Shiny application-based teaching module. Forecasting 4:604-633. https://doi.org/10.3390/forecast4030033"
  },
  {
    "objectID": "first-forecast.html",
    "href": "first-forecast.html",
    "title": "4  First forecast: Introduction to NEON Ecological Forecasting Challenge",
    "section": "",
    "text": "This module was developed by Olsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas\nhttps://github.com/OlssonF/NEON-forecast-challenge-workshop\nThis tutorial introduces participants to key concepts in ecological forecasting and provides hands-on materials for submitting forecasts to the Ecological Forecasting Initiative (EFI) - National Ecological Observatory Network (NEON) Forecasting Challenge (hereafter, Challenge). The tutorial has been developed and used with more than 300 participants in both classrooms and workshops and provides the ecological understanding, workflows, and tools to enable ecologists with minimal forecasting experience to participate in the Challenge via a hands-on code-based tutorial. This R-based tutorial introduces participants to a near-term, iterative forecasting workflow that includes obtaining observations from NEON, developing a simple forecasting model, generating a forecast, and submitting the forecast to the Challenge, as well as analyzing forecast performance once new observations become available for evaluation. The overarching aim of this tutorial is to lower the barrier to ecological forecasting and empower participants to continue to innovate and develop their own ecological forecasts for the Challenge into the future.\nThe assignment will require you to create a GitHub repository with your completed code from the NEON Forecasting Challenge workshop. You will build on this repository throughout the rest of section 1 of the book. You can learn more about setting up Git and GitHub in (background-skills.qmd#sec-github?)\nYou will be generating forecasts of water temperature at NEON aquatics sites.\nReference\nOlsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas. Can you predict the future? A tutorial for the National Ecological Observatory Network Ecological Forecasting Challenge. In review at Journal of Open Source Education."
  },
  {
    "objectID": "understand-uncertainty.html",
    "href": "understand-uncertainty.html",
    "title": "5  Understanding Uncertainty in Ecological Forecasts",
    "section": "",
    "text": "Ecological forecasting is a tool that can be used for understanding and predicting changes in populations, communities, and ecosystems. Ecological forecasting is an emerging approach which provides an estimate of the future state of an ecological system with uncertainty, allowing society to prepare for changes in important ecosystem services. Forecast uncertainty is derived from multiple sources, including model parameters and driver data, among others. Knowing the uncertainty associated with a forecast enables forecast users to evaluate the forecast and make more informed decisions. Ecological forecasters develop and update forecasts using the iterative forecasting cycle, in which they make a hypothesis of how an ecological system works; embed their hypothesis in a model; and use the model to make a forecast of future conditions and quantify forecast uncertainty. There are a number of approaches that forecasters can use to reduce uncertainty, which will be explored in this module.\nBackground presentation\nhttps://cdn.serc.carleton.edu/files/eddie/teaching_materials/modules/instructor_powerpoint_module6.pptx\nRmarkdown (includes assignment):\nhttps://github.com/MacrosystemsEDDIE/module6_R\nThis module was developed by Moore, T. N., Lofton, M.E., Carey, C.C. and Thomas, R. Q. 24 July 2023. Macrosystems EDDIE: Understanding Uncertainty in Ecological Forecasts. Macrosystems EDDIE Module 6, Version 2. http://module6.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016."
  },
  {
    "objectID": "second-forecast.html",
    "href": "second-forecast.html",
    "title": "6  Second forecast: Adding uncertainty to first forecast",
    "section": "",
    "text": "This module builds Chapter 4 by challenging you to update the code in your GitHub so that the forecast you are submitting has driver, parameter, and process uncertainty. You will submit the assignment as an updated GitHub repository with the code and plot of your forecast with all uncertainty sources included.\nYou will be generating forecasts of water temperature at NEON aquatics sites."
  },
  {
    "objectID": "using-data.html",
    "href": "using-data.html",
    "title": "7  Using data to improve ecological forecasts",
    "section": "",
    "text": "To be useful for management, ecological forecasts need to be both accurate enough for managers to be able to rely on them for decision-making and include a representation of forecast uncertainty, so managers can properly interpret the probability of future events. To improve forecast accuracy, we can update forecasts with observational data once they become available, a process known as data assimilation. Recent improvements in environmental sensor technology and an increase in the number of sensors deployed in ecosystems have resulted in an increase in the availability of data for assimilation to help develop and improve forecasts for natural resource management. In this module, students will develop an ecosystem model of primary productivity, use the model to generate forecasts, and then explore how assimilating different types of data at different temporal frequencies (e.g., daily, weekly) affects forecast accuracy. Finally, students will assimilate different types of data into forecasts and examine how data assimilation affects water resource management decisions.\nBackground presentation https://cdn.serc.carleton.edu/files/eddie/teaching_materials/modules/instructors_powerpoint_23sep22.v4.pptx\nRshiny App\nhttps://macrosystemseddie.shinyapps.io/module7/\nAssignment https://cdn.serc.carleton.edu/files/eddie/teaching_materials/modules/student_handout_23sep22.docx\nAs part of the book, I recommend focusing on the Rshiny version of the module. You will be introduced to the coding behind data assimilation in the second section of this book (Advanced Ecological Forecasting). You will not need to be able to write data assimilation code to complete the assignments in the first section of this book.\nThis module was developed by: Lofton, M.E., T.N. Moore, Thomas, R.Q., and C.C. Carey. 20 September 2022. Macrosystems EDDIE: Using Data to Improve Ecological Forecasts. Macrosystems EDDIE Module 7, Version 1. https://macrosystemseddie.shinyapps.io/module7. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nThis module has been peer-reviewed and included in the “On the Cutting Edge Exemplary Teaching Activities” collection."
  },
  {
    "objectID": "third-forecast.html",
    "href": "third-forecast.html",
    "title": "8  Third forecast: automatically updating second forecast",
    "section": "",
    "text": "The third forecast generation assignment will challenge you to automate the submission of your forecast. By running your forecast code each day, you will be updating your model with the latest data, which is a simple form of data assimilation.\nhttps://github.com/OlssonF/NEON-forecast-challenge-workshop/blob/main/Automate_forecasts/automation_tutorial.Rmd\nYou will submit the assignment as an updated GitHub repository that is successfully submitting automated forecasts.\nYou will be generating forecasts of water temperature at NEON aquatics sites."
  },
  {
    "objectID": "decision-making.html",
    "href": "decision-making.html",
    "title": "9  Using Ecological Foreccasts to Guide Decision Making",
    "section": "",
    "text": "Because of increased variability in populations, communities, and ecosystems due to land use and climate change, there is a pressing need to know the future state of ecological systems across space and time. Ecological forecasting is an emerging approach which provides an estimate of the future state of an ecological system with uncertainty, allowing society to preemptively prepare for fluctuations in important ecosystem services. However, forecasts must be effectively designed and communicated to those who need them to realize their potential for protecting natural resources. In this module, students will explore real ecological forecast visualizations, identify ways to represent uncertainty, make management decisions using forecast visualizations and learn decision support techniques. Lastly, students will then customize a forecast visualization for a specific forecast user’s decision needs. The overarching goal of this module is for students to understand how forecasts are connected to decision-making of forecast users, or the managers, policy-makers, and other members of society who use forecasts to inform decision-making.\nBackground presentation https://cdn.serc.carleton.edu/files/eddie/teaching_materials/modules/instructor_powerpoint_module8.pptx\nRshiny App (includes Assignment)\nhttps://macrosystemseddie.shinyapps.io/module8/\nThis module was developed by W.M. Woelmer, R.Q. Thomas, T.N. Moore and C.C. Carey. 21 January 2021. Macrosystems EDDIE: Using Ecological Forecasts to Guide Decision-Making. Macrosystems EDDIE Module 8, Version 1. http://module8.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nThis module has been peer-reviewed and included in the “On the Cutting Edge Exemplary Teaching Activities” collection.\nReference describing the effectiveness of the module:\nWoelmer, W.M., T.N. Moore, M.E. Lofton, R.Q. Thomas, and C.C. Carey. 2023. Embedding communication concepts in forecasting training increases students’ understanding of ecological uncertainty Ecosphere 14: e4628 https://doi.org/10.1002/ecs2.4628"
  },
  {
    "objectID": "visualizing.html#information-on-scoring-metrics",
    "href": "visualizing.html#information-on-scoring-metrics",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.1 Information on scoring metrics",
    "text": "10.1 Information on scoring metrics"
  },
  {
    "objectID": "visualizing.html#rmse",
    "href": "visualizing.html#rmse",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.2 RMSE",
    "text": "10.2 RMSE\nPending"
  },
  {
    "objectID": "visualizing.html#continuous-ranked-probability-score",
    "href": "visualizing.html#continuous-ranked-probability-score",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.3 Continuous Ranked Probability Score",
    "text": "10.3 Continuous Ranked Probability Score\nForecasts will be scored using the continuous ranked probability score (CRPS), a proper scoring rule for evaluating forecasts presented as distributions or ensembles (Gneiting & Raftery 2007). The CRPS compares the forecast probability distribution to that of the validation observation and assigns a score based on both the accuracy and precision of the forecast. We will use the ‘crps_sample’ function from the scoringRules package in R to calculate the CRPS for each forecast.\nWe will generate a combined score for all locations and forecast horizons. Forecasts will also be evaluated using the CRPS at each time-step in the forecast horizon and each location included in the forecasts.\nImportantly, we use the convention for CRPS where zero is lowest and best possible score, therefore teams want to achieve the lowest score. CPRS can be also expressed as a negative number with zero as highest and best possible score (Gneiting & Raftery 2007). The scoringRules package that we use follows the 0 or greater convention.\n\nlibrary(scoringRules)\nlibrary(tidyverse)\n\n\n10.3.1 Example of a CRPS calculation from an ensemble forecast\nThe following uses Equation 2 in Jordan, Kruger, and Lerch 2018\nFirst, create a random sample from a probability distribution. This is the “forecast” for a particular point in time. For simplicity, we will use a normal distribution with a mean of 8 and standard deviation of 1\n\nx &lt;- rnorm(1000, mean = 8, sd = 1.0)\n\nSecond, we have our data point (i.e., the target). We will set it to zero as well\n\ny &lt;- 8\n\nNow calculate CRPS using Equation 2\n\ns &lt;- 0\nfor(i in 1:length(x)){\n  for(j in 1:length(x)){\n    s &lt;- s + abs(x[i] - x[j])\n  }\n}\ncrps_equation_2 &lt;- mean(abs(x - y)) - s / (2 * length(x)^2)\ncrps_equation_2\n\n[1] 0.2337349\n\n\nNow calculate using the crps_sample() function in the scoringRules package\n\ncrps_sample(y = y, dat = x)\n\n[1] 0.2337349\n\n\n\n\n10.3.2 Exploring the scoring surface\nNow lets see how the CRPS changes as the mean and standard deviation of the forecasted distribution change\nFirst, set vectors for the different mean and SD values we want to explore\n\nsample_mean &lt;- seq(4, 12, 0.1)\nsample_sd &lt;- seq(0.1, 10, 0.1)\n\nSecond, set our observed value to 8 for simplicity\n\ny &lt;- 8\n\nNow calculate the CRPS at each combination of forest mean and SD\n\ncombined &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd)))\nfor(i in 1:length(sample_mean)){\n  for(j in 1:length(sample_sd)){\n    sample &lt;- rnorm(10000, sample_mean[i], sample_sd[j])\n    combined[i, j] &lt;- crps_sample(y = y, dat = sample)\n  }\n}\n\nFinally, visualize the scoring surface with the observed value represented by the red line\n\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined),nlevels = 20, xlab = \"Mean\", ylab = \"SD\")\nabline(v = y, col = \"red\")\n\n\n\n\nThe contour surface highlights the trade-off between the mean and standard deviation.\n\n\n10.3.3 CRPS from the Normal Distribution\nIf the distributional forecast is a normal distribution represented by a mean \\(\\mu\\) and standard deviation \\(\\sigma\\), an ensemble of predictions is not needed to evaluate CRPS because we can take advantage of the analytic solution to CRPS under the normal assumption (Equation 4 from Gneiting et al. 2005)\nEquation 5 from Gneiting et al. (2005) gives\n\\[\\begin{align*}\nCRPS(N(\\mu, \\sigma^2) | y) = \\sigma \\left( \\frac{y - \\mu}{\\sigma} \\left( 2 \\Phi\\left(  \\frac{y - \\mu}{\\sigma} \\right) - 1 \\right)  + 2 \\phi \\left(  \\frac{y - \\mu}{\\sigma} \\right) - \\frac{1}{\\sqrt{\\pi}} \\right)\n\\end{align*}\\]\nfor \\(\\Phi(\\cdot)\\) and \\(\\phi(\\cdot)\\) the standard normal CDF and PDF, respectively. Therefore, if the forecast distribution is truly a normal distribution (often this isn’t true in forecasts that only report a mean and sd) a simplified score can be applied as follows:\n\nsample_mean &lt;- seq(4, 12, 0.1)\nsample_sd &lt;- seq(0.1, 10, 0.1)\n\ncombined_norm &lt;- array(NA, dim = c(length(sample_mean), length(sample_sd)))\nfor(i in 1:length(sample_mean)){\n  for(j in 1:length(sample_sd)){\n    combined_norm[i, j] &lt;- crps_norm(y = y, mean = sample_mean[i], sd = sample_sd[j])\n  }\n}\n\nFinally, visualize the scoring surface with the observed value represented by the red line.\n\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\")\nabline(v = y, col = \"red\")\n\n\n\n\nNote that at a given value of the sd, the lowest score is achieved at \\(\\mu = y\\) as shown for each of the blue lines where the minimum value of the score across each blue line is at the red line. This behavior makes sense because the CRPS is a score that rewards accuracy and precision. Thus, for any given level of precision (represented by the standard deviation), CRPS is optimized by producing the most accurate prediction of the distribution’s location.\n\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\")\nabline(v = y, col = \"red\")\nabline(h = 2.5, col = \"blue\")\nabline(h = 4.3, col = \"blue\")\nabline(h = 6.8, col = \"blue\")\n\n\n\n\nInterestingly, for a given mean \\(\\mu \\neq y\\) we find a pattern that makes intuitive sense given the goal of CRPS to produce forecasts that are both accurate and precise. For a given amount of bias in the prediction (i.e., given a \\(\\mu \\neq y\\)), the optimal score is achieved by a standard deviation that slightly larger than the bias.\n\nlayout(matrix(1:4, 2, 2, byrow = TRUE))\n## plots for mu = 7\nmu &lt;- 7\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\", main = paste0(\"CRPS contour given mu = \", mu))\nabline(v = mu, col = \"red\")\nmin_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))]\nabline(h = min_sd, col = \"blue\")\nplot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = 'l', main = paste0(\"CRPS profile given mu = \", mu))\nabline(v = min_sd, col = \"blue\")\n## plots for mu = 11\nmu &lt;- 11\ncontour(x = sample_mean, y = sample_sd, z = as.matrix(combined_norm), nlevels = 20, xlab = \"Mean\", ylab = \"SD\", main = paste0(\"CRPS contour given mu = \", mu))\nabline(v = mu, col = \"red\")\nmin_sd &lt;- sample_sd[which.min(crps_norm(y, mean = mu, sd = sample_sd))]\nabline(h = min_sd, col = \"blue\")\nplot(sample_sd, crps_norm(y, mean = mu, sd = sample_sd), type = 'l', main = paste0(\"CRPS profile given mu = \", mu))\nabline(v = min_sd, col = \"blue\")\n\n\n\n\nNext, we plot the relationship between a given value of \\(\\mu\\) and the \\(\\sigma\\) that produces the optimal CRPS. This looks like a linear relationship.\n\noptimal_sd &lt;- rep(0, length(sample_mean))\nfor (i in 1:length(sample_mean)) {\n  optimal_sd[i] &lt;- sample_sd[which.min(crps_norm(y, mean = sample_mean[i], sd = sample_sd))]\n}\nplot(sample_mean, optimal_sd, type = 'l')\n\n\n\n\nLet’s estimate the slope of the relationship. It looks like the optimal \\(sd\\) for a normal distribution forecast that is biased by \\(|y - \\mu|\\) is \\(sd = 1.2|y - \\mu|\\) which makes sense as this would put the true value in a region of high probability.\n\ncoef(lm(optimal_sd[sample_mean &gt; 0] ~ sample_mean[sample_mean &gt; 0]))\n\n                 (Intercept) sample_mean[sample_mean &gt; 0] \n                2.430864e+00                -1.478665e-16"
  },
  {
    "objectID": "visualizing.html#crps-tutorial",
    "href": "visualizing.html#crps-tutorial",
    "title": "10  Visualizing and evaluating forecasts",
    "section": "10.4 CRPS tutorial",
    "text": "10.4 CRPS tutorial\nThis provide a tutorial for the Continuous Ranked Probability Score (CRPS), a proper scoring rule used to evaluate probabilistic forecasts.\nAt its core, CRPS presents the difference between a cumulative probability distribution generated from the forecast and the data.\n\n10.4.1 Set up data\nWe will start with the calculation of CRPS for a single forecast and observation pair.\n\nobs &lt;- 0\n\nOur forecast is a sample (n = 100) from a normal distribution with mean of 0 and standard deviation of 1\n\nforecast &lt;- rnorm(100, mean = 0, sd = 1)\n\n\ndy &lt;- 0.5\ny &lt;- seq(-3,3, dy)\n\n\nforecast_cdf_function &lt;- ecdf(forecast)\nforecast_cdf &lt;- forecast_cdf_function(y)\nobs_cdf &lt;- as.numeric((y &gt; obs[i]))\n\n\ndf &lt;- tibble(y = y,\n           cdf = forecast_cdf,\n           variable = \"forecast\")\n\ndf &lt;- bind_rows(df, \n                tibble(y = y,\n           cdf = obs_cdf,\n           variable = \"observation\"))\n\n\nggplot(df, aes(x = y, y = cdf, color = variable)) + geom_line()\n\n\n\n\n\nerror &lt;- (forecast_cdf - obs_cdf)^2\n\n\ntibble(y = y,\n       error = error) |&gt; \n  ggplot(aes(x = y, y = error)) + geom_line()\n\nWarning: Removed 13 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncdf_diff_area &lt;- error * dy\n\n\ntibble(y = y,\n       area = cdf_diff_area) |&gt; \n  ggplot(aes(x = y, y = area)) + geom_line()\n\n\n\n\n\ncrps &lt;- sum(cdf_diff_area)\n\n\n\n10.4.2 Comparing forecasts\n\ncalculate_crps &lt;- function(obs, forecast, dy = 0.01) {\n  \n  y &lt;- seq(-10,10, dy)\n  forecast_cdf &lt;- ecdf(forecast)\n  cdf_diff &lt;- (forecast_cdf(y) - as.numeric((y &gt; obs)))^2 * dy\n  crps &lt;- sum(cdf_diff)\n  \n  return(crps)\n}\n\n\nforecast1 &lt;- rnorm(100, mean = 0, sd = 4)\nforecast2 &lt;- rnorm(100, mean = 1, sd = 2)\nforecast3&lt;- rnorm(100, mean = 0, sd = 0.1)\nforecast4&lt;- rnorm(100, mean = 1, sd = 0.1)\n\n\ndy &lt;- 0.001\ny &lt;- seq(-10,10, dy)\n\nforecast1_cdf_function &lt;- ecdf(forecast1)\nforecast2_cdf_function &lt;- ecdf(forecast2)\nforecast3_cdf_function &lt;- ecdf(forecast3)\nforecast4_cdf_function &lt;- ecdf(forecast4)\n\ndf &lt;- tibble(y1 = c(y,y,y,y),\n             cdf = c(forecast1_cdf_function(y),\n                     forecast2_cdf_function(y),\n                     forecast3_cdf_function(y),\n                     forecast4_cdf_function(y)),\n             variable = c(rep(paste0(\"mean = 0, sd = 4, CRPS: \",calculate_crps(obs, forecast1)), length(y)),\n                          rep(paste0(\"mean = 1, sd = 1 CRPS: \",calculate_crps(obs, forecast2)), length(y)), \n                          rep(paste0(\"mean = 0, sd = 0.1 CRPS: \",calculate_crps(obs, forecast3)), length(y)),\n                          rep(paste0(\"mean = 1, sd = 0.1 CRPS: \",calculate_crps(obs, forecast4)), length(y))))\n\ndf &lt;- bind_rows(df, \n                tibble(y1 = y,\n           cdf = as.numeric((y &gt; obs)),\n           variable = \"observation\"))\n\n\nggplot(df, aes(x = y1, y = cdf, color = variable)) + geom_line()\n\n\n\n\n\nc(scoringRules::crps_sample(y = obs, dat = forecast1),scoringRules::logs_sample(y = obs, dat = forecast1)) \n\n[1] 0.9647685 2.4228407\n\nc(scoringRules::crps_sample(y = obs, dat = forecast2),scoringRules::logs_sample(y = obs, dat = forecast2)) \n\n[1] 0.6722863 1.7361076\n\nc(scoringRules::crps_sample(y = obs, dat = forecast3),scoringRules::logs_sample(y = obs, dat = forecast3)) \n\n[1]  0.02284421 -1.32656630\n\nc(scoringRules::crps_sample(y = obs, dat = forecast4),scoringRules::logs_sample(y = obs, dat = forecast4)) \n\n[1]   0.937594 169.182985\n\n\n\ncalculate_crps &lt;- function(obs, forecast, dy) {\n  n &lt;- nrow(forecast)\n  crps &lt;- rep(NA, n)\n  y &lt;- seq(-5,5, dy)\n  \n  for (i in 1:n) {\n    forecast_cdf &lt;- ecdf(forecast[i, ])\n    cdf_diff &lt;- (forecast_cdf(y) - as.numeric((y &gt; obs[i])))^2 * dy\n    crps[i] &lt;- sum(cdf_diff)\n  }\n  \n  return(crps)\n}"
  },
  {
    "objectID": "project1.html#intro-to-tidymodels",
    "href": "project1.html#intro-to-tidymodels",
    "title": "11  Project 1: Create model and submit",
    "section": "11.1 Intro to Tidymodels",
    "text": "11.1 Intro to Tidymodels\nhttps://github.com/frec-3044/machine-learning-template"
  },
  {
    "objectID": "project1.html#intro-to-fable-models",
    "href": "project1.html#intro-to-fable-models",
    "title": "11  Project 1: Create model and submit",
    "section": "11.2 Intro to Fable Models",
    "text": "11.2 Intro to Fable Models\nhttps://otexts.com/fpp3/"
  },
  {
    "objectID": "project1.html#how-to-submit-a-forecast",
    "href": "project1.html#how-to-submit-a-forecast",
    "title": "11  Project 1: Create model and submit",
    "section": "11.3 How to submit a forecast",
    "text": "11.3 How to submit a forecast\nhttps://projects.ecoforecast.org/neon4cast-ci/instructions.html"
  },
  {
    "objectID": "advanced-forecasting.html",
    "href": "advanced-forecasting.html",
    "title": "Advanced forecasting",
    "section": "",
    "text": "The advanced forecasting section applies the concepts of the first section to a process model. The topics include an introduction to process modeling (particularly dynamic models), model fitting through likelihood and Bayesian techniques, and data assimilation using the particular filter.\nMaterial in this section is partially dervived from the forecasting activities associated with Mike Dietze’s Ecological Modeling book (https://github.com/EcoForecast/EF_Activities) and his terrestrial forecasting tutorial that was created for the NEON Ecological Forecasting Challenge (https://github.com/mdietze/FluxCourseForecast)."
  },
  {
    "objectID": "process-model.html#overview-of-model",
    "href": "process-model.html#overview-of-model",
    "title": "12  Process model",
    "section": "12.1 Overview of model",
    "text": "12.1 Overview of model\n\ntimestep: 1 day\ncurrency: MgC/ha\ndimension: 0-D\n\nPhotosynthesis - LAI\nAutotrophic respiration\nAllocation\nLitterfall\nMortality\nHeterotrophic respiration\nNet Ecosystem Exchange"
  },
  {
    "objectID": "process-model.html#model-as-a-function",
    "href": "process-model.html#model-as-a-function",
    "title": "12  Process model",
    "section": "12.2 Model as a function",
    "text": "12.2 Model as a function\n\nOne time-step at a time\n\n\nforest_model &lt;- function(t, states, parms, inputs){\n  \n  ens_members &lt;- nrow(states)\n  \n  inputs_temp &lt;- inputs[, 1]\n  inputs_PAR &lt;- inputs[, 2]\n  inputs_doy &lt;- inputs[, 3]\n  \n  # Unit Conversion: umol/m2/sec to Mg/ha/timestep\n  k &lt;- 1e-6 * 12 * 1e-6 * 10000 * 86400 #mol/umol*gC/mol*Mg/g*m2/ha*sec/timestep\n\n\n  \n  # Convert leaf carbon to leaf area index\n  lai &lt;- states[, 1] * parms$SLA * 0.1  #0.1 is conversion from Mg/ha to kg/m2\n  \n  # photosynthesis\n  #Calculate gross primary productivity as a function of LAI and PAR (convert to daily)\n  ## pmax ensures GPP never goes negative\n  gpp &lt;- pmax(0, k * parms$alpha * (1 - exp(-0.5 * lai)) * inputs_PAR)\n\n  ## autotropic respiration & allocation to leaves and wood\n  ra &lt;- gpp * parms$Ra_frac\n  npp &lt;- gpp - ra\n  \n  leaf_alloc &lt;- npp * parms$leaf_frac\n  wood_alloc &lt;- npp * (1 - parms$leaf_frac)\n\n  # Calculate soil respiration using a base rate and a Q10 temperature modifier \n  #(more soil = more respiration, hotter = more respiration)\n  ## pmax ensures SOM never goes negative\n  rh &lt;- pmax(k * parms$Rbasal * states[, 3] * parms$Q10 ^ (inputs_temp / 10), 0) \n\n  ## turnover\n\n  #calculate the daily rate of leaf drop\n  litterfall &lt;- states[ , 1] * (parms$litterfall_rate * (365/ (params$litterfall_length)))\n  #Not leaf drop if outside the day of year window\n  litterfall[!(inputs_doy &gt; params$litterfall_start & inputs_doy[1] &lt; (params$litterfall_start + params$litterfall_length))] &lt;- 0.0\n  \n  #kill trees\n  mortality &lt;- states[ , 2] * parms$mortality\n  \n  #Change in states\n  dleaves_dt &lt;- leaf_alloc  - litterfall\n  dwood_dt &lt;- wood_alloc  - mortality\n  dSOM_dt &lt;- litterfall + mortality - rh\n  \n  #Update states by adding the change\n  states[, 1] &lt;- states[, 1] + dleaves_dt\n  states[, 2] &lt;- states[, 2] + dwood_dt\n  states[, 3] &lt;- states[, 3] + dSOM_dt\n  \n  ## Add normally distributed random noise to states\n  ## pmax ensures states never goes negative\n  states[, 1] &lt;- pmax(rnorm(ens_members, states[, 1] , parms$sigma.leaf), 0)\n  states[, 2] &lt;- pmax(rnorm(ens_members, states[, 2], parms$sigma.stem), 0)\n  states[, 3] &lt;- pmax(rnorm(ens_members, states[, 3], parms$sigma.soil), 0)\n  \n  #Dervived variables (LAI and net ecosystem exchange)\n  lai &lt;- states[, 1]  * parms$SLA * 0.1\n  nee &lt;- -(gpp - ra - rh)\n  \n  return(cbind(state1 = states[, 1],\n              state2 = states[, 2],\n              state3 = states[, 3],\n              lai = lai,\n               gpp = gpp ,\n               nee = nee,\n               ra =  ra,\n               npp_w = wood_alloc,\n               npp_l = leaf_alloc,\n               rh = rh,\n               litterfall = litterfall,\n               mortality = mortality))\n\n}"
  },
  {
    "objectID": "process-model.html#set-up-model-run",
    "href": "process-model.html#set-up-model-run",
    "title": "12  Process model",
    "section": "12.3 Set up model run",
    "text": "12.3 Set up model run\n\n12.3.1 Time frame, site id, and number of ensembles\nWe are doing a deterministic simulation (no uncertainty) so we only have one ensemble member\n\nsite &lt;- \"TALL\"\nens_members &lt;- 1\nsim_dates &lt;- seq(as_date(\"2020-09-30\"), Sys.Date() - lubridate::days(1), by = \"1 day\")\n\n\n\n12.3.2 Set drivers\nThere is a custom function to access the weater drivers. You can learn more about the function in the appendix (add link)\n\ninputs &lt;- get_historical_met(site = site, sim_dates, use_mean = TRUE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\n\n\n12.3.3 Set parameters\n\n#Set parameters\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(60, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.stem &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n\n\n12.3.4 Set inital condition\nSet the starting point of the simulation. All states need a starting point.\n\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- 5\noutput[1, , 2] &lt;- 140\noutput[1, , 3] &lt;- 140"
  },
  {
    "objectID": "process-model.html#run-model",
    "href": "process-model.html#run-model",
    "title": "12  Process model",
    "section": "12.4 Run model",
    "text": "12.4 Run model\nLoop over the sim_dates. The loop starts on index of 2 because index of 1 was set as the initial conditions above. Save the output at step time-step to the output array.\n\nfor(t in 2:length(sim_dates)){\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}"
  },
  {
    "objectID": "process-model.html#plot-output",
    "href": "process-model.html#plot-output",
    "title": "12  Process model",
    "section": "12.5 Plot output",
    "text": "12.5 Plot output\nThe output_to_df converts the output into a data frame.\n\noutput_df &lt;- output_to_df(output, sim_dates, sim_name = \"baseline\")\n\nPlot the data frame.\n\noutput_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_point(aes(y = prediction, group = ensemble)) +\n  facet_wrap(~variable, scale = \"free\")"
  },
  {
    "objectID": "prop-uncertainty.html#baseline-parameteres",
    "href": "prop-uncertainty.html#baseline-parameteres",
    "title": "13  Propogating uncertainty",
    "section": "13.1 Baseline parameteres",
    "text": "13.1 Baseline parameteres\n\nens_members &lt;- 100\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(70, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.stem &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)"
  },
  {
    "objectID": "prop-uncertainty.html#baseline-initial-conditions",
    "href": "prop-uncertainty.html#baseline-initial-conditions",
    "title": "13  Propogating uncertainty",
    "section": "13.2 Baseline initial conditions",
    "text": "13.2 Baseline initial conditions\n\n#Set initial conditions\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- 5\noutput[1, , 2] &lt;- 140\noutput[1, , 3] &lt;- 140"
  },
  {
    "objectID": "prop-uncertainty.html#baseline-drivers",
    "href": "prop-uncertainty.html#baseline-drivers",
    "title": "13  Propogating uncertainty",
    "section": "13.3 Baseline drivers",
    "text": "13.3 Baseline drivers\n\ninputs &lt;- get_forecast_met(site = site, sim_dates, use_mean = FALSE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)"
  },
  {
    "objectID": "prop-uncertainty.html#parameter-uncertainity",
    "href": "prop-uncertainty.html#parameter-uncertainity",
    "title": "13  Propogating uncertainty",
    "section": "13.4 Parameter uncertainity",
    "text": "13.4 Parameter uncertainity\nUpdate parameters to include uncertainty\n\nnew_params &lt;- params\nnew_params$alpha &lt;- rnorm(ens_members, params$alpha, sd = 0.005)\n\nTODO: add parameter distributions\n\nfor(t in 2:length(sim_dates)){\n\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = new_params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\nparameter_df &lt;- output_to_df(output, sim_dates, sim_name = \"parameter_unc\")\n\n\nparameter_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\")"
  },
  {
    "objectID": "prop-uncertainty.html#process-uncertainty",
    "href": "prop-uncertainty.html#process-uncertainty",
    "title": "13  Propogating uncertainty",
    "section": "13.5 Process uncertainty",
    "text": "13.5 Process uncertainty\nChange process uncertainty parameters from being zero to a positive value\n\nnew_params &lt;- params\nnew_params$sigma.leaf &lt;- rep(0.1, ens_members)\nnew_params$sigma.stem &lt;- rep(1, ens_members) #0.01 ## wood biomass\nnew_params$sigma.soil &lt;- rep(1, ens_members)# 0.01\n\nTODO: add process distributions\n\nfor(t in 2:length(sim_dates)){\n\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = new_params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\nprocess_df &lt;- output_to_df(output, sim_dates, sim_name = \"process_unc\")\n\n\nprocess_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\")"
  },
  {
    "objectID": "prop-uncertainty.html#initial-condition-uncertainty",
    "href": "prop-uncertainty.html#initial-condition-uncertainty",
    "title": "13  Propogating uncertainty",
    "section": "13.6 Initial condition uncertainty",
    "text": "13.6 Initial condition uncertainty\nUpdate the initial starting point\n\n#Set initial conditions\nnew_output &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\nnew_output[1, , 1] &lt;- rnorm(ens_members, 5, 0.5)\nnew_output[1, , 2] &lt;- rnorm(ens_members, 140, 10)\nnew_output[1, , 3] &lt;- rnorm(ens_members, 140, 20)\n\nTODO: add initial condition distributions\n\nfor(t in 2:length(sim_dates)){\n\n  new_output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(new_output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\ninitial_conditions_df &lt;- output_to_df(new_output, sim_dates, sim_name = \"initial_unc\")\n\n\ninitial_conditions_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\")"
  },
  {
    "objectID": "prop-uncertainty.html#driver-uncertainty",
    "href": "prop-uncertainty.html#driver-uncertainty",
    "title": "13  Propogating uncertainty",
    "section": "13.7 Driver uncertainty",
    "text": "13.7 Driver uncertainty\n\nnew_inputs &lt;- get_forecast_met(site = site, sim_dates, use_mean = FALSE)\nnew_inputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\nTODO: add plot of drivers\n\nfor(t in 2:length(sim_dates)){\n\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = params, \n                               inputs = matrix(new_inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\ndrivers_df &lt;- output_to_df(output, sim_dates, sim_name = \"driver_unc\")\n\n\ndrivers_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\")"
  },
  {
    "objectID": "prop-uncertainty.html#problem-set",
    "href": "prop-uncertainty.html#problem-set",
    "title": "13  Propogating uncertainty",
    "section": "13.8 Problem Set",
    "text": "13.8 Problem Set\n\n13.8.1 Part 1\nUsing a dataset that combines each of the uncertainty dataframes into a single data frame:\n\ncombined_df &lt;- bind_rows(parameter_df, process_df, initial_conditions_df, drivers_df)\n\nAnswer with text, code, and plots the following questions\n\n1 day-ahead, what the largest source of uncertainty for a flux (nee)? for a state (wood)?\n10 days-ahead, what is the largest source of uncertainty for a flux (nee)? for a state (wood)?\n30 days-ahead, what is the largest source of uncertainty for a flux (nee)? for a state (wood)?\n\n\n\n13.8.2 Part 2\nUsing the code above as a guide, create code to estimate uncertainty based on the propagation all sources at the same time (unlike the one-at-a-time approach above).\nAnswer with text, code, and plots the following questions\n\nPlot the forecast with the combined uncertainty.\nIf you calculate the variance of the combined uncertainty and compared to the sum of the individual variances, do they match? What does it mean if they are different?"
  },
  {
    "objectID": "neon-data.html#download-data",
    "href": "neon-data.html#download-data",
    "title": "14  NEON data",
    "section": "14.1 Download data",
    "text": "14.1 Download data\nFirst, we are going to want to define the site ID. The four letter site code denotes NEON site. You can learn more about NEON sites here: https://www.neonscience.org/field-sites/explore-field-sites. The elevation, latitude , and longitude are needed to convert the diameter measurements to biomass.\n\nsite &lt;- \"TALL\"\nelevation &lt;- 166    \nlatitude &lt;- 32.95047\nlongitude &lt;- -87.393259"
  },
  {
    "objectID": "neon-data.html#wood-carbon",
    "href": "neon-data.html#wood-carbon",
    "title": "14  NEON data",
    "section": "14.2 Wood carbon",
    "text": "14.2 Wood carbon\n\n14.2.1 Calculate carbon in trees\nIn this section, we will be calculating carbon in live and dead trees at a NEON site. At the end we will have a site-level mean carbon stock in live trees for each year that was sampled from the plots that are sampling the ecosystem under the flux tower (e.g., tower plots).\nThe code below read the data directly from NEON’s cloud storage. Since the columns may change between the official releases and the provisional data, they need to be read in separately before being combined.\n\nindex &lt;-  neonstore:::neon_data(product = \"DP1.10098.001\",\n                                site = site,\n                                type=\"basic\")\n\n## Mapping and tagging table\n\ndf1 &lt;-index |&gt;\n  filter(grepl(\"mappingandtagging\", name)) |&gt; \n  filter(grepl(\"RELEASE\", release)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  select(individualID, scientificName) |&gt; \n  collect()\n\ndf2 &lt;- index |&gt;\n  filter(grepl(\"mappingandtagging\", name)) |&gt; \n  filter(!grepl(\"RELEASE\", release)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  select(individualID, scientificName) |&gt; \n  collect()\n\nmap_tag_table &lt;- bind_rows(df1,df2) |&gt; \n  group_by(individualID) |&gt; \n  slice(1) |&gt; #This is needed because some individuals change species IDs\n  ungroup()\n\n## Individual table\n\ndf1 &lt;- index |&gt;\n  filter(grepl(\"apparentindividual\", name)) |&gt; \n  filter(grepl(\"RELEASE\", release)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  select(individualID, eventID, plotID, date, stemDiameter,plantStatus, measurementHeight) |&gt; \n  collect() \n\ndf2 &lt;- index |&gt;\n  filter(grepl(\"apparentindividual\", name)) |&gt; \n  filter(!grepl(\"RELEASE\", release)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt;\n  select(individualID, eventID, plotID, date, stemDiameter,plantStatus, measurementHeight) |&gt; \n  collect()\n\nind_table &lt;- bind_rows(df1,df2) |&gt; \n  distinct()\n\n## Plot table\n\ndf1 &lt;-index |&gt;\n  filter(grepl(\"perplotperyear\", name)) |&gt; \n  filter(grepl(\"RELEASE\", release)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  select(plotID,totalSampledAreaTrees,plotType) |&gt;  \n  collect() \n\ndf2 &lt;- index |&gt;\n  filter(grepl(\"perplotperyear\", name)) |&gt; \n  filter(!grepl(\"RELEASE\", release)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(perplotperyear$url, format=\"csv\", filename = TRUE) |&gt;\n    select(plotID,totalSampledAreaTrees,plotType) |&gt;  \n  collect()\n\nplot_table &lt;- bind_rows(df1,df2) |&gt; \n    distinct(plotID, .keep_all = TRUE)\n\nThe species names in the mapping and tagging table need to be separated into the genus and species so that we can calculate the biomass.\n\ngenus_species &lt;- unlist(str_split_fixed(map_tag_table$scientificName, \" \", 3))\n\nmap_tag_table &lt;- map_tag_table %&gt;% \n  mutate(GENUS = genus_species[,1], \n         SPECIES = genus_species[,2]) \n\nSelect the key variables in each table and join into the individual table, making sure that we have GENUS, SPECIES, and measurementHeight. These will be important when we use allodb to estimate the amount of carbon.\n\ncombined_table &lt;- left_join(ind_table, map_tag_table, by = \"individualID\") %&gt;% \n  arrange(plotID,individualID)\n\ncombined_table &lt;- inner_join(combined_table, plot_table, by = c(\"plotID\")) %&gt;% \n  arrange(individualID)\n\ncombined_table_dbh &lt;- combined_table[which(combined_table$measurementHeight == 130),]\n\ncombined_table_dbh &lt;- combined_table_dbh[which(!is.na(combined_table_dbh$stemDiameter)),]\n\nTidy up the individual tree data to include only live trees from the tower plots. Also create a variable that is the year of the sample date.\n\ncombined_table_live_tower &lt;- combined_table_dbh %&gt;% \n  filter(str_detect(plantStatus,\"Live\"),\n         plotType == \"tower\") \n\nCalculate the biomass of each tree in the table. We will do this using get_biomass in the allodb package. This function takes as arguments: dbh, genus, species, coords. We have already extracted genus and species, and in fact we have already extracted dbh as well. The steps where we were subsetting based on measurement heights of 130 cm was actually subsetting to include only data that had dbh measurements.\nIn this next section, as well as a future one where we calculate dead tree carbon, we are going to make a simplfying assumption. We will assume that the below ground biomass of a tree is some fixed proportion of its above ground biomass. In our analysis, we will assume this value is \\(.3\\), but it is a parameter that can be changed.\n\nlibrary(allodb)\n\nag_bg_prop &lt;- 0.3\n\ntree_live_carbon &lt;- combined_table_live_tower %&gt;%\n        mutate(ag_tree_kg = get_biomass(\n          dbh = combined_table_live_tower$stemDiameter,\n          genus = combined_table_live_tower$GENUS,\n          species = combined_table_live_tower$SPECIES,\n          coords = c(longitude, latitude)\n          ),\n         bg_tree_kg = ag_tree_kg * ag_bg_prop, ## assumption about ag to bg biomass\n         tree_kgC = (ag_tree_kg + bg_tree_kg) * 0.5) ## convert biomass to carbon\n\nCalculate the plot level biomass summing up the tree biomass in a plot and dividing by the area of plot.\n\nmeasurement_dates &lt;- tree_live_carbon |&gt; \n  summarise(measure_date = max(date), .by = eventID)\n\n plot_live_carbon &lt;-  tree_live_carbon %&gt;% \n   left_join(measurement_dates, by = \"eventID\") |&gt; \n    mutate(treeC_kgCm2 = (tree_kgC)/(totalSampledAreaTrees)) |&gt; \n    summarise(plot_kgCm2 = sum(treeC_kgCm2, na.rm = TRUE), .by = c(\"plotID\", \"measure_date\"))\n\nVisualize the plot level biomass\n\nggplot(plot_live_carbon, aes(x = measure_date, y = plot_kgCm2, color = plotID)) + \n  geom_point() +\n  geom_line()\n\n\n\n\nDetermine the set of plots that are measured each year (a subset, n = 5) are measured each year, while all the plots are measured every 5 years.\n\nlast_plots &lt;- plot_live_carbon |&gt; \n  filter(measure_date == max(measure_date)) |&gt; \n  pull(plotID)\n\nsite_live_carbon &lt;- plot_live_carbon |&gt; \n  filter(plotID %in% last_plots) |&gt; \n  pivot_wider(names_from = plotID, values_from = plot_kgCm2) |&gt; \n  na.omit() |&gt; \n  pivot_longer(-measure_date, names_to = \"plotID\", values_to = \"plot_kgCm2\") |&gt; \n    group_by(measure_date) %&gt;%\n    summarize(mean_kgCperm2 = mean(plot_kgCm2, na.rm = TRUE),\n              sd_kgCperm2 = sd(plot_kgCm2))\n\nPlot the site level carbon\n\nggplot(site_live_carbon, aes(x = measure_date, y = mean_kgCperm2)) + \n  geom_point() + \n  geom_errorbar(aes(ymin=mean_kgCperm2-sd_kgCperm2, ymax=mean_kgCperm2+sd_kgCperm2), width=.2,\n                 position=position_dodge(0.05))\n\n\n\n\n\n\n14.2.2 Calculate carbon in dead trees\nWe will now use allodb to extract the carbon in dead trees.\n\ncombined_table_dead_tower &lt;- combined_table_dbh %&gt;% \n  filter(grepl(\"Standing dead\",plantStatus),\n         plotType == \"tower\") \n\nCalculate the biomass of each tree in the table. This assumes that standing dead trees have the same carbon as a live tree (which is an incorrect assumption).\n\ntree_dead_carbon &lt;- combined_table_dead_tower %&gt;%\n    mutate(ag_tree_kg = get_biomass(\n          dbh = combined_table_dead_tower$stemDiameter,\n          genus = combined_table_dead_tower$GENUS,\n          species = combined_table_dead_tower$SPECIES,\n          coords = c(longitude, latitude)\n          ),\n         bg_tree_kg = ag_tree_kg * ag_bg_prop,\n         tree_kgC = (ag_tree_kg + bg_tree_kg) * 0.5)\n\nCalculate the plot level carbon\n\nmeasurement_dates &lt;- tree_dead_carbon |&gt; \n  summarise(measure_date = max(date), .by = eventID)\n\n plot_dead_carbon &lt;-  tree_dead_carbon |&gt; \n   left_join(measurement_dates, by = \"eventID\") |&gt; \n    mutate(treeC_kgCm2 = (tree_kgC)/(totalSampledAreaTrees)) |&gt; \n    summarise(plot_kgCm2 = sum(treeC_kgCm2, na.rm = TRUE), .by = c(\"plotID\", \"measure_date\"))\n\nVisualize plot level carbon\n\nggplot(plot_dead_carbon, aes(x = measure_date, y = plot_kgCm2, color = plotID)) + \n  geom_point() +\n  geom_line()\n\n\n\n\nCalculate site level carbon\n\n site_dead_carbon &lt;- plot_dead_carbon %&gt;%\n    filter(plotID %in% last_plots) |&gt; \n    group_by(measure_date) %&gt;%\n    summarize(mean_kgCperm2 = mean(plot_kgCm2, na.rm = TRUE),\n              sd_kgCperm2 = sd(plot_kgCm2))\n\nVisualize site level carbon\n\nggplot(site_dead_carbon, aes(x = measure_date, y = mean_kgCperm2)) + \n  geom_point() +\n  geom_line()"
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-trees-on-the-ground-coarse-woody-debris",
    "href": "neon-data.html#calculate-carbon-in-trees-on-the-ground-coarse-woody-debris",
    "title": "14  NEON data",
    "section": "14.3 Calculate carbon in trees on the ground (coarse woody debris)",
    "text": "14.3 Calculate carbon in trees on the ground (coarse woody debris)\nThe data needed to calculate carbon in trees that are laying on the ground are in two NEON data products.\n\nindex &lt;-  neonstore:::neon_data(product = \"DP1.10014.001\",\n                                site = site,\n                                type=\"basic\")\n\ncdw_density &lt;- index |&gt;\n  filter(grepl(\"cdw_densitydisk\", name)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  collect()\n\nlog_table &lt;- index |&gt;\n  filter(grepl(\"cdw_densitylog\", name)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  collect()\n\nindex &lt;-  neonstore:::neon_data(product = \"DP1.10010.001\",\n                                site = site,\n                                type=\"basic\")\n\ncdw_tally &lt;- index |&gt; \n  filter(grepl(\"cdw_fieldtally\", name)) |&gt; \n  pull(url) |&gt;\n  #duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt;\n  read_csv(show_col_types = FALSE)\n\n\n## filter by tower plot for log table\nlog_table_filter &lt;- log_table %&gt;% \n  filter(plotType == \"tower\",\n         plotID %in% last_plots)\n\n## filter by tower plot for cdw table\ncdw_tally &lt;- cdw_tally %&gt;%\n  filter(plotType == 'tower',\n         plotID %in% last_plots)\n\n## create \nlog_table_filter$gcm3 &lt;- rep(NA, nrow(log_table_filter))\n\n## set site specific volume factor\nsite_volume_factor &lt;- 8\n\nfor (i in 1:nrow(log_table_filter)){\n  ## match log table sampleID to cdw density table sample ID\n  ind &lt;- which(cdw_density$sampleID == log_table_filter$sampleID[i])\n  ## produce g/cm^3 by multiplying bulk density of disk by site volume factor\n  log_table_filter$gcm3[i] &lt;- mean(cdw_density$bulkDensDisk[ind]) * site_volume_factor\n}\n\n## table of coarse wood\nsite_cwd_carbon &lt;- log_table_filter %&gt;%\n  summarize(mean_kgCperm2 = .5 * sum(gcm3, na.rm = TRUE) * .1) |&gt; \n  mutate(year = unique(log_table_filter$yearBoutBegan))\n\nsite_cwd_carbon\n\n# A tibble: 1 × 2\n  mean_kgCperm2  year\n          &lt;dbl&gt; &lt;dbl&gt;\n1          2.57  2018"
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-fine-roots",
    "href": "neon-data.html#calculate-carbon-in-fine-roots",
    "title": "14  NEON data",
    "section": "14.4 Calculate carbon in fine roots",
    "text": "14.4 Calculate carbon in fine roots\nHere we are going to calculate the carbon stored in fine roots using the root chemistry data product. We will calculate the carbon in both dead and alive roots. Though we are interested mostly in live roots, at the time of writing this, there the 2021 NEON data for our site does not have rootStatus data available. Thus we will use historical data to compute an estimate of the ratio, so that we don’t have to throw away perfectly good information.\n\n## root chemistry data product\nindex &lt;-  neonstore:::neon_data(product = \"DP1.10067.001\",\n                                site = site,\n                                type=\"basic\")\n\nbbc_percore &lt;- index |&gt;\n  filter(grepl(\"bbc_percore\", name)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  collect()\n\nrootmass &lt;- index |&gt;\n  filter(grepl(\"bbc_rootmass\", name)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  collect()\n\n\nrootmass$year = year(rootmass$collectDate)\n\n## set variables for liveDryMass, deadDryMass, unkDryMass, area\nrootmass$liveDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$deadDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$unkDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$area &lt;- rep(NA, nrow(rootmass))\n\nfor (i in 1:nrow(rootmass)){\n  ## match by sample ID\n  ind &lt;- which(bbc_percore$sampleID == rootmass$sampleID[i])\n  ## extract core sample area\n  rootmass$area[i] &lt;- bbc_percore$rootSampleArea[ind]\n  ## categorize mass as live, dead, or unknown\n  if (is.na(rootmass$rootStatus[i])){\n    rootmass$unkDryMass[i] &lt;- rootmass$dryMass[i]\n  } else if (rootmass$rootStatus[i] == 'live'){\n    rootmass$liveDryMass[i] &lt;- rootmass$dryMass[i]\n  } else if (rootmass$rootStatus[i] == 'dead'){\n    rootmass$deadDryMass[i] &lt;- rootmass$dryMass[i]\n  } else{\n    rootmass$unkDryMass[i] &lt;- rootmass$dryMass[i]\n  }\n}\n\n##\nsite_roots &lt;- rootmass %&gt;%\n  ## filter plotID to only our plots of interest\n  filter(plotID %in% last_plots) %&gt;%\n  ## group by year\n  group_by(year) %&gt;%\n  ## sum live, dead, unknown root masses. multiply by\n  ## .5 for conversion to kgC/m^2\n  summarize(mean_kgCperm2_live = .5*sum(liveDryMass/area, na.rm = TRUE)/1000,\n            mean_kgCperm2_dead = .5*sum(deadDryMass/area, na.rm = TRUE)/1000,\n            mean_kgCperm2_unk = .5*sum(unkDryMass/area, na.rm = TRUE)/1000,\n            year_total = sum(c(mean_kgCperm2_dead, mean_kgCperm2_live, mean_kgCperm2_unk)) / length(unique(plotID)),\n            med_date = median(collectDate)) |&gt; \n  rename(mean_kgCperm2 = year_total) |&gt; \n  select(year, mean_kgCperm2)"
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-soils",
    "href": "neon-data.html#calculate-carbon-in-soils",
    "title": "14  NEON data",
    "section": "14.5 Calculate carbon in soils",
    "text": "14.5 Calculate carbon in soils\n\n#Download bieogeochemistry soil data to get carbon concentration\n#data_product1 &lt;- \"DP1.00097.001\"\n#Download physical soil data to get bulk density\nindex &lt;-  neonstore:::neon_data(product = \"DP1.00096.001\",\n                                site = site,\n                                type=\"basic\")\n\nmgc_perbiogeosample &lt;- index |&gt;\n  filter(grepl(\"mgp_perbiogeosample\", name)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  collect()\n\nmgp_perbulksample &lt;- index |&gt;\n  filter(grepl(\"mgp_perbulksample\", name)) |&gt; \n  pull(url) |&gt;\n  duckdbfs::open_dataset(format=\"csv\", filename = TRUE) |&gt; \n  collect()\n\n\nbulk_density &lt;- mgp_perbulksample %&gt;% \n    filter(bulkDensSampleType == \"Regular\") %&gt;% \n    select(horizonName,bulkDensExclCoarseFrag) \n\n  #gramsPerCubicCentimeter\nhorizon_carbon &lt;- mgc_perbiogeosample %&gt;% \n    filter(biogeoSampleType == \"Regular\") %&gt;% \n    select(horizonName,biogeoTopDepth,biogeoBottomDepth,carbonTot) \n\nyear &lt;- year(as_date(mgp_perbulksample$collectDate[1]))\n\n\n  #Unit notes\n  #bulkDensExclCoarseFrag = gramsPerCubicCentimeter\n  #carbonTot = gramsPerKilogram\n  \n  #Combine and calculate the carbon of each horizon\nhorizon_combined &lt;- inner_join(horizon_carbon,bulk_density, by = \"horizonName\") %&gt;%\n    #Convert volume in g per cm3 to mass per area in g per cm2 by multiplying by layer thickness\n    mutate(horizon_soil_g_per_cm2 = (biogeoBottomDepth - biogeoTopDepth) * bulkDensExclCoarseFrag) %&gt;% \n    #Units of carbon are g per Kg soil but we have bulk density in g per cm2 so convert Kg soil to g soil\n    mutate(CTot_g_per_g_soil = carbonTot*(1/1000),  #Units are g C per g soil\n           horizon_C_g_percm2 = CTot_g_per_g_soil*horizon_soil_g_per_cm2, #Units are g C per cm2\n           horizon_C_kg_per_m2 = horizon_C_g_percm2 * 10000 / 1000) %&gt;% #Units are g C per m2\n    select(-CTot_g_per_g_soil,-horizon_C_g_percm2) %&gt;%\n    arrange(biogeoTopDepth)\n  \nsite_soil_carbon &lt;- horizon_combined %&gt;% \n    summarize(soilC_gC_m2 = sum(horizon_C_kg_per_m2)) |&gt; \n  mutate(year = year)\n\n\nggplot(horizon_combined, map = aes(-biogeoTopDepth,horizon_C_kg_per_m2)) +\n  geom_line() +\n  geom_point() +\n  labs(y = \"Carbon\", x = \"Depth\", title = \"Soil carbon by depth\") +\n  coord_flip()"
  },
  {
    "objectID": "neon-data.html#combine-together",
    "href": "neon-data.html#combine-together",
    "title": "14  NEON data",
    "section": "14.6 Combine together",
    "text": "14.6 Combine together\n\nsite_live_carbon &lt;- site_live_carbon |&gt; \n  mutate(variable = \"live_tree\") |&gt; \n  rename(datetime = measure_date) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_dead_carbon &lt;- site_dead_carbon |&gt; \n  mutate(variable = \"dead_trees\") |&gt; \n  rename(datetime = measure_date) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_cwd_carbon &lt;- site_cwd_carbon |&gt; \n  mutate(variable = \"down_wood\") |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_roots &lt;- site_roots |&gt; \n  mutate(variable = \"fine_roots\") |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_soil_carbon &lt;- site_soil_carbon |&gt; \n  mutate(variable = \"soil_carbon\") |&gt; \n  rename(mean_kgCperm2 = soilC_gC_m2) |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\n\ntotal_carbon_components &lt;- bind_rows(site_live_carbon, site_dead_carbon, site_cwd_carbon, site_roots, site_soil_carbon)\n\nPlot the different pools of carbon\n\ntotal_carbon_components |&gt; \n  ggplot(aes(x = datetime, y = mean_kgCperm2, color = variable)) + \n  geom_point()\n\n\n\n\nCombine pools of carbon together to match the stocks used in our simple process model. This converts it to a long data format.\n\ntotal_carbon_simple &lt;- total_carbon_components |&gt; \n  pivot_wider(names_from = variable, values_from = mean_kgCperm2) |&gt; \n  mutate(wood = live_tree + mean(fine_roots, na.rm = TRUE),\n         som = mean(dead_trees, na.rm = TRUE) + mean(down_wood, na.rm = TRUE) + mean(soil_carbon, na.rm = TRUE),\n         som = ifelse(datetime != min(datetime), NA, som)) |&gt; \n  select(datetime, wood, som) |&gt; \n  pivot_longer(-datetime, names_to = \"variable\", values_to = \"observation\")"
  },
  {
    "objectID": "neon-data.html#modis-lai",
    "href": "neon-data.html#modis-lai",
    "title": "14  NEON data",
    "section": "14.7 MODIS LAI",
    "text": "14.7 MODIS LAI\nDownload the leaf area index\n\nlai &lt;- MODISTools::mt_subset(product = \"MCD15A2H\",\n                  lat = latitude,\n                  lon =  longitude,\n                  band = c(\"Lai_500m\", \"FparLai_QC\"),\n                  start = as_date(min(total_carbon_simple$datetime)),\n                  end = Sys.Date(),\n                  site_name = site,\n                  progress = FALSE)\n\n\nlai_cleaned &lt;- lai |&gt; \n  mutate(scale = ifelse(band == \"FparLai_QC\", 1, scale),\n         scale = as.numeric(scale),\n         value = scale * value,\n         datetime = lubridate::as_date(calendar_date)) |&gt; \n  select(band, value, datetime) |&gt; \n  pivot_wider(names_from = band, values_from = value) |&gt; \n  filter(FparLai_QC == 0) |&gt; \n  rename(observation = Lai_500m) |&gt; \n  mutate(variable = \"lai\") |&gt; \n  select(datetime, variable, observation)\n\nPlot MODIS LAI\n\nlai_cleaned |&gt; \n  ggplot(aes(x = datetime, y = observation)) +\n  geom_point() +\n  geom_smooth(span = 0.12)"
  },
  {
    "objectID": "neon-data.html#flux-data",
    "href": "neon-data.html#flux-data",
    "title": "14  NEON data",
    "section": "14.8 Flux data",
    "text": "14.8 Flux data\n\nurl &lt;- \"https://sdsc.osn.xsede.org/bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=P1D/terrestrial_daily-targets.csv.gz\"\n\nflux &lt;- read_csv(url, show_col_types = FALSE) |&gt; \n  filter(site_id %in% site, \n         variable == \"nee\") |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  select(datetime, variable, observation)"
  },
  {
    "objectID": "neon-data.html#combine-together-to-create-data-contraints",
    "href": "neon-data.html#combine-together-to-create-data-contraints",
    "title": "14  NEON data",
    "section": "14.9 Combine together to create data contraints",
    "text": "14.9 Combine together to create data contraints\n\nobs &lt;- total_carbon_simple |&gt; \n  bind_rows(lai_cleaned, flux) |&gt; \n  mutate(site_id = site) |&gt; \n  #convert from kgC/m2 to MgC/ha\n  mutate(observation = ifelse(variable %in% c(\"wood\", \"som\") , observation * 10, observation),\n         observation = ifelse(variable %in% c(\"nee\") , observation * 0.01, observation))\n\n\nobs |&gt; \n  ggplot(aes(x = datetime, y = observation)) + \n  geom_point() +\n  facet_wrap(~variable, scale = \"free\")\n\n\n\n\n\nwrite_csv(obs, \"data/site_carbon_data.csv\")"
  },
  {
    "objectID": "parameter-calibration1.html#problem-set",
    "href": "parameter-calibration1.html#problem-set",
    "title": "15  Parameter calibration: Intro to probability and likelihood",
    "section": "15.1 Problem set",
    "text": "15.1 Problem set\nCopy the text and code below into an .Rmd or .qmd document and complete the assignment. You will be asked submit (via Canvas) your rendered (or knitted) html document\n\nlibrary(tidyverse)\n\n\n15.1.1 Part 1\nLoad dataset\n\nd &lt;- read_csv(file = \"https://data.ecoforecast.org/neon4cast-targets/phenology/phenology-targets.csv.gz\", show_col_types = FALSE)\n\nFilter the dataset to only include the site_id BART (Bartlett Experimental Forest in the White Mountains of New Hampshire) and the dates between 2019-01-01 and 2019-07-01. Convert the date to Day of Year (hint: use lubridate:: yday() function). Remove rows with gcc_90 equal to NA or gcc_sd equal to 0.\n\nbart_2019 &lt;- d  %&gt;%\n  filter(site_id == \"BART\",\n         datetime &gt; as_date(\"2019-01-01\"),\n         datetime &lt; as_date(\"2019-07-01\"),\n         variable == \"gcc_90\") %&gt;%\n  mutate(doy = yday(datetime)) %&gt;% \n  filter(!is.na(observation),\n         observation &gt; 0)\n\nQuestion 1: How is gcc_90 related to day of year?\nAnswer 1:\n\n#Add Answer\n\nQuestion 2: Use a histogram to examine the distribution of the gcc_90\nAnswer 2:\n\n#Add Answer\n\nFirst create a function called `pred_logistic’ that is your process model. The model is the the logistic curve which ish the equation \\[P_1 + P_2 {{exp(P_3 + P_4 x)}\\over{1+exp(P_3 + P_4 x)}}\\] Question 3: Is this process model a dynamic model? Why or why not?\nAnswer 3:\nQuestion 4: Based on the equation above, write a function that predicts the gcc_90 as a function of the parameters (\\(P\\)) and x where x is the DOY. Name that function “pred_logistic”\nAnswer 4:\n\n#Add Answer\n\nQuestion 5: Write a function that calculates the negative log-likelihood of the data given a set of parameters governing the process and data models. Assume a normal distribution and be sure to estimate the sd in the data model.\nAnswer 5:\n\n#Add Answer\n\nQuestion 6: Use the optim function to find the most likely parameter values. Use the following as starting values par = c(0.34,0.11,-15,0.11, 1) where the first four are the theta parameters from your process model and the fifth is the sd of your data model.\nAnswer 6:\n\n#Add Answer\n\nQuestion 7: Use your optimal parameters in the pred_logistic function to predict the data. Save this as the object predicted\nAnswer 7:\n\n#Add Answer\n\nQuestion 8: Calculate the residuals and plot a histogram of the residuals\nAnswer 8:\n\n#Add Answer\n\nQuestion 9: How does the distribution of the data (Question 2) compare to the distribution of the residuals?\nAnswer 9:\nQuestion 10: Predict 2020 using the process model parameters from the 2019 fit.\n\n#Add Answer\n\nAnswer 10:\nQuestion 11: Plot the forecast from Question 10 over the data from 2020 (I give the code for getting the 2020 data)\nAnswer 11:\n\nbart_2020 &lt;- d  %&gt;%\n  filter(site_id == \"BART\",\n         datetime &gt; as_date(\"2020-01-01\"),\n         datetime &lt; as_date(\"2020-07-01\"),\n         variable == \"gcc_90\") %&gt;%\n  mutate(doy = yday(datetime)) %&gt;% \n  filter(!is.na(observation),\n         observation &gt; 0)\n\nQuestion 12: Do you think your model from 2019 is reasonable for predicting 2020?\nAnswer 12:\n\n\n15.1.2 Part 2\nUsing the following data\n\ndf &lt;- read_csv(\"data/soil_respiration_module_data.csv\", show_col_types = FALSE)\n\nIt is a dataset that reports soil respiration, soil temperature, and soil moisture over a year at the University of Michigan Biological Station (from Nave, L.E., N. Bader, and J.L. Klug)\nThe columns correspond to the following\n\ndoy = Day of Year\n\nsoil_resp: Soil respiration (micromoles CO2 per m2 per second)\n\nsoil_temp: Soil Temp (deg C) soil_moisture: Soil Moisture (%)\n\n\nUse maximium likelihood to estimate the parameters in the model that predicts the relationship between soil temperature and soil respiration using the Q10 function below\n\\[\\theta_1 * \\theta_2 ^{{(T - 20)}\\over{10}}\\]\nShow all the steps to determine the most likely parameter values, report the parameter values, and plot the data and predictions on the same plot"
  },
  {
    "objectID": "parameter-calibration2.html#starting-with-likelihood",
    "href": "parameter-calibration2.html#starting-with-likelihood",
    "title": "16  Parameter calibration: Intro to Bayesian statistics",
    "section": "16.1 Starting with likelihood",
    "text": "16.1 Starting with likelihood\nHere we start where we left off with the likelihood chapter. Imagine the following dataset with five data points drawn from a normal distribution.\n\nnum_data_points &lt;- 5\n\nmean_data &lt;- 3.0\nsd_data &lt;- 1.0\n\nnew_data &lt;- rnorm(num_data_points, mean = mean_data, sd = sd_data)\nhist(new_data)\n\n\n\n\nWe can calculate the likelihood for a set of different means using the manual likelihood estimation that we did in the likelihood lecture\n\n#DATA MODEL\ndelta_x &lt;- 0.1\nx &lt;- seq(-10,10, delta_x)\nnegative_log_likelihood &lt;- rep(NA,length(x))\nfor(i in 1:length(x)){\n  negative_log_likelihood[i] &lt;- -sum(dnorm(new_data, mean = x[i], sd = 1, log = TRUE))\n}\n\nplot(x, negative_log_likelihood)\n\n\n\n\nThe negative log likelihood is useful for using finding the maximum likelihood values with a optimizing function. However, here we want to convert back to density\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nplot(x, density_likelihood, type = \"l\")\n\n\n\n\nThe y-axis are tiny numbers because we multiplied probability densities together. This is OK for our illustrative purposes. To help visualize we can rescale so that area under the curve is 1. The area is approximated where the density is the height and the width is the distance between points on the x axis (delta_x) that we evaluated (height x width is the area of a bar under the curve). If we sum the bars together, we get the area under the curve (a value way less than 1). Dividing the likelihood by the area rescales the densities so the area under the curve is 1. Rescaling the likelihood is not a formal part of the the analysis - just used here for visualizing.\n\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_x) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\nThe rescaled likelihood looks like this\n\nd &lt;- tibble(x = x,\n            likelihood = density_likelihood_rescaled)\nggplot(d, aes(x = x, y = likelihood)) +\n  geom_line()\n\n\n\n\nRemember that this curve is the P(data | mean = x). We want the P(mean = x | data).\nFollowing Bayes rule we can multiply the likelihood x the prior\nTODO: add Bayes formula\nOur prior is the following normal distribution\n\nmean_prior &lt;- 0\nsd_prior &lt;- 1.0\n\n#Priors\ndensity_prior &lt;- dnorm(x, mean = mean_prior, sd = sd_prior)\nd &lt;- tibble(x = x,\n            density_prior = density_prior)\nggplot(d, aes(x = x, y = density_prior)) +\n  geom_line()\n\n\n\n\nPutting the rescaled likelihood on the same plot as the prior results in\n\ntibble(x = x,\n            prior = density_prior,\n            likelihood_rescaled = density_likelihood_rescaled) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line() \n\n\n\n\nMultiplying the prior x the likelihood (not rescaled)\n\nprior_times_likelihood &lt;- density_prior * density_likelihood\nd &lt;- tibble(x = x,\n            prior_times_likelihood = prior_times_likelihood)\nggplot(d, aes(x = x, y = prior_times_likelihood)) +\n  geom_line()\n\n\n\n\nBut from Bayes rule we can rescale the prior x likelihood by the area under the curve to convert to a probability density function\n\narea_under_curve &lt;- sum(prior_times_likelihood * delta_x)  #sum(Width * Height) = total probability of the data given all possible values of the parameter\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\npaste0(\"the probablity of the data is (i.e., area under likelihood x prior curve): \", area_under_curve)\n\n[1] \"the probablity of the data is (i.e., area under likelihood x prior curve): 1.88072630014752e-06\"\n\n\nNow we can visualize the rescaled likelihood (remember that the un-scaled likelihood was actually used in the calculations), the prior, and the normalized posterior.\nSee how the how the posterior is a blend of the prior and the likelihood\n\ntibble(x = x,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line()\n\n\n\n\nHere is a function that will allow you to explore how the posterior is sensitive to the likelihood and the prior\n\nexplore_senstivity &lt;- function(num_data_points, mean_data, sd_data, mean_prior, sd_prior, title){\n\nnew_data &lt;- rnorm(num_data_points, mean_data, sd_data)\ndelta_x &lt;- 0.1\nx &lt;- seq(-5,5, delta_x)\nnegative_log_likelihood &lt;- rep(NA,length(x))\n\n#DATA MODEL\nfor(i in 1:length(x)){\n  #Process model is that mean = x\n  negative_log_likelihood[i] &lt;- -sum(dnorm(new_data, mean = x[i], sd = 1, log = TRUE))\n}\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_x) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\n#Prior\ndensity_prior &lt;- dnorm(x, mean = mean_prior, sd = sd_prior)\n\n#Prior x Likelihood\nprior_times_likelihood &lt;- density_prior * density_likelihood\narea_under_curve &lt;- sum(prior_times_likelihood * delta_x) #Width * Height\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\np &lt;- tibble(x = x,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line() +\n  labs(title = title)\nreturn(p)\n}\n\nNow you can explore how the posterior is sensitive to 1) prior sd (i.e., confidence in the prior) 2) the number of data points used in the likelihood (how does increasing the number of data points influence the posterior) the prior mean 3) the mean of the data used in the likelihood (how different is it than the prior?)\n\n#Baseline\np1 &lt;- explore_senstivity(num_data_points = 5,\n                        mean_data = 3,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 1.0,\n                        title = \"Baseline\")\n\n#Increase confidence in prior\np2 &lt;- explore_senstivity(num_data_points = 5,\n                        mean_data = 3,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 0.1,\n                        title = \"Increase confidence in prior\")\n\n#Increase number of data points\np3 &lt;- explore_senstivity(num_data_points = 50,\n                        mean_data = 3,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 1.0,\n                        title = \"Increase data\")\n\n#Make likelihood mean closer to prior\np4 &lt;- explore_senstivity(num_data_points = 5,\n                        mean_data = 0,\n                        sd_data = 2,\n                        mean_prior = 0,\n                        sd_prior = 1.0,\n                        title = \"Make likelihood mean closer to prior\")\n(p1 / p2) | (p3 / p4) \n\n\n\n\nJust like we extended the likelihood analysis to the non-linear example, we can do the same for the Bayesian analysis.\nFirst create a data set using the Michaelis-Menten function from the likelihood exercise. Here, instead of fitting both parameters, we only fit one parameter (the maximum or saturating value) called par1. In the chunk below we set the number of data points, the true value for par1, and the standard deviation of the data.\n\nset.seed(100)\nnum_data_points &lt;- 10\npar1_true &lt;- 3\nsd_data &lt;- 0.5\nx &lt;- runif(num_data_points, 0, 10)\npar_true &lt;- c(par1_true, 0.5)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\nplot(x, y, ylim = c(0, par1_true + 2))\n\n\n\n\nNow we can define the prior. We think the prior is normally distributed with a mean and sd defined below\n\nmean_prior &lt;- 1.0\nsd_prior &lt;- 0.5\n\nHere is the manual calculation of the likelihood and the prior. We combine the results into a data frame for visualization.\n\ndelta_par1 &lt;- 0.1\npar1 &lt;- seq(-3,10, delta_par1)\nnegative_log_likelihood &lt;- rep(NA,length(par1))\nfor(i in 1:length(par1)){\n  #Process model\n  pred &lt;- par1[i] * (x / (x + par_true[2]))\n  #Data model\n  negative_log_likelihood[i] &lt;- -sum(dnorm(y, mean = pred, sd = sd_data, log = TRUE))\n}\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_par1) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\n#Priors\ndensity_prior &lt;- dnorm(par1, mean = mean_prior, sd = sd_prior)\nprior_times_likelihood &lt;- density_prior * density_likelihood\narea_under_curve &lt;- sum(prior_times_likelihood * delta_par1) #Width * Height\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\ntibble(par1 = par1,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -par1, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = par1, y = density, color = distribution)) +\n  geom_line()\n\n\n\n\nNow we can look at how our the prior and posterior distributions influence the shape of the process model curve (M-M). For illustration, the figure below shows the M-M curve using the most likely value from the prior, most likely value if we just looked at the likelihood, and most likely value from the posterior.\n\npar1_mle &lt;- par1[which.max(density_likelihood)]\npar1_post &lt;- par1[which.max(normalized_posterior)]\n\nd &lt;- tibble(x = seq(0,10, 0.1),\n            prior = mean_prior * (x / (x + par_true[2])),\n            likelihood = par1_mle * (x / (x + par_true[2])),\n            posterior = par1_post * (x / (x + par_true[2]))) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"prediction\") %&gt;% \n  mutate(distribution = factor(distribution))\nggplot(d, aes(x = x, y = prediction, col = distribution)) +\n  geom_line() +\n  labs(y = \"M-M model prediction (process model)\")"
  },
  {
    "objectID": "parameter-calibration2.html#solving-a-bayesian-model",
    "href": "parameter-calibration2.html#solving-a-bayesian-model",
    "title": "16  Parameter calibration: Intro to Bayesian statistics",
    "section": "16.2 Solving a Bayesian model",
    "text": "16.2 Solving a Bayesian model\nThe example above is designed to build an intuition for how a Bayesian analysis works but is not how the parameters in a Bayesian model are estimated in practice. For one thing, if there are multiple parameters being estimated it is very hard to estimate the area under the curve. Second, the area is very very same if there are many data points (so small that the computer can’t hold it well)\nThere are two common methods for estimating posterior distributions that involve numerical computation. Both methods involve randomly sampling from an unknown posterior distribution and saving the samples. The sequence of saved sample for the parameters is called a Markov chain Monte Carlo (MCMC). The distribution of parameter values in the MCMC chain is your posterior distribution.\nThe first is called Gibbs sampling and is used when you know the probability distribution type for the posterior but not the parameter values of the distribution. For example, if your prior is normally distributed and your likelihood is normally distributed then you know (from math that other have already done for you) that the posterior is normally distributed. Therefore, you can randomly draw from that distribution to build your MCMC chain that generates your posterior distribution. We will not go over this in detail but know that this method will give you an answer quicker but requires you (or the software you are using) to know that your prior and likelihood are conjunct (i.e., someone else has worked out that the posterior always has a certain PDF if the prior and likelihood are of certain PDF - see pages 86-89 and Table A3 in Hobbs and Hooten).\nThe second is called MCMC Metropolis–Hastings (MCMC-MH) and is a rejection sampling method. In this case you don’t know the form of the posterior. Basically the MCMC-MH is the following\n\nCreate vector that has a length equal to the total number of iterations you want to have in your MCMC chain. Set the first value of the vector to your parameter starting point.\nrandomly choose a new parameter value based on the previous parameter value in the MCMC chain (called a proposal). For example (where i is the current iteration in your MCMC chain): par_proposed &lt;- rnorm(1, mean = par[i - 1], sd = jump), where jump is the standard deviation that governs how far you want the proposed parameter to potentially be from the previous parameter.\nuse this proposed parameter in your likelihood and prior calculations and multiply the likelihood * prior (i.e., the numerator in Bayes formula). Save this as the proposed probability(prob_proposed)\nTake the ratio of the proposed probability to the previous probability (also called the current probability; prob_current). call this prob_proposed\nRandomly select a number between 0 and 1. Call this z\nIf prob_proposed from Step 3 is greater than z from Step 4 then save the proposed parameter for that iteration of MCMC chain. If it is less, then assign the previous parameter value for that iteration of the MCMC chain. As a result of Step 5:\n\n\nAll parameters that improve the probability will have prob_proposed/prob_current &gt; 1. Therefore all improvements will be accepted since z (by definition in Step 4) can never be greater than 1.\nWorse parameters where prob_proposed/prob_current &lt; 1, will be accepted in proportion to how worse they are. For example if prob_proposed/prob_current = 0.9, 90% of the time z will be less the .90 so the worse parameters than are worse by 10% will be accepted 90% of the time. If prob_proposed/prob_current = 0.01 (i.e., the new parameters are much worse), only 1% of the time will z be less then 0.01. Therefore it is possible but not common to save these worse parameters. As a result, the MCMC-MH approach explores the full distribution by spending more time at more likely parameter values. This is different than maximum likelihood optimization methods like optim' that only save parameters that are better than previous (thus finds the peak of the mountain rather than the shape of the mountain). The MCMC-HM approach requires taking a lot samples so that it spends some time at very unlikely values - a necessity for estimating the tails of a distribution.\nYou want to accept ~40% percent of all proposed parameter values. If your jump parameter from #1 is too large, you won’t be able two explore the area around the most likely values (i.e., you won’t get a lot of prob_proposed/prob_current values near 1). If your jump parameter from #1 is too small, you won’t be able to explore the tails of the distribution (i.e., you won’t get a lot of prob_proposed/prob_current values near 0 that have a random chance of being accepted).\n\nNote: there is a step that is ignored here that just confuses at this stage. Your proposal distribution in #1 doesn’t have to be normal, which is symmetric (i.e., your probability of jumping from a value of X to Y is the same as jumping from Y to X). There is adjustment for non-symmetric proposals that is on page page 71 in Dietze and page 158 in Hobbs and Hooten.\nHere is an example of the MCMC-MH method:\n(note: the example below should use logged probability densities for numerical reasons but uses the non-logged densities so that the method is more clear. The example in assignment uses logged densities)\nSet up data (same as above)\n\nnum_data_points &lt;- 10\npar1_true &lt;- 3\nsd_data &lt;- 0.5\nx &lt;- runif(num_data_points, 0, 10)\npar_true &lt;- c(par1_true, 0.5)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\nplot(x, y, ylim = c(0, par1_true + 2))\n\n\n\n\nRun MCMC-MH\n\n#Initialize chain\nnum_iter &lt;- 5000\npars &lt;- array(NA, dim = c(num_iter))\npars[1] &lt;- 2\nlog_prob_current &lt;- -10000000000\nprob_current &lt;- exp(log_prob_current)\njump &lt;- 0.1\n\nmean_prior &lt;- 1.0\nsd_prior &lt;- 0.5\n\nfor(i in 2:num_iter){\n  \n  #Randomly select new parameter values\n  proposed_pars &lt;- rnorm(1, pars[i - 1], jump)\n  \n    \n  #PRIORS: how likely is the proposed value given the prior distribution?\n  prior &lt;- dnorm(proposed_pars, mean = mean_prior, sd = sd_prior)\n\n  #PROCESS MODEL: Use new parameter values in the process model\n  pred &lt;- proposed_pars * (x / (x + par_true[2]))\n\n  #DATA MODEL: how likely is the data given the proposed parameter?\n  #We are multiplying here\n  likelihood &lt;- prod(dnorm(y, mean = pred, sd = sd_data))\n  \n  #Combine the prior and likelihood\n  #remember that you multiply probabilities which mean you can add log(probability)\n  prob_proposed &lt;- prior * likelihood\n  \n  z &lt;- (prob_proposed/prob_current)\n  \n  #Now pick a random number between 0 and 1\n  r &lt;- runif(1, 0, 1)\n  \n  #If z &gt; r then accept the new parameters\n  #Note: this will always happen if the new parameters are more likely than\n  #the old parameters z &gt; 1 means than z is always &gt; r no matter what value of\n  #r is chosen.  However it will accept worse parameter sets (P_new is less\n  #likely then P_old - i.e., z &lt; 1) in proportion to how much worse it is\n  #For example: if z = 0.9 and then any random number drawn by runif that is\n  #less than 0.90 will result in accepting the worse values (i.e., the slightly\n  #worse values will be accepted a lot of the time).  In contrast, if z = 0.01\n  #(i.e., the new parameters are much much worse), then they can still be accepted\n  #but much more rarely because random r values of &lt; 0.1 occur more rarely\n  if(z &gt; r){\n    pars[i] &lt;- proposed_pars\n    prob_current &lt;- prob_proposed\n  }else{\n    pars[i] &lt;- pars[i - 1]\n    prob_current &lt;- prob_current #this calculation isn't necessary but is here to show you the logic\n  }\n}\n\nThe pars variable is our MCMC chain estimating the posterior distribution . We can visualize it in two ways. The first is with iteration number on the axis. The second is as a histogram. A chain is that ready for analysis will have a constant mean and variance. The variance is important because it is the exploration of the posterior distribution. The histogram shows the posterior distribution.\n\nd &lt;- tibble(iter = 1:num_iter,\n       par1 = pars)\n\np1 &lt;-  ggplot(d, aes(x = iter, y = par1)) +\n  geom_line()\n\np2 &lt;- ggplot(d, aes(x = par1)) +\n  geom_histogram()\n\np1 | p2\n\n\n\n\nYou should notice that the chain starts at 2 before moving to a mean of 3. The starting value of 2 was arbitrary. Since it was far from 3, the proposed new parameter values often resulted in improvements and accepting values that were more likely. As a result the chain moves to the part with a mean of 3 and constant variance (i.e., where the chain has converged). This transition from the starting value to the point where the chain has converged should be discard. We call this the “burn-in”. Here are the same plots with the burn-in removed\n\nnburn &lt;- 100\nd_burn &lt;- tibble(iter = nburn:num_iter,\n       par1 = pars[nburn:num_iter])\n\np1 &lt;-  ggplot(d_burn, aes(x = iter, y = par1)) +\n  geom_line()\n\np2 &lt;- ggplot(d_burn, aes(x = par1)) +\n  geom_histogram()\n\np1 | p2\n\n\n\n\nNow your can analyze the chain to explore the posterior distribution\n\npar_post_burn &lt;- pars[nburn:num_iter]\n\n#Mean\nmean(par_post_burn)\n\n[1] 2.703547\n\n#sd\nsd(par_post_burn)\n\n[1] 0.1801216\n\n#Quantiles\nquantile(par_post_burn, c(0.025, 0.5,0.975))\n\n    2.5%      50%    97.5% \n2.359081 2.700100 3.065872 \n\n\nFinally, you can sample from the posterior distribution just like your would sample from a random variable using the rnorm, rexp, rlnorm, etc. function. The key is to randomly select an iteration (num_sample = 1) or a set of samples (if num_sample &gt; 0) with replacement (replace = TRUE; i.e., a iteration could be randomly selected multiple times).\n\nnum_samples &lt;- 100\nsample_index &lt;- sample(x = 1:length(par_post_burn), size = num_samples, replace = TRUE)\nrandom_draws &lt;- par_post_burn[sample_index]\nhist(random_draws)"
  },
  {
    "objectID": "parameter-calibration2.html#predictive-posterior-distributions",
    "href": "parameter-calibration2.html#predictive-posterior-distributions",
    "title": "16  Parameter calibration: Intro to Bayesian statistics",
    "section": "16.3 Predictive posterior distributions",
    "text": "16.3 Predictive posterior distributions\nFinally we can use the idea of randomly drawing from the posterior to develop predictions from the posterior. This is just like the Logistic growth module where you randomly sampled from the parameter uncertainty and used the random samples in the logistic equation.\nHere we are calculating two things 1) pred_posterior_mean just has uncertainty in the M-M parameter. Think about this as generating uncertainty around the mean prediction at each value of x. 2) y_posterior is the prediction of a observation. Therefore you take the value from #1 and add the uncertainty in the observations from sd_data that was set above. This is your predictive or forecast uncertainty.\nImportant: If you have multiple parameters your MCMC chain, you randomly draw posterior_sample_indices and use values for all the parameters at that iteration. By doing this you are representing the joint distribution of the parameter - e.g., if parameter 1 is high, parameter 2 is always low. If you select posterior_sample_indices for each parameter, you break the correlations of parameters that the MCMC method estimated and, as a result, overestimate the uncertainty.\n\nnum_samples &lt;- 1000\nx_new = x\npred_posterior_mean &lt;- matrix(NA, num_samples, length(x_new))   # storage for all simulations\ny_posterior &lt;- matrix(NA, num_samples, length(x_new)) \n\nfor(i in 1:num_samples){\n  sample_index &lt;- sample(x = 1:length(pars), size = 1, replace = TRUE)\n  pred_posterior_mean[i, ] &lt;- pars[sample_index] * (x_new / (x_new + par_true[2]))\n  y_posterior[i, ] &lt;- rnorm(length(x_new), pred_posterior_mean[i, ], sd = sd_data)\n  \n}\nn.stats.y &lt;- apply(y_posterior, 2, quantile, c(0.025, 0.5, 0.975))\nn.stats.y.mean &lt;- apply(y_posterior, 2, mean)\n\nn.stats.mean &lt;- apply(pred_posterior_mean, 2, quantile, c(0.025, 0.5, 0.975))\n\nd &lt;- tibble(x = x_new,\n            median = n.stats.y.mean,\n            lower95_y = n.stats.y[1, ],\n            upper95_y = n.stats.y[3, ],\n            lower95_mean = n.stats.mean[1, ],\n            upper95_mean = n.stats.mean[3, ],\n            obs = y)\n\nggplot(d, aes(x = x)) +\n  geom_ribbon(aes(ymin = lower95_y, ymax = upper95_y), fill = \"lightblue\", alpha = 0.5) +\n    geom_ribbon(aes(ymin = lower95_mean, ymax = upper95_mean), fill = \"pink\", alpha = 0.5) +\n  geom_line(aes(y = median)) +\n  geom_point(aes(y = obs)) +\n  labs(y = \"M-M Prediction\")"
  },
  {
    "objectID": "parameter-calibration2.html#problem-set",
    "href": "parameter-calibration2.html#problem-set",
    "title": "16  Parameter calibration: Intro to Bayesian statistics",
    "section": "16.4 Problem set",
    "text": "16.4 Problem set\nUsing the following as a guide\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n#Build fake dataset\nset.seed(100)\nnum_data_points &lt;- 200\nsd_data &lt;- 0.25\npar_true &lt;- c(3, 0.5)\nx &lt;- runif(num_data_points, 0, 10)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\nplot(x, y, ylim = c(0, par_true[1] + 2))\n\n\n\n\n\n#Set MCMC Configuration\nnum_iter &lt;- 2000\nnum_pars &lt;- 2\njump &lt;- c(0.05, 0.05)\n\n#Initialize chain\npars &lt;- array(NA, dim = c(num_pars, num_iter))\npars[1, 1] &lt;- 2\npars[2, 1] &lt;- 1\nlog_likelihood_prior_current &lt;- -10000000000\n\nfor(i in 2:num_iter){\n  \n  #Loop through parameter value\n  \n  for(j in 1:num_pars){\n      #Randomly select new parameter values\n    proposed_pars &lt;- pars[, i - 1]\n    proposed_pars[j] &lt;- rnorm(1, mean = pars[j, i - 1], sd = jump[j])\n    \n    ##########################\n    # PRIORS\n    #########################\n    #(remember that you multiply probabilities which mean you can add log(probability))\n    log_prior &lt;- dunif(proposed_pars[1], min = 0, max = 10, log = TRUE) + \n      dunif(proposed_pars[2], min = 0, max = 100, log = TRUE)\n    \n    #Likelihood.  \n    #You could use:\n    # pred &lt;- process_model(x, pars = proposed_pars)\n    # log_likelihood &lt;- sum(dnorm(new_data, mean = pred, sd = sd_data, log = TRUE)\n    # but we are looping here because it transitions well to the next section of the course\n    log_likelihood &lt;- rep(NA, length(x))\n    pred &lt;- rep(NA, length(x))\n    for(m in 1:length(x)){\n      ##########################\n      # PROCESS MODEL\n      #########################\n      pred[m] &lt;- proposed_pars[1] * (x[m] / (x[m] + proposed_pars[2]))\n      ##########################\n      # DATA MODEL\n      #########################\n      log_likelihood[m] &lt;- dnorm(y[m], mean = pred[m], sd = sd_data, log = TRUE)\n    }\n    #Remember that you multiply probabilities which mean you can add log(probability)\n    #Hence the use of sum\n    log_likelihood &lt;- sum(log_likelihood)\n    \n    ############################\n    ###  PRIOR x LIKELIHOOD\n    ############################\n    #Combine the prior and likelihood\n    #remember that you multiply probabilities which means you can add log(probability)\n    log_likelihood_prior_proposed &lt;- log_prior + log_likelihood\n    \n    #We want the ratio of new / old but since it is in log space we first\n    #take the difference of the logs: log(new/old) = log(new) - log(old) \n    # and then take out of log space exp(log(new) - log(old))\n    z &lt;- exp(log_likelihood_prior_proposed - log_likelihood_prior_current)\n    \n    #Now pick a random number between 0 and 1\n    r &lt;- runif(1, min = 0, max = 1)\n    #If z &gt; r then accept the new parameters\n    #Note: this will always happen if the new parameters are more likely than\n    #the old parameters z &gt; 1 means than z is always &gt; r no matter what value of\n    #r is chosen.  However it will accept worse parameter sets (P_new is less\n    #likely then P_old - i.e., z &lt; 1) in proportion to how much worse it is\n    #For example: if z = 0.9 and then any random number drawn by runif that is\n    #less than 0.90 will result in accepting the worse values (i.e., the slightly\n    #worse values will be accepted a lot of the time).  In contrast, if z = 0.01\n    #(i.e., the new parameters are much much worse), then they can still be accepted\n    #but much more rarely because random r values of &lt; 0.1 occur more rarely\n    if(log(z) &gt; log(r)){\n      pars[j, i] &lt;- proposed_pars[j]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_proposed\n    }else{\n      pars[j, i] &lt;- pars[j, i - 1]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_current #this calculation isn't necessary but is here to show you the logic\n    }\n  }\n}\n\n\nd &lt;- tibble(iter = 1:num_iter,\n            par1 = pars[1, ],\n            par2 = pars[2, ]) %&gt;%\n  pivot_longer(-iter, values_to = \"value\", names_to = \"parameter\")\n\np1 &lt;- ggplot(d, aes(x = iter, y = value)) +\n  geom_line() +\n  facet_wrap(~parameter, scales = \"free\")\n\np2 &lt;- ggplot(d, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~parameter, scales = \"free\")\n\np1 / p2\n\n\n\n\nYour task is to modify the code above to estimate the posterior distribution of parameters in Q10 function that was in the likelihood analysis exercise. Use the same data as used in the Q10 likelihood exercise.\nQuestion 1: Provide the distribution and parameters describing the distribution for your prior distributions. Justify why you chose the distribution and parameters. (do not spend time looking at the literature for values to use to build prior distribution - just give plausible priors and say why their plausible)\nAnswer 1:\nQuestion 2: Provide plots of your prior distributions.\nAnswer 2:\nQuestion 3: Modify the code above to estimate the posterior distribution of your parameters. Put your modified code below.\nAnswer 3:\nQuestion 4: Plot the your MCMC chain for all parameters (iteration # will be the x-axis)\nAnswer 4:\nQuestion 5: Approximately how many iterations did it take your chain to converge to a straight line with constant variation around the line (i.e., a fuzzy caterpillar). This is the burn-in. If your chain did not converge, modify the jump variable for each parameters and/or increase your iterations. You should not need more than 10000 iterations for convergence so running the chain for a long period of time will not fix issues that could be fixed by modifying the jump variable. Also, pay attention to the sd_data parameter. You should estimate it as a parameter or set it to a reasonable value. If it is too small your chain will fail because the probability of the some of parameters that are explored functionally zero.\nAnswer 5:\nQuestion 6: Remove the iterations between 1 and your burn-in number and plot the histograms for your parameters.\nAnswer 6:\nQuestion 7: Provide the mean and 95% Credible Intervals for each parameter\nAnswer 7:\nQuestion 8: Random select 1000 values from the parameters in your posterior distribution. Show the randomly selected values for each parameter as a histogram.\nAnswer 8:\nQuestion 9: Use the samples from Question 8 to generate posterior predictions of soil respiration at the observed temperature values (i.e., the same temperature data used in your model fit). Provide a plot with temperature on the x-axis and respiration on the y-axis. The plot should have the mean and 95% predictive uncertainty bounds (i.e., include uncertainty in parameters and in the data model)\nAnswer 9:"
  },
  {
    "objectID": "parameter-calibration3.html#runing-mcmc-on-the-forest-process-model",
    "href": "parameter-calibration3.html#runing-mcmc-on-the-forest-process-model",
    "title": "17  Parameter calibration: Applying to process model",
    "section": "17.1 Runing MCMC on the forest process model",
    "text": "17.1 Runing MCMC on the forest process model\n\nlibrary(tidyverse)\nlibrary(patchwork)\nsource(\"R/helpers.R\")\nsource(\"R/forest_model.R\")\nset.seed(100)\n\n\nsite &lt;- \"TALL\"\n\nRead in observations\n\nobs &lt;- read_csv(\"data/site_carbon_data.csv\", show_col_types = FALSE)\n\nSet up dates of simulation, parameters, initial conditions, and meterology inputs\n\nsim_dates &lt;- seq(as_date(\"2022-01-01\"), Sys.Date() - 1, by = \"1 day\")\n\nens_members &lt;- 1\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(250, ens_members)\nparams$litterfall_length&lt;- rep(60, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.stem &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n\nstate_init &lt;- rep(NA, 3)\n\nstate_init[1] &lt;- obs |&gt; \n  filter(datetime %in% sim_dates,\n         variable == \"lai\") |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  mutate(observation = observation / (mean(params$SLA) * 0.1)) |&gt; \n  pull(observation)\n\nstate_init[2] &lt;- obs |&gt; \n  filter(variable == \"wood\",\n         datetime %in% sim_dates) |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  pull(observation)\n\nstate_init[3] &lt;- obs |&gt; \n  filter(variable == \"som\") |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  pull(observation)\n\ninputs &lt;- get_historical_met(site = \"TALL\", sim_dates, use_mean = TRUE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\nSet up MCMC configuration\n\n#Set MCMC Configuration\nnum_iter &lt;- 1500\nlog_likelihood_prior_current &lt;- -10000000000\naccept &lt;- 0\n\n\n#Initialize chain\nnum_pars &lt;- 3\njump_params &lt;- c(0.001, 0.0002, 1)\nfit_params &lt;- array(NA, dim = c(num_pars, num_iter))\nfit_params[1, 1] &lt;- params$alpha\nfit_params[2, 1] &lt;- params$Rbasal\nfit_params[3, 1] &lt;- params$litterfall_start\nprior_mean &lt;- c(0.029, 0.002, 200)\nprior_sd &lt;- c(0.005, 0.0005, 10)\n\nRun MCMC\n\nfor(iter in 2:num_iter){\n  \n  #Loop through parameter value\n  \n  for(j in 1:num_pars){\n    \n    proposed_pars &lt;- fit_params[, iter - 1]\n    proposed_pars[j] &lt;- rnorm(1, mean = fit_params[j, iter - 1], sd = jump_params[j])\n    \n    log_prior &lt;- dnorm(proposed_pars[1], mean = prior_mean[1], sd = prior_sd[1], log = TRUE) + \n      dnorm(proposed_pars[2], mean = prior_mean[2], sd = prior_sd[2], log = TRUE) +\n      dnorm(proposed_pars[3], mean = prior_mean[3], sd = prior_sd[3], log = TRUE)\n    \n    params$alpha  &lt;- proposed_pars[1]\n    params$Rbasal  &lt;- proposed_pars[2]\n    params$litterfall_start &lt;- proposed_pars[3]\n    \n    #Set initial conditions\n    output &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\n    output[1, , 1] &lt;- state_init[1]\n    output[1, , 2] &lt;- state_init[2]\n    output[1, , 3] &lt;- state_init[3]\n    \n    for(t in 2:length(sim_dates)){\n      output[t, , ]  &lt;- forest_model(t, \n                                     states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                                     parms = params, \n                                     inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n    }\n    \n    output_df &lt;- output_to_df(output, sim_dates, sim_name = \"fitting\")\n    \n    combined_output_obs &lt;- combine_model_obs(output_df, \n                                             obs,\n                                             variables = c(\"lai\", \"wood\", \"som\", \"nee\"), \n                                             sds = c(0.1, 1, 20, 0.01))\n    \n    log_likelihood &lt;- sum(dnorm(x =  combined_output_obs$observation, \n                                mean = combined_output_obs$prediction, \n                                sd = combined_output_obs$sds, log = TRUE))\n    \n    log_likelihood_prior_proposed &lt;- log_prior + log_likelihood\n    \n    z &lt;- exp(log_likelihood_prior_proposed - log_likelihood_prior_current)\n    \n    r &lt;- runif(1, min = 0, max = 1)\n    \n    if(z &gt;  r){\n      fit_params[j, iter] &lt;- proposed_pars[j]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_proposed\n      accept &lt;- accept + 1\n    }else{\n      fit_params[j, iter] &lt;- fit_params[j, iter - 1]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_current #this calculation isn't necessary but is here to show you the logic\n    }\n  }\n}\n\nExamine acceptance rate (goal is 23-45%)\n\naccept / (num_iter * num_pars)\n\n[1] 0.4264444\n\n\nProcess MCMC chain by removing the first 500 iterations and pivoting to a long format\n\nnburn &lt;- 500\nparameter_MCMC &lt;- tibble(iter = nburn:num_iter,\n            alpha = fit_params[1, nburn:num_iter],\n            Rbasal = fit_params[2, nburn:num_iter],\n            litterfall_start = fit_params[3, nburn:num_iter]) %&gt;%\n  pivot_longer(-iter, values_to = \"value\", names_to = \"parameter\")\n\nExamine chains\n\np1 &lt;- ggplot(parameter_MCMC, aes(x = iter, y = value)) +\n  geom_line() +\n  facet_wrap(~parameter, scales = \"free\")\n\np2 &lt;- ggplot(parameter_MCMC, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~parameter, scales = \"free\")\n\np1 / p2"
  },
  {
    "objectID": "parameter-calibration3.html#examining-the-influence-of-the-parameter-optimization-on-model-predictions",
    "href": "parameter-calibration3.html#examining-the-influence-of-the-parameter-optimization-on-model-predictions",
    "title": "17  Parameter calibration: Applying to process model",
    "section": "17.2 Examining the influence of the parameter optimization on model predictions",
    "text": "17.2 Examining the influence of the parameter optimization on model predictions\n\n17.2.1 Simulation with prior parameter distributions\n\nens_members &lt;- 100\n\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\n#Set initial conditions\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- state_init[1]\noutput[1, , 2] &lt;- state_init[2]\noutput[1, , 3] &lt;- state_init[3]\n\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(70, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.stem &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n#Replace parameters with prior distribution\nparams$alpha &lt;- rnorm(ens_members, mean = prior_mean[1], sd = prior_sd[1])\nparams$Rbasal &lt;- rnorm(ens_members, mean = prior_mean[2], sd = prior_sd[2])\nparams$litterfall_start &lt;- rnorm(ens_members, mean = prior_mean[3], sd = prior_sd[3])\n\nfor(t in 2:length(sim_dates)){\n  \n  output[t, , ]  &lt;- forest_model(t, \n                                 states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                                 parms = params, \n                                 inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\noutput_df_no_optim &lt;- output_to_df(output, sim_dates, sim_name = \"no_optim\")\n\n\n\n17.2.2 Simulation with posterior parameter distributions\n\n#Set initial conditions\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- state_init[1]\noutput[1, , 2] &lt;- state_init[2]\noutput[1, , 3] &lt;- state_init[3]\n\n# Sample from posterior distributions\n\nparams$alpha &lt;- sample(fit_params[1, nburn:num_iter], ens_members, replace = TRUE)\nparams$Rbasal &lt;- sample(fit_params[2, nburn:num_iter], ens_members, replace = TRUE)\nparams$litterfall_start &lt;- sample(fit_params[3, nburn:num_iter], ens_members, replace = TRUE)\n\nfor(t in 2:length(sim_dates)){\n  \n  output[t, , ]  &lt;- forest_model(t, \n                                 states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                                 parms = params, \n                                 inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\noutput_df_optim &lt;- output_to_df(output, sim_dates, sim_name = \"optimized\")\n\n\n\n17.2.3 Visualize influence of optimization\n\nobs_filtered &lt;- obs |&gt; \n  filter(datetime &gt; min(output_df_no_optim$datetime))\n\n\nbind_rows(output_df_no_optim, output_df_optim) |&gt; \n   summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\", \"model_id\")) |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90, fill = model_id), alpha = 0.7) +\n  geom_line(aes(y = median, color = model_id)) +\n  geom_point(data = obs_filtered, aes(x = datetime, y = observation), color = \"blue\", alpha = 0.5) +\n  facet_wrap(~variable, scale = \"free\")\n\n\n\n\n\n\n17.2.4 Save posteriors for future use\n\n  write_csv(parameter_MCMC, \"data/saved_parameter_chain.csv\")"
  },
  {
    "objectID": "particle-filter.html#review-of-batch-vs-sequential-methods",
    "href": "particle-filter.html#review-of-batch-vs-sequential-methods",
    "title": "18  Data assimilation: particle filter",
    "section": "18.1 Review of Batch vs Sequential Methods",
    "text": "18.1 Review of Batch vs Sequential Methods\nBefore beginning the introduction of the particle filter it is important to revisit the MCMC-MH approach that we used to estimate the parameters of a Bayesian model. Here is the general algorithm again:\nfor(i in 1:num_iterations){\n\n    Choose new parameters based on previous parameters\n\n    for(t in 1:length_of_time_series){\n    \n      Make predictions over full time series\n    \n    }\n    \n    Calculate likelihood of data given model and current parameters\n    Calculate probability of priors given current parameters\n    \n    Accept or reject parameters\n\n}\nAbove you see that the outermost for-loop is looping over the number of iterations (num_iterations). The inner loop is looping over the length of the time series (length_of_time_series). Therefore this approach tests each parameter value choose in an iteration (and latent state if using a state space model) over ALL time points in the time series. As a result, we all this a batch method because it considers all data as a single “batch” of data.\nStrengths of the batch method are:\n- The parameters are consistent with all data - Straightforward to estimate parameters, uncertainty parameters, and latent states\nWeaknesses:\n- Can be computationally slow\n- Require re-fitting model if there is even a single new data point\nAlternatively, sequential methods only analyze data one time point as a time. Here is general code for a sequential method - notice that the for-loop order is reversed\n\nCalculate prior distribution of parameters\n\nfor(t in 1:length_of_time_series){\n\n    for(i in 1:num_of_particles){\n    \n      Make predictions for each particle based on previous value for particle\n    \n    }\n    \n    Compare particle to data (likelihood or other technique)\n\n    Weight particles based on the comparsion to the data\n    \n    Adjust particles based on weights\n\n}\nIn the sequential method we can restart at any time point, as long as we have the values for the states and parameters that are associated with each particle. When doing an iterative forecast, these values are what you would save to wait for new data to arrive."
  },
  {
    "objectID": "particle-filter.html#introduction-to-particle-filter",
    "href": "particle-filter.html#introduction-to-particle-filter",
    "title": "18  Data assimilation: particle filter",
    "section": "18.2 Introduction to Particle Filter",
    "text": "18.2 Introduction to Particle Filter\nThere are many sequential data assimilation methods that make different assumptions about data and model distributions in order to simplify the analysis. Many of the methods emerged before the our massive computational resources or were developed for HUGE problems like assimilating terabytes of data into a global weather model that simulates the physics of the atmosphere at 25 km vertical resolution. Examples include the Kalman Filter, Extending Kalman Filter, Ensemble Kalman Filter, and 4D-var. These methods all heavily use matrix algebra which I have not introduced in this class. Since these methods commonly assume that the data and model errors are normally distributed, the numerical version (Ensemble Kalman Filter) can run fewer particles (in this case: ensemble members) because it doesn’t take as many samples to estimate the mean and variance of a distribution than it does to estimate the full distribution\nHere I introduce the Particle Filter. The particle filter is a sequential method that is familiar to folks that have learned the Bayesian methods that we have covered in the class. It has the concept of a likelihood, of which you are well versed. The particle filter does not assume a specific distribution of the data and model error so it requires more particles to estimate the full distributions. As a result, it is more appropriate for “smaller” problems like the ones we are tackling in this class.\nThe particle filter is quite simple\n\nInitialize a set of particles: Set the initial distribution of the states for each particle and, if estimating parameters, initial distribution of parameters that you want to estimate. Think of this as your initial priors.\nPredict the next time step using your process model for each particle. Add process uncertainty to each particle (i.e., rnorm)\nIf there are observations at the time-step, calculate the likelihood of the data given the particle just like we calculated the likelihood in the likelihood and Bayesian exercises. For example: LL &lt;- dnorm(obs, mean = pred, sd_obs). You will typically use the uncertainty of the observations in the likelihood because you have already included the process uncertainty in #2. If observations are not available at the time step, then continue to the time step.\nIf there are observations at the time-step, resample the particles using the likelihood as the weights (don’t forget to exponentiate the likelihood if you logged it in #3, the weights must be probabilities rather than log probabilities). The weighted sampling, with replacement, will randomly pick the more likely particles more often. You will keep the same number of particles but the values for each particle will change. Less likely particles will be replaced with more likely particles (though the less likelihood particles can still be selected). Be sure to resample all states together and, if also estimating parameters, the parameters as well. The key to resampling is the following:\n\n  ## calculate likelihood (weight) for one state with an observation at that time-step\n  ## The dimensiosn of x\n  wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_data)\n  \n  # Normalize the weights\n  wt_norm &lt;- wt / sum(wt)\n  \n  ## resample ensemble members in proportion to their weight.  \n  ## Since the total number of samples = the number of particles then you will preserve the \n  ## Same number of particles\n  resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n  \n  ##Use the index to resample\n  x[t, ] &lt;- x[t, resample_index]\n\nContinue to next time step.\n\nFundamentally, the particle filter depends on two concepts that you have already been exposed to: likelihood and sampling from a set of particles (you sampled iterations from MCMC in previous exercises).\nSpecifically we refer to the particle filter described above as a bootstrap particle filter"
  },
  {
    "objectID": "particle-filter.html#example-of-particle-filter",
    "href": "particle-filter.html#example-of-particle-filter",
    "title": "18  Data assimilation: particle filter",
    "section": "18.3 Example of Particle Filter",
    "text": "18.3 Example of Particle Filter\nHere is an example of the particle filter applied to the google flu data\nFirst load in data. We are only going to focus on the first 15 weeks of the data so that you can see the particles more easily\n\ngflu &lt;- read_csv(\"data/gflu_data.txt\", skip = 11, show_col_types = FALSE) %&gt;% \n  select(Date, Virginia) |&gt; \n  mutate(Date = as.Date(Date))\n         \n         \ngflu &lt;- gflu[1:15, ]\n\nggplot(data = gflu, aes(x = Date, y = log(Virginia))) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Date\", y = \"Log(flu cases)\", title = \"Virginia Google Flu Trends\")\n\n\n\n\n\n18.3.1 PF with no-observations\nFirst we are going to run the particle filter with all data missing. This is equal to the random walk.\nThe sd_add would have been determined from a state-space Bayesian MCMC chain that used previously available data.\nThe sd_obs is the observation uncertainty.\nThe sd_init sets the initial uncertainty in the model states (you could use the distribution of the last latent state from a state-space Bayesian MCMC chain)\nThe key decision is the num_particles. More is always better but it comes at a computational and computer memory cost.\n\nnum_particles &lt;- 25\n\ny &lt;- log(gflu$Virginia)\n\nnt &lt;- length(y)\n\n#This sets all the observations to NA after the first\ny[2:nt] &lt;- NA\n\nsd_init &lt;- 0.5\nsd_add &lt;- 0.2\nsd_obs &lt;- 0.2\n\nx &lt;- array(NA, dim = c(nt, num_particles))\n\nx[1, ] &lt;- rnorm(num_particles, mean = y[1], sd = sd_obs)\n\n### resampling bootstrap particle filter\n\nfor(t in 2:nt){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    x[t, m ] &lt;- x[t - 1, m  ] + rnorm(1, mean = 0, sd = sd_add)\n  }\n \n  ## analysis step\n  if(!is.na(y[t])){ \n\n      ## calculate Likelihood (weights)\n      wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      wt_norm &lt;- wt / sum(wt)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n    }\n}\n\nNow plot the particles individually\n\ntibble(time = 1:nt,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -time, names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  ggplot(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_line() +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"State of Virginia\")\n\n\n\n\nLets save the output as a different name so we can compare to a PF with observations\n\nx_no_obs &lt;- x\n\n\n\n18.3.2 PF with observations\nNow we can examine how the PF uses observations to update the model and modify the trajectory. The following is the same as above except that there are data at week 1, 5, and 10.\n\nnum_particles &lt;- 25\n\ny &lt;- log(gflu$Virginia)\n\nnt &lt;- length(y)\n\n#This sets all the observations to NA after the first\ny[2:nt] &lt;- NA\ny[c(5, 10)] &lt;- log(gflu$Virginia)[c(5, 10)]\n\nsd_init &lt;- 0.5\nsd_add &lt;- 0.2\nsd_obs &lt;- 0.2\n\nx &lt;- array(NA, dim = c(nt, num_particles))\n\nx[1, ] &lt;- rnorm(num_particles, mean = y[1], sd = sd_init)\n\n### resampling bootstrap particle filter\n\nfor(t in 2:nt){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    x[t, m ] &lt;- x[t - 1, m  ] + rnorm(1, 0, sd_add)\n  }\n \n  ## analysis step\n  if(!is.na(y[t])){\n\n      ## calculate Likelihood (weights)\n      wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      wt_norm &lt;- wt / sum(wt)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n    }\n}\n\nNow plot the particles with the observations. You can see how the particles are adjusted when data are present.\n\ntibble(time = 1:nt,\n       as_tibble(x),\n       obs = y) %&gt;% \n  pivot_longer(cols = -c(\"time\",\"obs\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(x = exp(x),\n         obs = exp(obs)) %&gt;% \n  ggplot(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_line() +\n  geom_point(aes(y = obs), color = \"red\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"State of Virginia\")\n\n\n\n\nSave the output as a different object to compare to other PF simulations\n\nx_with_obs &lt;- x\n\nNow we can compare the influence of data assimilation on the last 5 weeks of the time-series (think of this as a 5-week forecast)\n\nno_obs &lt;- tibble(time = gflu$Date,\n       as_tibble(x_no_obs)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"no obs\")\n\nwith_obs &lt;- tibble(time = gflu$Date,\n       as_tibble(x_with_obs)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"with obs\")\n\ncombined &lt;- bind_rows(no_obs, with_obs)\n\ngflu$obs_in_fit &lt;- exp(y)\n\ncombined %&gt;% \n  group_by(time, type) %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = gflu, aes(x = Date, y = Virginia), color = \"red\") +\n  geom_point(data = gflu, aes(x = Date, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"Google Flu Trends for Virginia\")\n\n\n\n\n\n\n18.3.3 PF with parameter estimation\nThe estimate of parameters using PF is also straightforward. Here we will estimate the b1 parameter in the Dynamic Linear Model from the state-space exercise. The DLM uses minimum temperature as a covariant\nFirst, create the observed data that includes the minimum temperature. (same as above and using the daymetr package from the previous exercise)\n\ngflu &lt;- read_csv(\"data/gflu_data.txt\", skip = 11, show_col_types = FALSE)  %&gt;% \n  select(Date, Virginia) |&gt; \n  mutate(Date = as.Date(Date))\n\n\ndf &lt;- daymetr::download_daymet(site = \"Blacksburg\",\n                               lat = 37.22,\n                               lon = -80.41,\n                               start = 2003,\n                               end = 2016,\n                               internal = TRUE)$data\n\ndf$date &lt;- as.Date(paste(df$year,df$yday,sep = \"-\"),\"%Y-%j\")\nTmean &lt;- mean(df$tmin..deg.c.)\ngflu$Tmin = df$tmin..deg.c.[match(gflu$Date,df$date)] - Tmean\n\ngflu &lt;- gflu[1:15, ]\nggplot(data = gflu, aes(x = Date, y = log(Virginia))) +\n  geom_line()\n\n\n\n\nNow modify the PF with the DLM model instead of the random walk. This should be familiar to you.\nThe values for b0, b2, and sd_add are the mean values from the MCMC chain in the previous assignment.\nWe are estimating b1 (the sensitivity to minimum temperature). Just like we need to initialize the states at the first time step, we will initialize the distribution of the b1 at the first time step using a normal distribution with a mean = b1_mean and sd = b1_sd. This were the mean an sd of b1 from the MCMC chain in the previous assignment.\n\ny &lt;- log(gflu$Virginia)\nnum_particles &lt;- 25\nnt &lt;- length(y)\n\ny[2:nt] &lt;- NA\ny[c(5, 10)] &lt;- log(gflu$Virginia)[c(5, 10)]\n\nsd_init &lt;- 0.5\nsd_add &lt;- 0.130144\nsd_obs &lt;- 0.2\n\nb0 &lt;- 0.482462\nb2 &lt;- -0.063887\nb1_mean &lt;- -0.002479\nb1_sd &lt;- 0.01\n\nx &lt;- array(NA, dim = c(nt, num_particles))\nx[1, ] &lt;- rnorm(num_particles, y[1], sd = sd_init)\n\nb1 &lt;- array(NA, dim = c(nt, num_particles))\nb1[1, ] &lt;- rnorm(num_particles, mean = b1_mean, sd = b1_sd)\n\nTmin &lt;- gflu$Tmin\n\nNow run the PF with the DLM model. You need to also carry through the values for b1.\nImportantly, the distribution of b1 carries through from the previous time-step. When there is an observation, b1 is resampled using the same index that the states are resampled. This ensures that the parameters match the states from the same particle.\n\nfor(t in 2:nt){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    \n    pred &lt;- x[t-1, m] + b0 + b1[t-1, m] * Tmin[t] + b2*x[t-1, m]\n    \n    x[t, m ] &lt;- pred + rnorm(1, mean = 0, sd = sd_add)\n    b1[t, m] &lt;- b1[t-1, m]\n  }\n  \n  ## analysis step\n  if(!is.na(y[t])){\n    \n    ## calculate Likelihood (weights)\n    wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n    \n    wt_norm &lt;- wt / sum(wt)\n    \n    ## resample ensemble members in proportion to their weight\n    resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n\n    x[t, ] &lt;- x[t, resample_index]  ## update state\n    b1[t, ] &lt;- b1[t, resample_index] ## Parameter update\n  }\n}\n\nNow visualize the states from the PF\n\ntibble(time = 1:nt,\n       obs = y,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -c(time,obs), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  ggplot() +\n  geom_line(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_point(aes(x = time, y = exp(obs)), color = \"red\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"State of Virginia\")\n\n\n\n\nAnd visualize the time-evolution of the parameter. There are two new concepts illustrated below:\n\nparameters distributions evolve through time. As a result the distribution of parameters is strongly influenced by the most recent observations. The distributions produced by a PF are not the same as the distributions produced by MCMC chain.\n\nparticles can have degeneracy, whereby the values of the parameters collapse down to one or a few values. This occurs because the PF does not propose new parameter values, it only selects (through resampling) parameter values from the initial set that you started with. Over the time-step the PF weeds out bad parameters and only a few ones are left. This is a major issue with a PF. Degeneracy can also occur in the states but since we are adding process uncertainty (sd_add) the particles are able to separate through time. There are a couple of ways to solve degeneracy - increase the number of particles so you sample more initial parameter values or propose new parameter values as part of the PF (how do to this is beyond the scope of this lecture)\n\n\ntibble(time = 1:nt,\n       as_tibble(b1)) %&gt;% \n  pivot_longer(cols = -c(time), names_to = \"ensemble\", values_to = \"b1\") %&gt;% \n  ggplot() +\n  geom_line(aes(x = time, y = b1, group = factor(ensemble))) +\n  labs(x = \"Date\", y = \"B1\", title = \"State of Virginia\")\n\n\n\n\n\n\n18.3.4 Sensitivity to number of particles\nThe number of particles is a key decision when using a PF. To explore the sensitivity of the PF to the number of particles, here is a function that can be reused with different numbers of particles. It is the same as the DLM above and returns the x and b1 for the particles in a list\n\nbootstrap_pf &lt;- function(num_particles, sd_add = 0.2, sd_obs = 0.2){\n  y &lt;- log(gflu$Virginia)\n  nt &lt;- length(y)\n  \n  y[2:nt] &lt;- NA\n  y[c(5, 10)] &lt;- log(gflu$Virginia)[c(5, 10)]\n  \n  sd_init &lt;- 0.5\n\n  b0 &lt;- 0.482462\n  b2 &lt;- -0.063887\n  b1_mean &lt;- -0.002479\n  b1_sd &lt;- 0.01\n  sd_add &lt;- 0.130144\n  \n  x &lt;- array(NA, dim = c(nt, num_particles))\n  \n  x[1, ] &lt;- rnorm(num_particles, mean = y[1], sd = sd_init)\n  \n  b1 &lt;- array(NA, dim = c(nt, num_particles))\n  b1[1, ] &lt;- rnorm(num_particles, b1_mean, sd = b1_sd)\n  \n  Tmin &lt;- gflu$Tmin\n  \n  ### resampling bootstrap particle filter\n  \n  for(t in 2:nt){\n    \n    ## forward step\n    for(m in 1:num_particles){\n      \n      pred &lt;- x[t-1, m] + b0 + b1[t-1, m] * Tmin[t] + b2*x[t-1, m]\n      \n      x[t, m ] &lt;- pred + rnorm(1, mean = 0, sd = sd_add)\n      b1[t, m] &lt;- b1[t-1, m]\n    }\n    \n    ## analysis step\n    if(!is.na(y[t])){\n      \n      ## calulate Likelihood (weights)\n      wt &lt;- dnorm(y[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      wt_norm &lt;- wt / sum(wt)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n      b1[t, ] &lt;- b1[t, resample_index] ## Parameter update\n    }\n  }\n  return(list(x = x, b1 = b1))\n}\n\nFirst, run the PF using 10, 100, 1000, and 10000 particles\n\npf_10 &lt;- bootstrap_pf(10)\npf_100 &lt;- bootstrap_pf(100)\npf_1000 &lt;- bootstrap_pf(1000)\npf_10000 &lt;- bootstrap_pf(10000)\n\nAnd combine to a single plot. You see the width of the 95 % confidence interval increases substantially from 10 to 1000 particles but then is similar from 1000 to 10000. This reflects what we learned from the very first exercise where you drew random samples from a distribution and found that between 1000 and 10000 random samples were required to approximate the distribution well.\n\np10 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_10$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_10\")\n\np100 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_100$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_100\")\n\np1000 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_1000$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_1000\")\n\np10000 &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_10000$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_10000\")\n\ngflu$obs_in_fit &lt;- exp(y)\n\nbind_rows(p10, p100, p1000, p10000) %&gt;% \n  group_by(time, type) %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = gflu, aes(x = Date, y = Virginia), color = \"red\") +\n  geom_point(data = gflu, aes(x = Date, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"Google Flu Trends for Virginia\")\n\n\n\n\n\n\n18.3.5 Senstivity to observation uncertainity\nWe can also explore the sensitivity of the PF updating to our observation uncertainty. Intuitively, we should have stronger update (i.e., the particles are adjusted to be closer to the observation) when there is less uncertainty in the observations. The code below explores whether this intuition is correct.\n\npf_low_obs &lt;- bootstrap_pf(5000, sd_obs = 0.1)\n\npf_high_obs &lt;- bootstrap_pf(5000, sd_obs = 0.5)\n\npf_low &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_low_obs$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"low obs uncertainity\")\n\npf_high &lt;- tibble(time = gflu$Date,\n       as_tibble(pf_high_obs$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"high obs uncertainity\")\n\ngflu$obs_in_fit &lt;- exp(y)\n\nbind_rows(pf_low, pf_high) %&gt;% \n  group_by(time, type) %&gt;% \n  mutate(x = exp(x)) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = gflu, aes(x = Date, y = Virginia), color = \"red\") +\n  geom_point(data = gflu, aes(x = Date, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Date\", y = \"Flu cases\", title = \"Google Flu Trends for Virginia\")"
  },
  {
    "objectID": "particle-filter.html#problem-set",
    "href": "particle-filter.html#problem-set",
    "title": "18  Data assimilation: particle filter",
    "section": "18.4 Problem set",
    "text": "18.4 Problem set\nYou will need the following packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n18.4.1 Overview\nThis exercise involves the following objectives\n\nModify the particle filter examples to apply to a new model. The model is defined as NIMBLE code so you have to convert to a particle filter.\nUse out put from a state-space model fit to initialize the particle filter\nRun the particle filter without observations to forecast\nRun the particle filter to assimilate new observations and forecast\nEvaluate how the forecast depends on data assimilation\n\n\n18.4.1.1 Data\nThe data for this exercise is above biomass vs. age for a single forest plot. The data has two columns: age (in years) and biomass (in gC/m2)\n\n\n18.4.1.2 Model\nWe are predicting the aboveground biomass using the following model\nbiomass = previous biomass + constant growth - mortality rate * previous biomass\nThe constant growth is the parameter g below (in units of gC/m2/yr) and mortality rate is the parameter ubelow (proportion of biomass per year). We fit the model as a state-space model.\n\n\n\n18.4.2 Part 1: Fit model to historical data (Already done for you!)\nThis step is already done for you.\nHere is the data for ages 1 through 50 for the plot (starting in 1950-01-01). It was measured every 5 years.\n\nplot_data &lt;- read_csv(\"data/PF_data1.csv\", show_col_types = FALSE)\n\nggplot(plot_data, aes(x = datetime, y = biomass)) +\n  geom_point() +\n  labs(x = \"age\", y = \"aboveground biomass (gC/m2)\")\n\n\n\n\nThe following model was used to estimate the posterior distributions of the parameters using a Bayesian framework.\nsim_dates &lt;- seq(as_date(\"1950-01-01\"), length.out = 50, by = \"1 year)\nbiomass[1, ] &lt;- 0\n\nfor(t in 2:length(sim_dates)){\n\n  for(i in 1:num_particles){\n\n    biomass_predicted &lt;- biomass[t+1, i] + param$g - param$u\n\n    biomass[t, i] &lt;- rnorm(1, mean = biomass_predicted , sd = param$sd_add)\n  }\n}\nThe MCMC chain has posterior distributions for the parameters (g, u, sd_add) and biomass at age 50 (age50_biomass)\n\ndf &lt;- read_csv(\"data/PF_MCMC.csv\", show_col_types = FALSE)\n\n\n\n18.4.3 Part 2: Forecast using PF\nNow you will use the MCMC chain to determine the mean parameter values and the initial condition at age 50 for the particle filter.\nUsing the lecture material create a particle filter that uses the forest growth model to simulate the aboveground biomass of the forest for age 50 through 70.\n\n18.4.3.1 Step 1: Set up PF\nFollowing the code in the PF lecture set up the particle filter.\nInstead of using data from a file you will use the following for your data: obs &lt;- NULL\nBe sure to:\n\nuse the mean values for g, u, and sd_add from the MCMC chain as the parameter values\nuse the distribution of the biomass at age 50 in the MCMC chain as your initial state for the PF\nThe standard deviation for the obseravtions (sd_data) is 200.\n\n\n#ADD CODE TO SET UP PF HERE\n\n\n\n18.4.3.2 Step 2: Run particle filter\nWrite the code and run the particle filter based on the examples from the lecture. You will need to include the process model that is in the NIMBLE code above.\n\n#ADD PF CODE HERE\n\n\n\n18.4.3.3 Step 3: Visualize particle filter output\nGenerate a plot that visualizes the output of the PF (see examples from the lecture). Your plot must have age on the x-axis and biomass on the y-axis with different lines for the particles.\n\n# ADD VISUALIZATION CODE HERE\n\n\n\n18.4.3.4 Step 4: Save PF output\nuse this code to save your PF output as the object initial_forecast\n\n\n\n18.4.4 Part 3:\nNow we have new data!\n\nnew_data &lt;- read_csv(\"data/PF_data2.csv\", show_col_types = FALSE)\n\nggplot(new_data, aes(x = datetime, y = biomass)) +\n  geom_point() +\n  labs(x = \"age\", y = \"aboveground biomass (gC/m2)\")\n\n\n\n\n\n18.4.4.1 Step 1: Repeat the PF setup\nUsing the new data, repeat the PF set up in Part 2 Step 1. You will be starting at age 50 just like above.\n\n#ADD CODE TO SET UP PF HERE\n\n\n\n18.4.4.2 Step 2: Run particle filter using the new data\nUsing the new data, run the the PF again. This will be the same code as in Part 2 Step 2 (just copy and paste)\n\n#COPY AND PASTE PF CODE FROM ABOVE\n\n\n\n18.4.4.3 Step 3: Visualize PF output\nGenerate a plot that visualizes the output of the PF (see examples from the lecture). Your plot must have age on the x-axis and biomass on the y-axis with different lines for the particles. Your observations from the new data must be on the plot.\n\n#ADD VISUALIZATION CODE HERE\n\n\n\n18.4.4.4 Step 4: Save output\n\n\n\n18.4.5 Part 4:\nCombine the two PF forecast and evaluate how data assimilation influence the forecast of the last 10 years (age 60 to 70). Produce a plot with the mean and 90% CI for the initial_forecast and assimilated_forecast on the same plot. Include the observations from the new data set.\n\n#ADD CODE TO COMPARE THE TWO PF OUTPUTS\n\n\n\n18.4.6 Part 5:\nAnswer the follow question\nHow did assimilating data influence your forecast for ages 60 to 70? Consider both the mean and uncertainty in your answer."
  },
  {
    "objectID": "pulling-together.html#example-of-the-forecast-and-analysis-cycle",
    "href": "pulling-together.html#example-of-the-forecast-and-analysis-cycle",
    "title": "19  Putting it together: forecast - analysis cycle",
    "section": "19.1 Example of the forecast and analysis cycle",
    "text": "19.1 Example of the forecast and analysis cycle\n\n19.1.1 Step 1: Set initial conditions and parameter distributions\nWe will start our forecast - analysis cycle by running the model with a particle filter over a period of time in the past. This is designed to spin-up the model and set the initial states and parameters for a forecast.\n\nens_members &lt;- 100\nsim_dates &lt;- seq(Sys.Date()-365, Sys.Date()-1, by = \"1 day\")\n\n\ninputs &lt;- get_historical_met(site = \"TALL\", sim_dates, use_mean = FALSE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\n\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(70, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.1, ens_members) #0.01 \nparams$sigma.stem &lt;- rep(1, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(1, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n\nobs &lt;- read_csv(\"data/site_carbon_data.csv\", show_col_types = FALSE)\nstate_init &lt;- rep(NA, 3)\n\nstate_init[1] &lt;- obs |&gt; \n  filter(variable == \"lai\",\n         datetime %in% sim_dates) |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  mutate(observation = observation / (mean(params$SLA) * 0.1)) |&gt; \n  pull(observation)\n\nstate_init[2] &lt;- obs |&gt; \n  filter(variable == \"wood\") |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  pull(observation)\n\nstate_init[3] &lt;- obs |&gt; \n  filter(variable == \"som\") |&gt; \n  na.omit() |&gt;  \n  slice(1) |&gt; \n  pull(observation)\n\n\n#Set initial conditions\nforecast &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\nforecast[1, , 1] &lt;- rnorm(ens_members, state_init[1], sd = 0.1)\nforecast[1, , 2] &lt;- rnorm(ens_members, state_init[2], sd = 10)\nforecast[1, , 3] &lt;- rnorm(ens_members, state_init[2], sd = 20)\n\nwt &lt;- array(1, dim = c(length(sim_dates), ens_members))\n\n\nfit_params_table &lt;- read_csv(\"data/saved_parameter_chain.csv\", show_col_types = FALSE) |&gt; \n  pivot_wider(names_from = parameter, values_from = value)\n\nnum_pars &lt;- 2\nfit_params &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nsamples &lt;- sample(1:nrow(fit_params_table), size = ens_members, replace = TRUE)\nfit_params[1, , 1] &lt;- fit_params_table$alpha[samples]\nfit_params[1, , 2] &lt;- fit_params_table$Rbasal[samples]\n\n\nfor(t in 2:length(sim_dates)){\n  \n  fit_params[t, , 1] &lt;- rnorm(ens_members, fit_params[t-1, ,1], sd = 0.0005)\n  fit_params[t, , 2] &lt;- rnorm(ens_members, fit_params[t-1, ,2], sd = 0.00005)\n  \n  params$alpha  &lt;- fit_params[t, , 1]\n  params$Rbasal  &lt;- fit_params[t, , 2]\n  \n  forecast[t, , ]  &lt;- forest_model(t, \n                                   states = matrix(forecast[t-1 , , 1:3], nrow = ens_members) , \n                                   parms = params, \n                                   inputs = matrix(inputs_ensemble[t , , ], nrow = ens_members))\n  \n  analysis &lt;- particle_filter(t, forecast, obs, sim_dates, wt, fit_params)\n  \n  forecast &lt;- analysis$forecast\n  fit_params &lt;- analysis$fit_params\n  wt &lt;- analysis$wt\n}\n\n\nsave(analysis, file = \"data/PF_analysis_0.Rdata\")\n\n\nforecast_weighted &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12))\nparams_weighted &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nfor(t in 1:length(sim_dates)){\n  wt_norm &lt;-  wt[t, ]/sum(wt[t, ])\n  resample_index &lt;- sample(1:ens_members, ens_members, replace = TRUE, prob = wt_norm ) \n  forecast_weighted[t, , ] &lt;- forecast[t, resample_index, 1:12] \n  params_weighted[t, , ] &lt;- fit_params[t,resample_index, ] \n}\noutput_df &lt;- output_to_df(forecast_weighted, sim_dates, sim_name = \"parameter_unc\")\nparameter_df &lt;- parameters_to_df(params_weighted, sim_dates, sim_name = \"parameter_unc\", param_names = c(\"alpha\",\"Rbasal\"))\n\n\n\n\n\n\n\n\n19.1.2 Step 2: Generate forecaset\nNow we are going to generate a forecast that started from our 1 year spin up completed above. First load the analysis data into memory so that we can access the initial conditions and parameters in it.\n\nload(\"data/PF_analysis_0.Rdata\")\n\nThis example introduces the concept of a “look back”. This is stepping back in the past to restart the particle filter so that it can assimilate any data that has been make avialable since the last time that time period was run through the particle filter. The look back is important because many observations have delays before becoming avialable (called latency). NEON flux data can have a 5 day to 1.5 month latency. MODIS LAI is a 8 day average so has an 8-day latency.\nHere we use a look back of 14-days. Since our forecast horizon is 30-days in the future, the total days of the simulation is 14 + 30. Our reference_datetime is today.\nWe will find the first day of our simulation (14 days ago) in the last saved analysis and use this for initial conditions.\n\nlook_back &lt;- 14\nhorizon &lt;- 30\nreference_datetime &lt;- Sys.Date()\nsim_dates &lt;- seq(reference_datetime - look_back, length.out = look_back + horizon, by = \"1 day\")\nindex &lt;- which(analysis$sim_dates == sim_dates[1])\n\nThe use of the loop back requires combining the “historial” weather and future weather into a single input data frame.\n\ninputs_past &lt;- get_historical_met(site = \"TALL\", sim_dates[1:(look_back-2)], use_mean = FALSE)\ninputs_future &lt;- get_forecast_met(site = \"TALL\", sim_dates[(look_back-1):length(sim_dates)], use_mean = FALSE)\ninputs &lt;- bind_rows(inputs_past, inputs_future)\n\nThe combined weather drivers look like the following. The vertical line is where the look back period transitions to the future.\n\n\n\n\n\n\nnum_pars &lt;- 2\nfit_params &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nsamples &lt;- sample(1:nrow(fit_params_table), size = ens_members, replace = TRUE)\nfit_params[1, , 1] &lt;- analysis$fit_params[index, ,1]\nfit_params[1, , 2] &lt;- analysis$fit_params[index, ,1]\n\n\n#Set initial conditions\nforecast &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\nforecast[1, , 1] &lt;- analysis$forecast[index, ,1]\nforecast[1, , 2] &lt;- analysis$forecast[index, ,2]\nforecast[1, , 3] &lt;- analysis$forecast[index, ,3]\n\nwt &lt;- array(1, dim = c(length(sim_dates), ens_members))\nwt[1, ] &lt;- analysis$wt[index, ]\n\n\nfor(t in 2:length(sim_dates)){\n  \n  fit_params[t, , 1] &lt;- rnorm(ens_members, fit_params[t-1, ,1], sd = 0.0005)\n  fit_params[t, , 2] &lt;- rnorm(ens_members, fit_params[t-1, ,2], sd = 0.00005)\n  \n  params$alpha  &lt;- fit_params[t, , 1]\n  params$Rbasal  &lt;- fit_params[t, , 2]\n  \n  if(t &gt; 1){\n  \n  forecast[t, , ]  &lt;- forest_model(t, \n                                   states = matrix(forecast[t-1 , , 1:3], nrow = ens_members) , \n                                   parms = params, \n                                   inputs = matrix(inputs_ensemble[t , , ], nrow = ens_members))\n  }\n  \n  analysis &lt;- particle_filter(t, forecast, obs, sim_dates, wt, fit_params)\n  \n  forecast &lt;- analysis$forecast\n  fit_params &lt;- analysis$fit_params\n  wt &lt;- analysis$wt\n}\n\n\n\n19.1.3 Step 3: Save forecast and data assimiliation output\nConvert forecast to a dataframe.\n\noutput_df &lt;- output_to_df(forecast, sim_dates, sim_name = \"simple_forest_model\")\n\nVisualize forecast. The vertical line is where the look back period transitions to the future.\n\n\n\n\n\nSave the states and weights for use as initial conditions in the next forecast.\n\nsave(analysis, file = \"data/PF_analysis_1.Rdata\")\n\nConvert to the format required by the NEON Ecological Forecasting Challenge\n\nefi_output &lt;- output_df |&gt; \n  mutate(datetime = as_datetime(datetime),\n         duration = \"P1D\",\n         project_id = \"neon4cast\",\n         site_id = \"TALL\",\n         family = \"ensemble\",\n         reference_datetime = as_datetime(reference_datetime)) |&gt; \n  rename(parameter = ensemble) |&gt; \n  filter(datetime &gt;= reference_datetime) \n\nWrite the forecast to a csv file\n\nfile_name &lt;- paste0(\"data/terrestrial_daily-\", reference_datetime, \"-simple_forest_model.csv\")\nwrite_csv(efi_output, file_name)\n\nand submit to the Challenge\n\nneon4cast::submit(file_name, ask = FALSE)\n\n\n\n19.1.4 Step 4: Repeat Steps 2 -3\nWait for a day to pass and then use yesterday’s analysis today for initial conditions and parameter distributions\n\nload(\"data/PF_analysis_1.Rdata\")"
  },
  {
    "objectID": "pulling-together.html#code-example-of-forecast-analysis-cycle-implimented-in-a-more-complex-forest-model",
    "href": "pulling-together.html#code-example-of-forecast-analysis-cycle-implimented-in-a-more-complex-forest-model",
    "title": "19  Putting it together: forecast - analysis cycle",
    "section": "19.2 Code example of forecast analysis cycle implimented in a more complex forest model",
    "text": "19.2 Code example of forecast analysis cycle implimented in a more complex forest model\nhttps://github.com/mdietze/FluxCourseForecast"
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "20  Project 2",
    "section": "",
    "text": "Project 2 challenges you to\n\nEither modify the process model to better represent an ecosystem process or include a new process that is missing, or\nApply the process model to a new NEON site.\n\nYou are ask to provide a kitted R markdown (or Quarto) document with text, code, and plots following:\n\nYour forest process model\nParameter estimation either using likelihood, Bayesian batch, or particle filter methods\nA 30-day ahead forecast using your model"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "21  Summary",
    "section": "",
    "text": "PENDING"
  },
  {
    "objectID": "weather-drivers.html#historical-weather",
    "href": "weather-drivers.html#historical-weather",
    "title": "22  Accessing weather drivers",
    "section": "22.1 “Historical weather”",
    "text": "22.1 “Historical weather”\nOur goal is to generate genuine forecasts of the future. Therefore we need to use weather forecasts as driver inputs into our model.\nThis opens a critical issue when calibrating a model that is then used for forecasting. A model could be calibrated using observed weather from a weather station as inputs. As a result the parameters are tuned to the weather from the station. However, the bias and statistics of the observed weather may be different from the forecasted weather since weather forecasts are for a larger grid (e.g., the weather forecast may be consistently warmer by 2 degrees C). This offset in the meteorology used to calibrate and forecast could result in the forecast being biased. The issue can be fixed by either adjusting the weather forecast to be more consistent with the observed weather or calibrating the model using weather inputs from the weather forecast model. In this book we use the later.\nThe get_historical_met function gets the first day of all the weather forecasts generated since September 2020. The first day of each forecast is the closest to what was observed because it directly follows data assimilation. Combing the first days together gives a complete time series of weather until the present day.\nThe function is print out here so that you can modify if you want to add variables or process in a different way.\n\nget_historical_met &lt;- function(site, sim_dates, use_mean = TRUE){\n\n  if(use_mean){\n    groups &lt;- c(\"datetime\", \"variable\")\n  }else{\n    groups &lt;- c(\"datetime\", \"variable\",\"parameter\")\n  }\n  site &lt;- \"TALL\"\n  met_s3 &lt;- arrow::s3_bucket(paste0(\"bio230014-bucket01/neon4cast-drivers/noaa/gefs-v12/stage3/site_id=\", site),\n                             endpoint_override = \"sdsc.osn.xsede.org\",\n                             anonymous = TRUE)\n\n  inputs_all &lt;- arrow::open_dataset(met_s3) |&gt;\n    filter(variable %in% c(\"air_temperature\", \"surface_downwelling_shortwave_flux_in_air\")) |&gt;\n    mutate(datetime = as_date(datetime)) |&gt;\n    mutate(prediction = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", prediction/0.486, prediction),\n           variable = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", \"PAR\", variable),\n           prediction = ifelse(variable == \"air_temperature\", prediction - 273.15, prediction),\n           variable = ifelse(variable == \"air_temperature\", \"temp\", variable)) |&gt;\n    summarise(prediction = mean(prediction, na.rm = TRUE), .by =  all_of(groups)) |&gt;\n    mutate(doy = yday(datetime)) |&gt;\n    filter(datetime %in% sim_dates) |&gt;\n    collect()\n\n  if(use_mean){\n    inputs_all &lt;- inputs_all |&gt;\n      mutate(parameter = \"mean\")\n  }\n\n  return(inputs_all)\n}"
  },
  {
    "objectID": "weather-drivers.html#forecasts",
    "href": "weather-drivers.html#forecasts",
    "title": "22  Accessing weather drivers",
    "section": "22.2 Forecasts",
    "text": "22.2 Forecasts\nAn individual forecast can be accessed using the get_forecast_met function. This provides the full horizon (35-days ahead) for a forecast that was generated on a specific day (reference_datetime).\nThe function is print out here so that you can modify if you want to add variables or process in a different way.\n\nget_forecast_met &lt;- function(site, sim_dates, use_mean = TRUE){\n\n  if(use_mean){\n    groups &lt;- c(\"datetime\", \"variable\")\n  }else{\n    groups &lt;- c(\"datetime\", \"variable\",\"parameter\")\n  }\n\n  met_s3 &lt;- arrow::s3_bucket(paste0(\"bio230014-bucket01/neon4cast-drivers/noaa/gefs-v12/stage2/reference_datetime=\",sim_dates[1],\"/site_id=\",site),\n                             endpoint_override = \"sdsc.osn.xsede.org\",\n                             anonymous = TRUE)\n\n  inputs_all &lt;- arrow::open_dataset(met_s3) |&gt;\n    filter(variable %in% c(\"air_temperature\", \"surface_downwelling_shortwave_flux_in_air\")) |&gt;\n    mutate(datetime = as_date(datetime)) |&gt;\n    mutate(prediction = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", prediction/0.486, prediction),\n           variable = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", \"PAR\", variable),\n           prediction = ifelse(variable == \"air_temperature\", prediction- 273.15, prediction),\n           variable = ifelse(variable == \"air_temperature\", \"temp\", variable)) |&gt;\n    summarise(prediction = mean(prediction, na.rm = TRUE), .by = all_of(groups)) |&gt;\n    mutate(doy = yday(datetime)) |&gt;\n    filter(datetime %in% sim_dates) |&gt;\n    collect()\n\n  if(use_mean){\n    inputs_all &lt;- inputs_all |&gt;\n      mutate(parameter = \"mean\")\n  }\n\n  return(inputs_all)\n}"
  }
]