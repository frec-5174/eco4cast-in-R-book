[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Practical Guide to Ecosystem Forecasting",
    "section": "",
    "text": "Welcome\nThis book is designed for a graduate class in ecological forecasting (Virginia Tech graduate course FREC 5174: Ecological Modeling and Forecasting) and is used as a how-to guide for ecological forecasting. It leverages work through the NSF-funded Macrosystems EDDIE (DEB-1926050) and Ecological Forecasting Initiative Research Coordination Network (DEB-1926388) projects. It includes training materials developed by the Macrosystems EDDIE forecasting team at Virginia Tech (Cayelan Carey, Mary Lofton, Tadhg Moore, Whitney Woelmer), Freya Olsson (NEON Ecological Forecasting Challenge workshop), Mike Dietze (Terrestrial carbon flux tutorial and modified activities from his Ecological Forecasting; Dietze (2017)), and John Smith (NEON carbon data processing code). The imprint of Carl Boettiger on the materials is throughout the book.\nThe book uses R code to teach concepts by example (hence the name “A practical guide”). The book includes very few equations because often I find myself wishing I had code to better understand equations in papers. It also uses a “from scratch” approach that provides code with the details of key algorithms (like Markov Chain Monte Carlo Metropolis-Hastings and the bootstrap particle filter). Future work on this book could add examples of how to implement the analyses using existing R packages (e.g., JAGS and Nimble). In practice, using these packages is better because they can improve the efficiency and accuracy of estimates. Finally, there are a few places where I make functions that hide the details but these are cases where the algorithms are highly specific to the application (e.g., access weather data from a specific server).\nThe book is a near-term iterative forecast itself. It automatically rebuilds every day using GitHub actions. Each build accesses 1) the latest NEON data and NOAA weather forecasts, 2) assimilates the data into a process-based ecosystem model, 3) and generates a new forecast of daily net ecosystem exchange (NEE) for one NEON site (OSBS). The forecast is submitted to the NEON Ecological Forecasting Challenge. As a result, the NEON data in Chapter 18  Data to constrain process model and the forecast in Chapter 21  Forecast - analysis cycle should be near real-time.\nThe terms ecosystem forecasting and ecological forecasting will be used interchangeably throughout the book. The use of ecosystem forecasting in the title reflects the focus on ecosystem-scale data in the book and distinguishes it from Mike Dietze’s excellent book called Ecological Forecasting Dietze (2017).\nThe book is a work in progress that involves harmonizing materials developed for many different use cases (undergraduate courses, tutorials, graduate classes). As a result, there are likely to be differences among chapters in style and approaches to assignments.\nI welcome feedback on the book through GitHub issues and/or Pull Requests.\nThis online book is licensed under the CC BY-NC-ND 4.0 license\n\n\n\n\nDietze, Michael C. 2017. Ecological Forecasting. Princeton: Princeton University Press.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Learning objectives\nHaving completed the course a student will be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "1  Introduction",
    "section": "",
    "text": "Create computer models that mathematically represent an ecological system,\nApply maximum likelihood methods to estimate parameters in ecological models using data,\nApply Bayesian methods to estimate parameter distributions in ecological models using data,\nApply sequential data assimilation to improve ecological model predictions, and\nCreate, evaluate, and interpret an ecological forecast of the future that includes uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#readings",
    "href": "intro.html#readings",
    "title": "1  Introduction",
    "section": "1.2 Readings",
    "text": "1.2 Readings\nClark, J. S., Carpenter, S. R., Barber, M., Collins, S., Dobson, A., Foley, J. A., et al. (2001). Ecological Forecasts: An Emerging Imperative. Science, 293(5530), 657–660. https://doi.org/10.1126/science.293.5530.657\nDietze, M., & Lynch, H. (2019). Forecasting a bright future for ecology. Frontiers in Ecology and the Environment, 17(1), 3–3. https://doi.org/10.1002/fee.1994\nThomas, R. Q., Boettiger, C., Carey, C. C., Dietze, M. C., Johnson, L. R., Kenney, M. A., et al. (2023). The NEON Ecological Forecasting Challenge. Frontiers in Ecology and the Environment, 21(3), 112–113. https://doi.org/10.1002/fee.2616\n\n\n\n\nMoore, Tadhg N., R. Quinn Thomas, Whitney M. Woelmer, and Cayelan C. Carey. 2022. “Integrating Ecological Forecasting into Undergraduate Ecology Curricula with an R Shiny Application-Based Teaching Module.” Forecasting 4 (3): 604–33. https://doi.org/10.3390/forecast4030033.\n\n\nThomas, R Quinn, Carl Boettiger, Cayelan C Carey, Michael C Dietze, Leah R Johnson, Melissa A Kenney, Jason S McLachlan, et al. 2023. “The NEON Ecological Forecasting Challenge.” Frontiers in Ecology and the Environment 21 (3): 112–13. https://doi.org/10.1002/fee.2616.\n\n\nWoelmer, Whitney M., Tadhg N. Moore, Mary E. Lofton, R. Quinn Thomas, and Cayelan C. Carey. 2023. “Embedding Communication Concepts in Forecasting Training Increases Students’ Understanding of Ecological Uncertainty.” Ecosphere 14 (8): e4628. https://doi.org/10.1002/ecs2.4628.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro-ecoforecast.html",
    "href": "intro-ecoforecast.html",
    "title": "2  Introduction to Ecological Forecasting",
    "section": "",
    "text": "2.1 Reading\nDietze, M. C., Fox, A., Beck-Johnson, L. M., Betancourt, J. L., Hooten, M. B., Jarnevich, C. S., et al. (2018). Iterative near-term ecological forecasting: Needs, opportunities, and challenges. Proc Natl Acad Sci U S A, 115(7), 1424–1432. https://doi.org/10.1073/pnas.1710231115",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ecological Forecasting</span>"
    ]
  },
  {
    "objectID": "intro-ecoforecast.html#problem-set",
    "href": "intro-ecoforecast.html#problem-set",
    "title": "2  Introduction to Ecological Forecasting",
    "section": "2.2 Problem set",
    "text": "2.2 Problem set\nComplete activities A, B, and C in the R Shiny application and submit answers to the questions in the assignment word document",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ecological Forecasting</span>"
    ]
  },
  {
    "objectID": "intro-ecoforecast.html#module-references",
    "href": "intro-ecoforecast.html#module-references",
    "title": "2  Introduction to Ecological Forecasting",
    "section": "2.3 Module references",
    "text": "2.3 Module references\nMoore, T.N., R.Q. Thomas, W.M. Woelmer, C.C Carey. 2022. Integrating ecological forecasting into undergraduate ecology curricula with an R Shiny application-based teaching module. Forecasting 4:604-633. https://doi.org/10.3390/forecast4030033\nLofton, M.E., T.N. Moore, W.M. Woelmer, R.Q. Thomas, and C.C. Carey. A modular curriculum to teach undergraduates ecological forecasting improves student and instructor confidence in their data science skills. ESS Open Archive. https://doi.org/10.22541/essoar.171269260.08508117/v1\nThis module was developed by Moore, T.N., Lofton, M.E., C.C. Carey, and R.Q. Thomas. 03 July 2023. Macrosystems EDDIE: Introduction to Ecological Forecasting. Macrosystems EDDIE Module 5, Version 2. http://module5.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Ecological Forecasting</span>"
    ]
  },
  {
    "objectID": "understand-uncertainty.html",
    "href": "understand-uncertainty.html",
    "title": "3  Understanding Uncertainty in Ecological Forecasts",
    "section": "",
    "text": "3.1 Option: code based assignment\nThis chapter sets the foundation for Chapter 9. In prep for Chapter 9, there is a code-based version of this chapter that provides example code and introduces key concepts that will guide your work adding uncertainty to your forecast from Chapter 8.\nThe code-based assignment will require you to create a GitHub repository with your completed code from the NEON Forecasting Challenge workshop Chapter 8. You will build on this repository throughout the rest of section 1 of the book. You can learn more about setting up Git and GitHub in Section 7.3.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Uncertainty in Ecological Forecasts</span>"
    ]
  },
  {
    "objectID": "understand-uncertainty.html#reading",
    "href": "understand-uncertainty.html#reading",
    "title": "3  Understanding Uncertainty in Ecological Forecasts",
    "section": "3.2 Reading",
    "text": "3.2 Reading\nDietze, M. C., Fox, A., Beck-Johnson, L. M., Betancourt, J. L., Hooten, M. B., Jarnevich, C. S., et al. (2018). Iterative near-term ecological forecasting: Needs, opportunities, and challenges. Proc Natl Acad Sci U S A, 115(7), 1424–1432. https://doi.org/10.1073/pnas.1710231115",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Uncertainty in Ecological Forecasts</span>"
    ]
  },
  {
    "objectID": "understand-uncertainty.html#problem-set",
    "href": "understand-uncertainty.html#problem-set",
    "title": "3  Understanding Uncertainty in Ecological Forecasts",
    "section": "3.3 Problem set",
    "text": "3.3 Problem set\n\nFork repository for the assignment.\nComplete activities A, B, and C (through question 16) in the Rmarkdown document. You will be modifying the Rmarkdown document with the answers to the questions (both text and code).\nKnit the Rmarkdown Rmd to html.\nCommit Rmarkdown and HTML file to GitHub",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Uncertainty in Ecological Forecasts</span>"
    ]
  },
  {
    "objectID": "understand-uncertainty.html#module-reference",
    "href": "understand-uncertainty.html#module-reference",
    "title": "3  Understanding Uncertainty in Ecological Forecasts",
    "section": "3.4 Module Reference",
    "text": "3.4 Module Reference\nLofton, M.E., T.N. Moore, W.M. Woelmer, R.Q. Thomas, and C.C. Carey. A modular curriculum to teach undergraduates ecological forecasting improves student and instructor confidence in their data science skills. ESS Open Archive. https://doi.org/10.22541/essoar.171269260.08508117/v1\nThis module was developed by Moore, T. N., Lofton, M.E., Carey, C.C. and Thomas, R. Q. 24 July 2023. Macrosystems EDDIE: Understanding Uncertainty in Ecological Forecasts. Macrosystems EDDIE Module 6, Version 2. http://module6.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Uncertainty in Ecological Forecasts</span>"
    ]
  },
  {
    "objectID": "using-data.html",
    "href": "using-data.html",
    "title": "4  Using data to improve ecological forecasts",
    "section": "",
    "text": "4.1 Reading\nNiu, S., Luo, Y., Dietze, M. C., Keenan, T. F., Shi, Z., Li, J., & III, F. S. C. (2014). The role of data assimilation in predictive ecology. Ecosphere, 5(5), art65-16. https://doi.org/10.1890/ES13-00273.1",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using data to improve ecological forecasts</span>"
    ]
  },
  {
    "objectID": "using-data.html#problem-set",
    "href": "using-data.html#problem-set",
    "title": "4  Using data to improve ecological forecasts",
    "section": "4.2 Problem set",
    "text": "4.2 Problem set\nComplete activities A, B, and C in the R Shiny application and submit answers to the questions in the assignment word document",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using data to improve ecological forecasts</span>"
    ]
  },
  {
    "objectID": "using-data.html#module-references",
    "href": "using-data.html#module-references",
    "title": "4  Using data to improve ecological forecasts",
    "section": "4.3 Module references",
    "text": "4.3 Module references\nLofton, M.E., T.N. Moore, W.M. Woelmer, R.Q. Thomas, and C.C. Carey. A modular curriculum to teach undergraduates ecological forecasting improves student and instructor confidence in their data science skills. ESS Open Archive. https://doi.org/10.22541/essoar.171269260.08508117/v1\nThis module was developed by: Lofton, M.E., T.N. Moore, Thomas, R.Q., and C.C. Carey. 20 September 2022. Macrosystems EDDIE: Using Data to Improve Ecological Forecasts. Macrosystems EDDIE Module 7, Version 1. https://macrosystemseddie.shinyapps.io/module7. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nThis module has been peer-reviewed and included in the “On the Cutting Edge Exemplary Teaching Activities” collection.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Using data to improve ecological forecasts</span>"
    ]
  },
  {
    "objectID": "decision-making.html",
    "href": "decision-making.html",
    "title": "5  Using Ecological Forecasts to Guide Decision Making",
    "section": "",
    "text": "5.1 Reading\nSpiegelhalter, D., Pearson, M., & Short, I. (2011). Visualizing Uncertainty About the Future. Science, 333(6048), 1393–1400. https://doi.org/10.1126/science.1191181",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using Ecological Forecasts to Guide Decision Making</span>"
    ]
  },
  {
    "objectID": "decision-making.html#problem-set",
    "href": "decision-making.html#problem-set",
    "title": "5  Using Ecological Forecasts to Guide Decision Making",
    "section": "5.2 Problem Set",
    "text": "5.2 Problem Set\nComplete activities A, B, and C in the R Shiny application. The answers to the questions will be provided within the Shiny App.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using Ecological Forecasts to Guide Decision Making</span>"
    ]
  },
  {
    "objectID": "decision-making.html#module-references",
    "href": "decision-making.html#module-references",
    "title": "5  Using Ecological Forecasts to Guide Decision Making",
    "section": "5.3 Module references",
    "text": "5.3 Module references\nWoelmer, W.M., T.N. Moore, M.E. Lofton, R.Q. Thomas, and C.C. Carey. 2023. Embedding communication concepts in forecasting training increases students’ understanding of ecological uncertainty Ecosphere 14: e4628 https://doi.org/10.1002/ecs2.4628\nLofton, M.E., T.N. Moore, W.M. Woelmer, R.Q. Thomas, and C.C. Carey. A modular curriculum to teach undergraduates ecological forecasting improves student and instructor confidence in their data science skills. ESS Open Archive. https://doi.org/10.22541/essoar.171269260.08508117/v1\nThis module was developed by W.M. Woelmer, R.Q. Thomas, T.N. Moore and C.C. Carey. 21 January 2021. Macrosystems EDDIE: Using Ecological Forecasts to Guide Decision-Making. Macrosystems EDDIE Module 8, Version 1. http://module8.macrosystemseddie.org. Module development was supported by NSF grants DEB-1926050 and DBI-1933016.\nThis module has been peer-reviewed and included in the “On the Cutting Edge Exemplary Teaching Activities” collection.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using Ecological Forecasts to Guide Decision Making</span>"
    ]
  },
  {
    "objectID": "best-practices.html",
    "href": "best-practices.html",
    "title": "6  Best Practices in Ecological Forecasting",
    "section": "",
    "text": "6.1 Reading\nHobday, A. J., Hartog, J. R., Manderson, J. P., Mills, K. E., Oliver, M. J., Pershing, A. J., & Siedlecki, S. (2019). Ethical considerations and unanticipated consequences associated with ecological forecasting for marine resources. ICES Journal of Marine Science. https://doi.org/10.1093/icesjms/fsy210\nLewis, A. S. L., Woelmer, W. M., Wander, H. L., Howard, D. W., Smith, J. W., McClure, R. P., et al. (2022). Increased adoption of best practices in ecological forecasting enables comparisons of forecastability. Ecological Applications, 32(2), e02500. https://doi.org/10.1002/eap.2500",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Best Practices in Ecological Forecasting</span>"
    ]
  },
  {
    "objectID": "best-practices.html#assignment",
    "href": "best-practices.html#assignment",
    "title": "6  Best Practices in Ecological Forecasting",
    "section": "6.2 Assignment",
    "text": "6.2 Assignment\nRead the two papers above and discuss how the forecasts developed as part of this book address the best practices and principles described in the papers.\n\n\n\n\nHarris, David J., Shawn D. Taylor, and Ethan P. White. 2018. “Forecasting Biodiversity in Breeding Birds Using Best Practices.” PeerJ 6. https://doi.org/10.7717/peerj.4278.\n\n\nHobday, Alistair J., Jason R. Hartog, John P. Manderson, Katherine E. Mills, Matthew J. Oliver, Andrew J. Pershing, and Samantha Siedlecki. 2019. “Ethical Considerations and Unanticipated Consequences Associated with Ecological Forecasting for Marine Resources.” ICES Journal of Marine Science. https://doi.org/10.1093/icesjms/fsy210.\n\n\nLewis, Abigail S. L., Whitney M. Woelmer, Heather L. Wander, Dexter W. Howard, John W. Smith, Ryan P. McClure, Mary E. Lofton, et al. 2022. “Increased Adoption of Best Practices in Ecological Forecasting Enables Comparisons of Forecastability.” Ecological Applications 32 (2): e02500. https://doi.org/10.1002/eap.2500.",
    "crumbs": [
      "Introduction to forecasting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Best Practices in Ecological Forecasting</span>"
    ]
  },
  {
    "objectID": "background-skills.html",
    "href": "background-skills.html",
    "title": "7  Background skills",
    "section": "",
    "text": "7.1 R/Rstudio Installation\nThe R for Data Science book provides instructions for installing R and Rstudio.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "background-skills.html#r-skills",
    "href": "background-skills.html#r-skills",
    "title": "7  Background skills",
    "section": "7.2 R skills",
    "text": "7.2 R skills\nThe book uses R as the focal programming language and uses the Tidyverse approach when working with and visualizing data. The following functions are commonly used: read_csv, write_csv, mutate, filter, group_by, summarize, ggplot, select, pipes (|&gt; or %&gt;), pivot_wider, pivot_longer, arrange, left_join). If you are new to R and Tidyverse there are many great materials on the internet. The Data Carpentry “Data Analysis and Visualization in R for Ecologists” is an excellent starting point for learning. The R for Data Science book is an especially useful reference for learning the Tidyverse commands. Finally, I have created an introductory to tidyverse module for my undergraduate Environmental Data Science class. You can use the module as a “test” of your Tidyverse skills",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "background-skills.html#sec-github",
    "href": "background-skills.html#sec-github",
    "title": "7  Background skills",
    "section": "7.3 Git Skills",
    "text": "7.3 Git Skills\nYou will be required to use Git and GitHub to complete the assignments in the book. In particular, Git and GitHub are uses for the generation and submission of forecasts to the NEON Ecological Forecasting Challenge. Below are instructions for setting up Git and GitHub on your computer.\n\n7.3.1 Setting up Git and GitHub\n\nCreate a GitHub user account at https://github.com, if you don’t already have one. Here is advice about choosing a user name, because choosing a good user name is critical.\nGo to Rstudio and install the usethis package.\n\ninstall.packages(\"usethis\")\n\nRun the following command where you replace the user.email and user.name with the email used for GitHub and your GitHub user name. You can learn more about the command here\n\nlibrary(usethis)\nuse_git_config(user.name = \"Jane Doe\", user.email = \"jane@example.org\")\nIf you get an error at this step it is likely due to your computer not having Git. Follow the instructions here about installing Git\n\nSet up your GitHub credentials on your computer. Follow the instructions here about using usethis::create_github_token() and gitcreds::gitcreds_set() functions. Also, save your GitHub PAT to a password manager so that you can find it in the future (in case you need to interact with GitHub from a different computer).\n\nIf you are having issues (i.e., your computer does not seem to have Git installed), here is an excellent resource to help you debug your git + Rstudio issues.\n\n\n7.3.2 Working with GitHub: a quarto example\nThis section provides instructions on working with Git and GitHub in Rstudio in the context of creating, modifying, and rendering a Quarto document.\n\nGo to https://github.com/frec3044/git-rmd-intro. Find the “fork” bottom near the top right. Click “Fork” and tell it to fork to your personal GitHub account.\nGo to the repo on your personal GitHub account. It will be something like https://github.com/[your-user-name]/git-rmd-intro\nUnder the green “Code” button, select the local tab, and copy the URL link.\nOpen Rstudio on your computer and create a new project. First, File -&gt; New Project -&gt; Version Control -&gt; Git. Paste the URL from you repo in the first box, hit tab to fill in the repo name in the second, and then use Browse to select where you want the project on your computer (I recommend having a directory on your computer where you keep all repositories we use in the class). If you don’t see a Version Control option then you may not have Git installed on your computer (use the instructions here to install Git)\nYour project will load. Then go to File -&gt; New -&gt; New File -&gt; Quarto Document\nIn the prompt use Title = “Assignment 1” and Author = [Your name]\nSave file as “assignment1.qmd” in the assignment subdirectory of the Project.\nCommit your assignment1.qmd file using the Git tab at the top right pane using a useful commit message. You will need to check the box for the files that you want to commit. A useful message helps you broadly remember what you did to the files that are included in the commit. The Git tab may not be in the top right panel if you have moved the panels around. If you don’t have the Git tab on pane, then you may not have created a project from GitHub correctly or you do not have Git installed on your computer.\nFind the Sources / Visual buttons right above the document. Select Source (which is the code view).\nCopy the code chunk on lines 21-24 and paste it at end of the document. Change to echo: TRUE.\nFind the following code at the top\n\nformat: html:\nand change it so that all the necessary files are saved in a single html file.\nformat:   \n  html:\n    embed-resources: true\n\nFind the Render (found above the document) button and click it to render the document to an html document. You will see a file named “assignment1.html” appear. The html is like a webpage version of your code. If you have a directory called assignment1_files then you did not do step 15 correctly.\nClick on the “assignment1.html” in your “Files” pane and select “View in Web Browser”. Confirm that it looks as expected.\nCommit the updated .qmd and new .html files to git.\nPush to your repository on GitHub.\nGo to https://github.com/[your-user-name]/git-rmd-intro You should also see your two most recent commits.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Background skills</span>"
    ]
  },
  {
    "objectID": "first-forecast.html",
    "href": "first-forecast.html",
    "title": "8  Introduction to NEON Ecological Forecasting Challenge",
    "section": "",
    "text": "8.1 Reading\nThomas, R. Q., Boettiger, C., Carey, C. C., Dietze, M. C., Johnson, L. R., Kenney, M. A., et al. (2023). The NEON Ecological Forecasting Challenge. Frontiers in Ecology and the Environment, 21(3), 112–113. https://doi.org/10.1002/fee.2616",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to NEON Ecological Forecasting Challenge</span>"
    ]
  },
  {
    "objectID": "first-forecast.html#assignment",
    "href": "first-forecast.html#assignment",
    "title": "8  Introduction to NEON Ecological Forecasting Challenge",
    "section": "8.2 Assignment",
    "text": "8.2 Assignment\nPre-assignment set up: Follow the instructions for setting up R and the code repository\n\nCreate a GitHub repository with the module\nWork through the Submit_forecast/NEON_forecast_challenge_workshop_aquatics.Rmd into TASKS sections\nModify the simple model in the Submit_forecast/forecast_code_template.Rmd in some way (see TASKS for ideas).\nAdd a brief description of what you changed to the beginning of the template.\nKnit the template Rmd to html.\nCommit template Rmd and html to GitHub",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to NEON Ecological Forecasting Challenge</span>"
    ]
  },
  {
    "objectID": "first-forecast.html#module-reference",
    "href": "first-forecast.html#module-reference",
    "title": "8  Introduction to NEON Ecological Forecasting Challenge",
    "section": "8.3 Module reference",
    "text": "8.3 Module reference\nOlsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas. Can you predict the future? A tutorial for the National Ecological Observatory Network Ecological Forecasting Challenge. In review at Journal of Open Source Education.\n\n\n\n\nThomas, R Quinn, Carl Boettiger, Cayelan C Carey, Michael C Dietze, Leah R Johnson, Melissa A Kenney, Jason S McLachlan, et al. 2023. “The NEON Ecological Forecasting Challenge.” Frontiers in Ecology and the Environment 21 (3): 112–13. https://doi.org/10.1002/fee.2616.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to NEON Ecological Forecasting Challenge</span>"
    ]
  },
  {
    "objectID": "second-forecast.html",
    "href": "second-forecast.html",
    "title": "9  Adding uncertainty to forecasts",
    "section": "",
    "text": "9.1 Reading\nThomas, R. Q., Boettiger, C., Carey, C. C., Dietze, M. C., Johnson, L. R., Kenney, M. A., et al. (2023). The NEON Ecological Forecasting Challenge. Frontiers in Ecology and the Environment, 21(3), 112–113. https://doi.org/10.1002/fee.2616",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adding uncertainty to forecasts</span>"
    ]
  },
  {
    "objectID": "second-forecast.html#assignment",
    "href": "second-forecast.html#assignment",
    "title": "9  Adding uncertainty to forecasts",
    "section": "9.2 Assignment",
    "text": "9.2 Assignment\nPre-assignment set up: Complete Chapter 8 and Chapter 3\n\nModify your template code to include the additional sources of uncertainty\nKnit the template Rmd to an HTML file.\nCommit template and the HTML file to GitHub",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adding uncertainty to forecasts</span>"
    ]
  },
  {
    "objectID": "second-forecast.html#module-reference",
    "href": "second-forecast.html#module-reference",
    "title": "9  Adding uncertainty to forecasts",
    "section": "9.3 Module reference",
    "text": "9.3 Module reference\nOlsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas. Can you predict the future? A tutorial for the National Ecological Observatory Network Ecological Forecasting Challenge. In review at Journal of Open Source Education.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Adding uncertainty to forecasts</span>"
    ]
  },
  {
    "objectID": "third-forecast.html",
    "href": "third-forecast.html",
    "title": "10  Automated, iterative forecasting",
    "section": "",
    "text": "10.1 Key concepts\nGitHub Actions is used to execute the forecast each day in the “cloud”. The “cloud” are small computers that are run somewhere in a Microsoft data center (Microsoft owns GitHub). You create a YAML file in the .github/workflows in your GitHub repo that defines the time of day that your job is run, the environment (type of operating system and Docker container), and the steps in the job.\nYou will use Docker container (eco4cast/neon4cast-rocker) to ensure a consistent computation environment each day. The Docker is based on a rocker container that already has R, Rstudio, and many common packages (e.g., tidyverse). It also contains packages that are specific to the NEON Forecasting Challenge.\nNothing persists in the container after it finishes in GitHub Actions. You will use the neon4cast::stage2 and neon4cast::stage3 to read the meteorology data. You will use read_csv and a weblink to read the targets. You will export your forecasts using the neon4cast::submit function that uploads your forecast to the NEON Ecological Forecasting Challenge S3 submission bucket.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automated, iterative forecasting</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#docker-skills",
    "href": "third-forecast.html#docker-skills",
    "title": "10  Automated, iterative forecasting",
    "section": "10.2 Docker Skills",
    "text": "10.2 Docker Skills\nDocker is a tool that the activities in the book use for improving the reproducibility and automation of data analysis and forecasting workflows. Below are the instructions for setting up and interacting with a Docker container (instructions are from Freya Olsson’s workshop)\nGo to https://docs.docker.com/get-docker/ to install the relevant install for your platform (available for PC, Mac, and Linux). Also see https://docs.docker.com/desktop/.\nNOTE: * If you’re running Windows, you will need WSL (Windows Subsystem for Linux) * If you’re running a Linux distribution, you may have to enable Virtualization on your computer (see here)",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automated, iterative forecasting</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#running-a-docker-container",
    "href": "third-forecast.html#running-a-docker-container",
    "title": "10  Automated, iterative forecasting",
    "section": "10.3 Running a docker container",
    "text": "10.3 Running a docker container\n\nLaunch Docker Desktop (either from the Command Line or by starting the GUI)\nAt the command line run the following command which tells docker to run the container with the name eco4cast/rocker-neon4cast that has all the packages and libraries installed already. The PASSWORD=yourpassword sets a simple password that you will use to open the container. The -ti option starts both a terminal and an interactive session.\n\ndocker run --rm -ti -e PASSWORD=yourpassword -p 8787:8787 eco4cast/rocker-neon4cast\nThis can take a few minutes to download and install. It will be quicker the next time you launch it.\n\nOpen up a web browser and navigate to http://localhost:8787/\nEnter the username: rstudio and password: yourpassword\nYou should see an R Studio interface with all the packages etc. pre-installed and ready to go.\n\nYou can close this localhost window (and then come back to it) but if you close the container from Docker (turn off your computer etc.) any changes will be lost unless you push them to Github or export to your local environment.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automated, iterative forecasting</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#reading",
    "href": "third-forecast.html#reading",
    "title": "10  Automated, iterative forecasting",
    "section": "10.4 Reading",
    "text": "10.4 Reading\nThomas, R. Q., Boettiger, C., Carey, C. C., Dietze, M. C., Johnson, L. R., Kenney, M. A., et al. (2023). The NEON Ecological Forecasting Challenge. Frontiers in Ecology and the Environment, 21(3), 112–113. https://doi.org/10.1002/fee.2616",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automated, iterative forecasting</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#assignment",
    "href": "third-forecast.html#assignment",
    "title": "10  Automated, iterative forecasting",
    "section": "10.5 Assignment",
    "text": "10.5 Assignment\nPre-assignment set up: Complete Chapter 9\n\nConvert your template markdown code to a .R script\nUpdate your repository to include the GitHub Action files\nCommit updated files to GitHub\nProvide the instructor with a link to the GitHub repository that demonstrates multiple successful automated executions of forecasting code.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automated, iterative forecasting</span>"
    ]
  },
  {
    "objectID": "third-forecast.html#module-reference",
    "href": "third-forecast.html#module-reference",
    "title": "10  Automated, iterative forecasting",
    "section": "10.6 Module reference",
    "text": "10.6 Module reference\nOlsson, F., C. Boettiger, C.C. Carey, M.E. Lofton, and R.Q. Thomas. Can you predict the future? A tutorial for the National Ecological Observatory Network Ecological Forecasting Challenge. In review at Journal of Open Source Education.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automated, iterative forecasting</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html",
    "href": "evaluating-forecasts.html",
    "title": "11  Evaluating forecasts",
    "section": "",
    "text": "11.1 Scoring metrics\nA scoring metric is a value that can be used to evaluate how well a forecast compares to observations. Scores can then be compared among forecast models to evaluate relative performance. Typically lower scores are better. Scoring metrics are also called “costs”. ## Accuracy\nThe most basic score would be the difference between the forecast mean and observation. However, there are a couple of issues with that score. First, forecasts below the observation will receive negative values and forecasts above will receive positive values, thus the lowest value is not the best. Second, it does not use any information from the forecast uncertainty in the evaluation. Metrics like absolute error (absolute value of the difference between the forecast mean and observation) and root-mean-squared error (RMSE) address the first issue but still do not consider the uncertainty.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#precision",
    "href": "evaluating-forecasts.html#precision",
    "title": "11  Evaluating forecasts",
    "section": "11.2 Precision",
    "text": "11.2 Precision\nIs the variance in the forecast larger than an acceptable value? For example, if the uncertainty in a rain forecast ranges from downpour to sunny then the variance is too large to be useful since one can not make a specific decision based on the forecast (e.g., do you cancel the soccer game ahead of time). One can also compare the variance in a forecast to a baseline variable from a simple model to see if the spread is less, and therefore a more informative forecast.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#accuracy-precision",
    "href": "evaluating-forecasts.html#accuracy-precision",
    "title": "11  Evaluating forecasts",
    "section": "11.3 Accuracy + Precision",
    "text": "11.3 Accuracy + Precision\nIt is important to consider uncertainty because intuitively a forecast that has the correct mean but a large uncertainty (so it puts very little confidence on the observed value) should have a lower score than a forecast with a mean that is slightly off but with lower uncertainty (so it puts more confidence on the observed value). The Continuous Ranked Probability Score (CRPS) is one metric that evaluates the forecast distribution. Another is the ignorance score (also called the Good or the log score). The CRPS is described below.\nAdd the concept of proper and local?\n\n11.3.1 Continuous Ranked Probability Score\nForecasts can scored using the continuous ranked probability score (CRPS), a proper scoring rule for evaluating forecasts presented as distributions or ensembles (Gneiting and Raftery (2007)). The CRPS compares the forecast probability distribution to that of the validation observation and assigns a score based on both the accuracy and precision of the forecast. We will use the ‘crps_sample’ function from the scoringRules package in R to calculate the CRPS for each forecast.\nCRPS is a calculation for each forecast-observation pair (e.g., a single datetime from a reference_datetime). CRPS can then be aggregated across all sites and forecast horizons to understand the aggregate performance.\nImportantly, we use the convention for CRPS where zero is the lowest and best possible score, therefore forecasts want to achieve the lowest score. CPRS can also be expressed as a negative number with zero as the highest and best possible score (Gneiting and Raftery (2007)). The scoringRules package we use follows the 0 or greater convention.\n\nlibrary(scoringRules)\nlibrary(tidyverse)\n\n\n\n11.3.2 Example of a CRPS calculation from an ensemble forecast\nThis section aims to provide an intuition for the CRPS metric.\nFirst, create a random sample from a probability distribution. This is the “forecast” for a particular point in time. For simplicity, we will use a normal distribution with a mean of 8 and a standard deviation of 1\n\nx &lt;- rnorm(1000, mean = 8, sd = 1.0)\n\nSecond, we have our data point (i.e., the target) that we set to 8 as well.\n\ny &lt;- 8\n\nNow calculate using the crps_sample() function in the scoringRules package\n\nlibrary(scoringRules)\ncrps_sample(y = y, dat = x)\n\n[1] 0.2512637\n\n\n\n\n11.3.3 Exploring the scoring surface\nNow lets see how the CRPS changes as the mean and standard deviation of the forecasted distribution change\nFirst, set vectors for the different mean and SD values we want to explore\n\nforecast_mean &lt;- seq(4, 12, 0.05)\nforecast_sd &lt;- seq(0.1, 10, 0.05)\n\nSecond, set our observed value to 8 for simplicity\n\ny &lt;- 8\n\nNow calculate the CRPS at each combination of forest mean and SD. We used crps_norm because we know the forecast is normally distributed\n\ncombined &lt;- array(NA, dim = c(length(forecast_mean), length(forecast_sd)))\nfor(i in 1:length(forecast_mean)){\n  for(j in 1:length(forecast_sd)){\n    combined[i, j] &lt;- crps_norm(y = y, mean = forecast_mean[i], forecast_sd[j])\n  }\n}\n\nFinally, visualize the scoring surface with the observed value represented by the red line\n\ncontour(x = forecast_mean, y = forecast_sd, z = as.matrix(combined),nlevels = 20, xlab = \"Forecast mean\", ylab = \"forecast SD\", main = \"CPRS score at different forecasted mean and sd\\nThe observation is at 8\")\nabline(v = y, col = \"red\")\nabline(v = 6, col = \"darkgreen\")\n\n\n\n\n\n\n\n\nThe contour surface highlights the trade-off between the mean and standard deviation. Each isocline is the same CRPS score. Holding the mean constant at the observed value (vertical red line at 8) but increases the uncertainty (increasing SD) results in a higher (worse) score. However, a mean of 6 (green line) with an SD of 2 has a similar score to a mean of 8 and a standard deviation of 1. The following figure capture this idea. The two lines are the distribution of the mean = 8, sd = 4 and mean = 6, sd = 2 forecasts. The red line is the observation of 8. The forecast distribution with the mean of 6 actually has more density at x = 8 (the observation) and scores slightly better.\n\nplot(density(rnorm(10000, mean = 8, sd = 4)), ylim = c(0, 0.2), main = \"\", xlab = \"forecasted value\")\nlines(density(rnorm(10000, mean = 6, sd = 2)), col = \"darkgreen\")\nabline(v = 8, col = \"red\")",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#dynamics",
    "href": "evaluating-forecasts.html#dynamics",
    "title": "11  Evaluating forecasts",
    "section": "11.4 Dynamics",
    "text": "11.4 Dynamics\nAdd discussion of shadow time and correlation.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#skill-scores",
    "href": "evaluating-forecasts.html#skill-scores",
    "title": "11  Evaluating forecasts",
    "section": "11.5 Skill scores",
    "text": "11.5 Skill scores\nIt is a best practice in forecasting to compare forecasts generated by more complex models to thus generated by simple “naive” models so that we can understand whether the additional complexity provides any gain in forecast capacity. Two common naive models are persistence and “climatology”.\nThe skill score compared a forecast model to a baseline model using the following equation\nskill_score &lt;- 1 - (mean_metric_baseline - mean_metric_forecast)/mean_metric_baseline\nThe mean_metric_baseline is the mean score for the baseline model. It can be RMSE, MAE, CPRS, etc. The mean_metric_forecast is the same metric for the forecast that is being evaluated.\nThe skill score ranges from -Inf - 1 where values from -Inf to 0 are forecast models that perform worse than the baseline. Values from 0 to 1 are forecast models that improve on the baseline. A value of 1 means that the forecast model is perfect.\n\n11.5.1 Example baseline models\nPersistence forecasts that tomorrow will be the same as today, thus capturing the inertia of the system. Uncertainty is added to a persistence forecast by simulating it as a random walk where each ensemble member has a random trajectory without any direction (so the mean of the forecast is still previous days mean value). Persistence models perform well in systems with real-time observations because you actually know the current value and in systems with high inertia (like the water temperatures in the bottom of a large lake).\n\npersistence &lt;- arrow::open_dataset(\"s3://anonymous@bio230014-bucket01/challenges/forecasts/bundled-summaries/project_id=neon4cast/duration=P1D/variable=temperature/model_id=persistenceRW?endpoint_override=sdsc.osn.xsede.org\") |&gt; \n  filter(site_id == \"BARC\",\n         reference_date == \"2024-01-10\") |&gt; \n  collect()\n\n\npersistence |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = quantile10, ymax = quantile90), color = \"lightblue\", fill = \"lightblue\", alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  labs(y = \"water temperature\", x = \"datetime\", title = \"NEON site: BARC\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n“Climatology” is the historical distribution of observations for the forecast days. In a seasonal system, we often represent this using the mean and standard deviation of historical data from the same day-of-year in the past. Less seasonal systems may represent it as the historical mean and standard deviation. I used climatology in quotes because the meteorological definition of climatology is 30 years but most ecological systems don’t have 30-years of observations. We use the term “day-of-year mean” instead of climatology when forecasting systems with limited historical data. Climatology forecasts the capacity of the long-term behavior of the system to capture seasonal patterns.\n\nclimatology &lt;- arrow::open_dataset(\"s3://anonymous@bio230014-bucket01/challenges/forecasts/bundled-summaries/project_id=neon4cast/duration=P1D/variable=temperature/model_id=climatology?endpoint_override=sdsc.osn.xsede.org\") |&gt; \n  filter(site_id == \"BARC\",\n         reference_date == \"2024-01-10\") |&gt; \n  collect()\n\n\nclimatology |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = quantile10, ymax = quantile90), color = \"lightblue\", fill = \"lightblue\", alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  labs(y = \"water temperature\", x = \"datetime\", title = \"NEON site: BARC\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nBoth of these models are simple to calculate and include no “knowledge” of the system that might be embedded in more complex models.\n\n\n11.5.2 Example skill score calculation\nThis example compares the skill of the climatology forecast to the persistence baseline\n\nbaseline_models &lt;- arrow::open_dataset(\"s3://anonymous@bio230014-bucket01/challenges/scores/bundled-parquet/project_id=neon4cast/duration=P1D/variable=temperature?endpoint_override=sdsc.osn.xsede.org\") |&gt; \n  filter(site_id == \"BARC\",\n         reference_datetime &gt; lubridate::as_datetime(\"2024-01-10\") & reference_datetime &lt; lubridate::as_datetime(\"2024-03-10\"),\n         model_id %in% c(\"climatology\", \"persistenceRW\")) |&gt; \n  collect() \n\nskill_score &lt;- baseline_models |&gt; \n  mutate(horizon = as.numeric(datetime - reference_datetime)) |&gt; \n  summarize(mean_crps = mean(crps, na.rm = TRUE), .by = c(\"model_id\", \"horizon\")) |&gt; \n  pivot_wider(names_from = model_id, values_from = mean_crps) |&gt; \n  mutate(skill_score = 1 - (climatology/persistenceRW))\n\n\nggplot(skill_score, aes(x = horizon, y = skill_score)) +\n  geom_line() +\n  geom_hline(aes(yintercept = 0)) +\n  labs(x = \"Horizon (days in future)\", y = \"Skill score\") +\n  theme_bw() +\n  annotate(\"text\", label = \"climatology is better\", x = 15, y = 0.2) +\n  annotate(\"text\", label = \"baseline (persistence) is better\", x = 15, y = -0.2)\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#take-homes",
    "href": "evaluating-forecasts.html#take-homes",
    "title": "11  Evaluating forecasts",
    "section": "11.6 Take homes",
    "text": "11.6 Take homes\nDifferent metrics evaluate different components of the forecast. Be sure to match your evaluation metric to evaluate the aspects of the forecast that match the needs of the forecast user.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#reading",
    "href": "evaluating-forecasts.html#reading",
    "title": "11  Evaluating forecasts",
    "section": "11.7 Reading",
    "text": "11.7 Reading\nSimonis, J. L., White, E. P., & Ernest, S. K. M. (2021). Evaluating probabilistic ecological forecasts. Ecology, 102(8). https://doi.org/10.1002/ecy.3431",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "evaluating-forecasts.html#problem-set",
    "href": "evaluating-forecasts.html#problem-set",
    "title": "11  Evaluating forecasts",
    "section": "11.8 Problem Set",
    "text": "11.8 Problem Set\nIf you have forecast submissions the scores from your submitted forecasts and the climatology and persistence forecasts. If you do not have submitted forecasts, use the model_id = XXXX to answer the questions\nCreate a new Rmarkdown document that you use to answer the questions below with a mix of code and text.\n\nPlot climatology, persistence, and your model for a single forecast day on the same plot\nBased on visual inspection of your plot, how do the medians model each forecast differ in capacity to represent the observations?\nBased on visual inspection of your plot, how does the uncertainty of each model forecast differ in capacity to represent the observations?\nCalculate the mean CRPS for the single-day forecast (averaged across all horizons). Which model has the lower score?\nCalculate the mean CRPS for climatology, persistence, and your model for all reference datetimes and horizons. Which model has the lower score when you consider all submissions?\nDo you think you have enough important from the mean CRPS score to determine the “best” model? If not, what is needed to better characterize the best-performing model?\n\nCommit your Rmd and knitted HTML to your GitHub repository that you used in Chapter 10.\n\n\n\n\nGneiting, Tilmann, and Adrian E Raftery. 2007. “Strictly Proper Scoring Rules, Prediction, and Estimation.” Journal of the American Statistical Association 102 (477): 359–78. https://doi.org/10.1198/016214506000001437.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Evaluating forecasts</span>"
    ]
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "12  Build your own forecast",
    "section": "",
    "text": "12.1 Helpful tips\nYou may want to use lags in your model. Lags in the target variables are used in autoregressive models. Lags in the inputs may improve your predictions because past weather may be a better predictor than current weather. Considerations for working with lags are below.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Build your own forecast</span>"
    ]
  },
  {
    "objectID": "project1.html#helpful-tips",
    "href": "project1.html#helpful-tips",
    "title": "12  Build your own forecast",
    "section": "",
    "text": "12.1.1 Lags in targets\nOne issue to consider when working with auto-regression (lagged) models is that data may not be available on the date you start your forecast (e.g., today). In this case, the forecast needs to start at the most recent observation and run to the present day. The model is continued on into the future over the full horizon. Only the future datetimes are submitted as a true forecast. Effectively, The output of the model run to the present day is the starting point for the true forecast into the future.\n\n\n12.1.2 Lags in inputs\nYou may want to use linear modeling (like lm) but add more covariates. If you want to explore the use of lag or running sums in the weather inputs, you will need to combine the historical and future weather data tomorrow (otherwise you won’t have a value for the lagged variable on the first day of the first). Here is some code to create a combined weather dataset.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\nnoaa_forecast_start &lt;- Sys.Date() - days(1)\nmin_historical_day &lt;- Sys.Date() - days(60)\nvariables &lt;- \"air_temperature\"\nsites &lt;- \"BARC\"\n\nnoaa_past_mean &lt;-  neon4cast::noaa_stage3() |&gt; \n  filter(site_id %in% sites,\n                variable %in% variables,\n                datetime &gt; min_historical_day,\n                datetime &lt; noaa_forecast_start) |&gt; \n  collect() |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  summarize(prediction = mean(prediction), .by = c(\"datetime\", \"site_id\", \"parameter\", \"variable\")) |&gt; \n  pivot_wider(names_from = variable, values_from = prediction) |&gt; \n  mutate(air_temperature = air_temperature - 273.15)\n\nnoaa_future_mean &lt;- neon4cast::noaa_stage2(start_date = noaa_forecast_start) |&gt; \n  filter(datetime &gt;= noaa_forecast_start,\n                site_id %in% sites,\n                variable %in% variables) |&gt; \n  collect() |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  summarize(prediction = mean(prediction), .by = c(\"datetime\", \"site_id\", \"parameter\", \"variable\")) |&gt;\n  pivot_wider(names_from = variable, values_from = prediction) |&gt;\n  mutate(air_temperature = air_temperature - 273.15) |&gt; \n  select(datetime, site_id, air_temperature, parameter)\n\ncombined_met_df &lt;- bind_rows(noaa_past_mean, noaa_future_mean)\n\n\nggplot(combined_met_df, aes(x = datetime, y = air_temperature, group = parameter)) +\n  geom_line() + \n  geom_vline(aes(xintercept = Sys.Date()), color = \"blue\")\n\n\n\n\n\n\n\n\nRemember that the lag needs to be calculated within an ensemble member.\n\n\n12.1.3 Reforecasting\nA good way to evaluate your forecast model is to generate forecasts for a starting date (e.g., reference_datetime) in the past. These “reforecasts” or “retroactive forecasts” use NOAA forecasts as inputs (or other forecasts) just like they are real-time forecasts so they mimic a genuine forecast of the future. Importantly, you should only use target data before the reforecast start date to train your forecast model. The key difference is that you can immediately compare the results to observations.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Build your own forecast</span>"
    ]
  },
  {
    "objectID": "project1.html#intro-to-tidymodels",
    "href": "project1.html#intro-to-tidymodels",
    "title": "12  Build your own forecast",
    "section": "12.2 Intro to Tidymodels",
    "text": "12.2 Intro to Tidymodels\nTidmodels is a meta-package that provides a standardized interface with many machine-learning algorithms. This brief tutorial that provides an example application with tidymodels builds on the machine learning module in the FREC 3044: Environmental Data Science course at Virginia Tech. https://github.com/frec-3044/machine-learning-template.\nTidymodels does not estimate uncertainty so forecast uncertainty needs to be generated output the tidymodels framework. For example, driver uncertainty can be generated by creating tidymodel forecasts for each meteorological ensemble member.\n\n12.2.1 Overview of tidymodels steps\nStep 1: Obtain data\nRead in data to memory. In the course, we have accessed CSV, excel, and database forms located in the GitHub repo, on websites, and through an API.\nStep 2: Pre-process data\nSplit data: divide data into the training and test sets.\n\nYou will use the training set to fit your model and the testing set to evaluate the model performance.\nWe only want to evaluate our model using data that the model has not used yet. Otherwise, we can only say that the model is good at fitting the data that was used to train it. This does not help us understand how it would perform as a tool to generate new predictions.\nYou want most of your data in the training set because machine learning models can be quite “hungry for data”.\nA typical split is 80% training and 20% testing but there isn’t a required split.\nData are randomly assigned to the two splits.\nYour data may have obvious groupings that you want to be equally represented in both the training and testing sets. For example, if you have cats and dogs as a variable you are using to predict tail length, you may want to be sure that your training set is not randomly full of dogs because that means it may not predict the cats well in the testing set. You can prevent this by defining the strata used when assigning the training and testing sets.\n\nRecipes: Modify predictors/features/covariates/independent variables for analysis\n\nModify the data set so that variables are correctly formatted for a particular model. For example, a linear regression requires groups to be dummy variables. Therefore, a column called “ecosystem” with two values: “forest” and “grass” would be converted to two columns: ecosystem_forest with a value of 0 or 1 and ecosystem_grass with a value of 0 and 1)\nRemoving highly correlated predictors\nRescaling predictor (e.g., converting to 0 mean and 1 standard deviation)\ntransforming predictors (e.g., log)\n\nStep 3: Specify model and workflow\nA workflow combines the model and recipe in a single “object” that you can use for training, testing, and predicting new data.\n\nDefine model type: linear regression, regression tree, neutral net, etc.\nDefine model engine: particular R package (lm, ranger, etc.)\nDefine model mode: regression or classification\nDefine workflow: combine recipe and model definition\n\nStep 4: Train model\n\nTune hyper-parameters: hyper-parameters are configuration settings that govern how a particular ML method is fit to data. They are called “hyper” because “regular” parameters are the parameters within the model that are learned by the ML method. For example, a method called “random forecast” requires a hyper-parameter that controls the minimum size of a regression tree that is allowed. This parameter (called min_n) could be directly provided by you or could be tuned. The tuning process involves repeatedly fitting the model using different values of the hyper-parameter and using the hyper-parameter values that yield the best fit to the data. Importantly: not all ML methods have hyper-parameters (e.g., linear regression using the lm engine does not have hyper-parameters). We don’t tune hyperparameters in this example. See example-with-tuning.Rmd for an extension of this application that includes hyperparameter tuning.\nFit model (using best hyper-parameter if they are tuned). The model is fit to the training data.\n\nStep 5: Predict\n\nPredict testing data using the model that was fit to the training data. This step is critical because we only want to evaluate our model using data that the model has not used yet. Otherwise, we can only say that the model is good at fitting the data that was used to train it. This does not help understand how it would perform as a tool to generate new predictions.\nPredictions are quite easy using the predict() function.\n\nStep 6: Evaluate model\n\nIt is important to use the appropriate metrics to evaluate how the model performs. Some metrics only apply to classification problems and others only apply to regression problems. A list of metric types can be found here\nWe will be focusing on root-mean-squared error (rmse), a metric that subtracts each observation from the predictions, squares it, averages all the squared errors for all the data points, and then takes the square root of the mean squared error.\n\nR-squared (rsq) is another metric that we used in the lake ice module.\n\nStep 7: Deploy model\n\nOne of the main points of using machine learning is to develop a tool that can be used with new predictors to predict data that wasn’t involved in the training and testing. These are data that have all the columns necessary to make predictions but lack data in the column you are trying to predict.\n\nThis step is simple because it involves using the predict() function with the same trained model but with new data.\n\n\n\n12.2.2 Application: Predicting water temperature at NEON sites\n\nlibrary(tidymodels)\ntidymodels_prefer()\nset.seed(100) #for random number generation\n\n\n12.2.2.1 Step 1: Obtain data\n\nlake_sites &lt;- c(\"BARC\", \"SUGG\")\n\n\ntargets &lt;- read_csv('https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz',\n                    show_col_types = FALSE) |&gt; \n  filter(site_id %in% lake_sites)\n\n\ntargets &lt;- targets |&gt; \n  filter(site_id %in% lake_sites,\n         variable == \"temperature\")\n\n\n# past stacked weather\ndf_past &lt;- neon4cast::noaa_stage3()\n\nvariables &lt;- c(\"air_temperature\")\n\nnoaa_past &lt;- df_past |&gt; \n  dplyr::filter(site_id %in% lake_sites,\n                datetime &gt;= ymd('2017-01-01'),\n                variable %in% variables) |&gt; \n  dplyr::collect()\n\n\n# aggregate the past to mean values\nnoaa_past_mean &lt;- noaa_past |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  group_by(datetime, site_id, variable) |&gt; \n  summarize(prediction = mean(prediction, na.rm = TRUE), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = variable, values_from = prediction) |&gt; \n  # convert air temp to C\n  mutate(air_temperature = air_temperature - 273.15)\n\n\nforecast_date &lt;- Sys.Date() \nnoaa_date &lt;- forecast_date - lubridate::days(2)\n\ndf_future &lt;- neon4cast::noaa_stage2(start_date = noaa_date)\n\nvariables &lt;- c(\"air_temperature\")\n\nnoaa_future &lt;- df_future |&gt; \n  dplyr::filter(reference_datetime == noaa_date,\n                datetime &gt;= forecast_date,\n                site_id %in% lake_sites,\n                variable %in% variables) |&gt; \n  dplyr::collect()\n\n\nnoaa_future_daily &lt;- noaa_future |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  # mean daily forecasts at each site per ensemble\n  summarize(prediction = mean(prediction), .by = c(\"datetime\", \"site_id\", \"parameter\", \"variable\")) |&gt;\n  pivot_wider(names_from = variable, values_from = prediction) |&gt;\n  # convert to Celsius\n  mutate(air_temperature = air_temperature - 273.15) |&gt; \n  select(datetime, site_id, air_temperature, parameter)\n\n\ntargets_df &lt;- targets |&gt; \n  filter(variable == 'temperature') |&gt;\n  pivot_wider(names_from = 'variable', values_from = 'observation') |&gt; \n  left_join(noaa_past_mean, \n            by = c(\"datetime\",\"site_id\")) |&gt; \n  mutate(doy = yday(datetime))\n\n\n\n12.2.2.2 Step 2: Pre-process data\nSplit data into training/testing sets\nWe are going to split the data into training and testing sets using the initial_split function. prop = 0.80 says to use 80% of the data in the training set.\n\nsplit &lt;- initial_split(targets_df, prop = 0.80, strata = site_id)\n\nOur split should reflect the 80/20 that we defined using prop\n\nsplit\n\n&lt;Training/Testing/Total&gt;\n&lt;3951/989/4940&gt;\n\n\nTo get the training and testing data we need to apply the training() and testing() functions to the split.\n\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\nYou can see that train_data is a data frame that we can work with.\n\ntrain_data\n\n# A tibble: 3,951 × 5\n   datetime   site_id temperature air_temperature   doy\n   &lt;date&gt;     &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1 2017-08-27 BARC           31.5              NA   239\n 2 2017-08-28 BARC           31.1              NA   240\n 3 2017-08-29 BARC           31.1              NA   241\n 4 2017-08-30 BARC           31.4              NA   242\n 5 2017-08-31 BARC           31.7              NA   243\n 6 2017-09-01 BARC           31.6              NA   244\n 7 2017-09-02 BARC           31.1              NA   245\n 8 2017-09-03 BARC           31.1              NA   246\n 9 2017-09-06 BARC           30.8              NA   249\n10 2017-09-07 BARC           30.5              NA   250\n# ℹ 3,941 more rows\n\n\nFeature engineering using a recipe\n\nrequires starting with dataset that is used to provide the columns.\na formula with the dependent variable and the predictors. If . is used as the predictor, that means using all columns other than the dependent variable.\nSteps that modify the data\n\nWe will use the following steps:\n\nstep_rm because we don’t want to use the datetime in the fit.\nstep_naomit because there are na values in the temperature and air_temperature columns. This is used here for illustrative purposes and can also be done by filtering the target data before it is split into training and testing groups\n\nHere are the different recipe steps used above\n\nFilter rows and select columns\n\nHere is our recipe:\n\nour_recipe &lt;- train_data |&gt; \n  recipe(temperature ~ . ) |&gt; \n  step_rm(datetime) |&gt;\n  step_naomit(air_temperature, temperature)\n\nThe recipe should show the steps that will be performed when applying the recipe. Importantly, these steps have not yet been applied, we just have a recipe of what to do.\n\nour_recipe\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\n\n\n\n\n\n── Operations \n\n\n• Variables removed: datetime\n\n\n• Removing rows with NA values in: air_temperature and temperature\n\n\nStep 3: Specify model, engine, and workflow\nWe need to create the model and engine that we will use. In this example, we are using linear_reg with the mode of regression (as opposed to classification). Setting the mode for a linear regression is not necessary because it only allows regressions but it is included here for completeness.\nThe engine is lm because we are using the standard R function lm. There are a ton of other functions for linear regression modeling that we could use. They would be specified as a different engine.\n\nour_model &lt;- linear_reg(mode = \"regression\") |&gt; \n  set_engine(\"lm\")\n\nYou will see the model, mode, and engine in the model object\n\nour_model \n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nWe now combine the model and the recipe to make a workflow that can be used to fit the training and testing data. workflow() initiates the workflow and add_model and add_recipe add those components to the workflow. Importantly, the workflow has not yet been applied, we just have a description of what to do.\n\nwflow &lt;-\n  workflow() |&gt; \n  add_model(our_model) |&gt; \n  add_recipe(our_recipe)\n\nYou can see that the workflow object has all the components together\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_rm()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nStep 4: Train model on Training Data\nWe will use the workflow object to train the model. We need to provide the workflow object and the dataset to the fit function to fit (i.e., train the model)\n\nfit &lt;- wflow |&gt; \n  fit(data = train_data)\n\nYou can see that the fit object is the workflow object + the results of the model fitting\n\nfit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_rm()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n    (Intercept)      site_idSUGG  air_temperature              doy  \n       4.758280        -0.689446         0.891196         0.003502  \n\n\nStep 5: Predict Test Data\nNow we will predict the testing data using the model that was fit to the training data.\n\npredictions &lt;- predict(fit, new_data = test_data)\n\nThe predictions are a single column called .pred\n\npredictions\n\n# A tibble: 989 × 1\n   .pred\n   &lt;dbl&gt;\n 1    NA\n 2    NA\n 3    NA\n 4    NA\n 5    NA\n 6    NA\n 7    NA\n 8    NA\n 9    NA\n10    NA\n# ℹ 979 more rows\n\n\nWe need to combine the .pred column with the testing data using the bind_cols function\n\npred_test &lt;- bind_cols(test_data, predictions)\n\nNow we have a data frame with the prediction and all the predictors used to predict it.\n\npred_test\n\n# A tibble: 989 × 6\n   datetime   site_id temperature air_temperature   doy .pred\n   &lt;date&gt;     &lt;chr&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2017-09-04 BARC           31.4              NA   247    NA\n 2 2017-09-05 BARC           31.0              NA   248    NA\n 3 2017-09-14 BARC           28.5              NA   257    NA\n 4 2017-09-17 BARC           28.8              NA   260    NA\n 5 2017-09-19 BARC           28.8              NA   262    NA\n 6 2017-09-22 BARC           29.0              NA   265    NA\n 7 2017-09-29 BARC           30.1              NA   272    NA\n 8 2017-10-04 BARC           27.6              NA   277    NA\n 9 2017-10-11 BARC           29.6              NA   284    NA\n10 2017-10-14 BARC           28.5              NA   287    NA\n# ℹ 979 more rows\n\n\nStep 6: Evaluate model\nWe will evaluate the performance of our predictions of the testing data using two metrics (rmse and rsq). The function metric_set defines the set of metrics we will be using them. It creates a function called multi_metric() that we will use to calculate the metrics. We pipe in the predicted test data (pred_test) and tell the function that our truth (i.e., observed data) is the temperature column and the predictions (i.e., estimate) is the .pred column\n\nmulti_metric &lt;- metric_set(rmse, rsq)\n\nmetric_table &lt;- pred_test |&gt; \n  multi_metric(truth = temperature, estimate = .pred)\n\nThe resulting table has the metrics for evaluation\n\nmetric_table\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       2.32 \n2 rsq     standard       0.827\n\n\nStep 7: Deploy model\nThe final step is to apply your model to predict new data.\nNow read in the new data.\n\ntargets_future &lt;- noaa_future_daily |&gt; \n  mutate(temperature = NA,\n         doy = yday(datetime)) |&gt; \n  filter(parameter == 1) |&gt; \n  select(-parameter)\n\nYou will notice that the temperature is all NA because you don’t know what the carbon stock is of the plot.\n\ntargets_future\n\n# A tibble: 68 × 5\n   datetime   site_id air_temperature temperature   doy\n   &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;lgl&gt;       &lt;dbl&gt;\n 1 2024-09-26 SUGG               25.6 NA            270\n 2 2024-09-27 SUGG               27.9 NA            271\n 3 2024-09-28 SUGG               26.3 NA            272\n 4 2024-09-29 SUGG               26.5 NA            273\n 5 2024-09-30 SUGG               26.0 NA            274\n 6 2024-10-01 SUGG               26.2 NA            275\n 7 2024-10-02 SUGG               27.1 NA            276\n 8 2024-10-03 SUGG               26.3 NA            277\n 9 2024-10-04 SUGG               24.8 NA            278\n10 2024-10-05 SUGG               25.7 NA            279\n# ℹ 58 more rows\n\n\nAs in “Step 5: Predict Test Data”, use the model fitting on the training data (fit) to predict the new data.\n\nnew_predictions &lt;- predict(fit, new_data = targets_future)\n\n\ntargets_future &lt;- noaa_future_daily |&gt; \n  mutate(temperature = NA,\n         doy = yday(datetime))\n\ntidymodels_forecast &lt;- data.frame()\n\nfor(i in unique(targets_future$parameter)){\n  curr_ens &lt;- targets_future |&gt; \n    filter(parameter == i) |&gt; \n    select(-parameter)\n  \n  new_predictions &lt;- predict(fit, new_data = curr_ens)\n  curr_ens &lt;- bind_cols(curr_ens, new_predictions) |&gt; \n    mutate(parameter = i)\n  tidymodels_forecast &lt;- bind_rows(tidymodels_forecast, curr_ens)\n}\n\n\ntidymodels_forecasts_EFI &lt;- tidymodels_forecast %&gt;%\n  rename(prediction = .pred) %&gt;%\n  mutate(variable = \"temperature\") |&gt; \n  # For the EFI challenge we only want the forecast for future\n  filter(datetime &gt; Sys.Date()) %&gt;%\n  group_by(site_id, variable) %&gt;%\n  mutate(reference_datetime = min(datetime) - lubridate::days(1),\n         family = \"ensemble\",\n         model_id = \"tidymodels_lm\") %&gt;%\n  select(model_id, datetime, reference_datetime, site_id, family, parameter, variable, prediction)\n\n\ntidymodels_forecasts_EFI |&gt;\n  filter(variable == \"temperature\") |&gt;\n  ggplot(aes(x = datetime, y = prediction, group = parameter)) +\n  geom_line() + \n  facet_wrap(~site_id)\n\n\n\n\n\n\n\n\n\n\n\n12.2.3 Tidymodel extras\n\n12.2.3.1 Changing the model approach\nYou can easily change the modeling approach by changing the model and engine. For example, the following code replaces the linear regression model with a random forest model from the ranger package. All other code used to fit and predict with the model stays the same.\n\nour_model &lt;- rand_forest(mode = \"regression\") |&gt; \n  set_engine(\"ranger\")\n\nA list of models that are available for you to use can be found here. Not all models are designed for all applications. For example, forecasting water temperature requires regression models rather than classification models.\n\n\n12.2.3.2 Tuning hyper-parameters\nSome modeling approaches have parameters that govern how the model fitting is done. These are called hyperparameters. Hyperparameters are different from the parameters that are optimized using data in the modeling fitting step. Tidymodels provides tools to also fit hyperparameters. An example can be found here.",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Build your own forecast</span>"
    ]
  },
  {
    "objectID": "project1.html#intro-to-fable-models",
    "href": "project1.html#intro-to-fable-models",
    "title": "12  Build your own forecast",
    "section": "12.3 Intro to Fable Models",
    "text": "12.3 Intro to Fable Models\nThe Fable modeling framework provides an interface for many common models used in forecasting. The useful thing about Fable is that it is specifically focused on forecasting so uncertainty estimation is a core feature. Fable also has an associated ebook called “Forecasting: Principles and Practice” that is an excellent resource for learning empirical methods in time-series forecasting. Similar to Tidymodels, once you have learned the general structure of how the package works, it opens up many different modeling approaches.\nThe example below forecasts using a simple random walk. Since the random walk starts at the last observation, it shows how to generate a forecast for variables that have a gap between the last observation and the current day.\n\nlibrary(tsibble)\nlibrary(fable)\n\n\n12.3.1 Step 1: Set up targets\n\ntargets &lt;- read_csv('https://data.ecoforecast.org/neon4cast-targets/aquatics/aquatics-targets.csv.gz',\n                    show_col_types = FALSE)\n\n\nmax_horizon &lt;- 35\nvar &lt;- \"temperature\"\nsite &lt;- \"BARC\"\n\nWe want to collect when to start our forecast. If we have observations from today then our forecast starts today. But if there is a gap between our last observation and today, our forecast needs to start on the date of the last observation and forecast until today. Then the forecast needs to extend into the future for our desired horizon. As a result, the horizon that we provide the fable model may be longer than the horizon that we want to forecast into the future. The code below calculates the start date of the forecast and the full horizon (the desired horizon in the future + the gap between the last observation and today)\n\nforecast_starts &lt;- targets |&gt; \n  dplyr::filter(!is.na(observation) & site_id == site & variable == var) |&gt; \n  # Start the day after the most recent non-NA value\n  dplyr::summarise(start_date = max(datetime) + lubridate::days(1)) |&gt;  # Date\n  dplyr::mutate(h = (Sys.Date() - start_date) + max_horizon,\n                h = as.numeric(h)) |&gt;  # Horizon value\n  dplyr::ungroup()\n\nforecast_starts\n\n# A tibble: 1 × 2\n  start_date     h\n  &lt;date&gt;     &lt;dbl&gt;\n1 2024-09-25    36\n\n\nThe targets have to be converted into a time-series tibble (called a tsibble). To make a tsibble you need to define the grouping variables (key) and the common that defines the time-series (index). Many modeling approaches require the datetime to be continuous (no gaps) but allow na values for values in the gaps. The fill_gaps() function fills the gaps with NAs.\n\ntargets_use &lt;- targets |&gt; \n  dplyr::filter(site_id == site,\n                variable == var) %&gt;%\n  tsibble::as_tsibble(key = c('variable', 'site_id'), index = 'datetime') |&gt; \n  tsibble::fill_gaps()   # add NA values up to today (index)\n\nOnce you have a tsibble, you combine it with a modeling approach to generate a model that is ready to be used. In this case, the model is a random walk (RW) with the target of observation.\n\nRW_model &lt;- targets_use |&gt; \n  fabletools::model(RW = fable::RW(observation))\n\nThe model is then used to generate a forecast for h number of time-steps in the future. The uncertainty is calculated using a bootstrap method (bootstrap= T) with 200 ensemble members, e.g. samples (times = 200).\n\nforecast &lt;- RW_model |&gt;  \n  fabletools::generate(h = forecast_starts$h, bootstrap = T, times = 200)\n\nThe forecast is converted into a standard format\n\nRW_forecasts_EFI &lt;- forecast %&gt;%\n  rename(parameter = .rep,\n         prediction = .sim) %&gt;%\n  # For the EFI challenge we only want the forecast for future\n  #filter(datetime &gt; Sys.Date()) %&gt;%\n  group_by(site_id, variable) %&gt;%\n  mutate(reference_datetime = Sys.Date(),\n         family = \"ensemble\",\n         model_id = \"persistenceRW\") %&gt;%\n  select(model_id, datetime, reference_datetime, site_id, family, parameter, variable, prediction)\n\nand plotted. The reference_datetime is shown in blue.\n\nRW_forecasts_EFI |&gt;\n  filter(variable == \"temperature\") |&gt;\n  ggplot(aes(x = datetime, y = prediction, group = parameter)) +\n  geom_line() + \n  geom_vline(aes(xintercept = reference_datetime), color = \"blue\") +\n  facet_wrap(~site_id)\n\n\n\n\n\n\n\n\n\n\n12.3.2 Fable extras\n\n12.3.2.1 Other models\nOther models can be added at the step that the targets are combined with the modeling approach. This example is a time-series linear model with a trend and seasonal term.\n\nRW_model &lt;- targets_use |&gt; \n  fabletools::model(lm = TSLM(observation ~ trend() + season()))\n\nA list of the other models are are: https://fable.tidyverts.org/reference/index.html",
    "crumbs": [
      "Iterative forecasting using empirical models",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Build your own forecast</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html",
    "href": "likelihood-methods.html",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "",
    "text": "13.1 Intro to probability and likelihood\nA random variable is a quantity that can take on values due to chance - it does not have a single value but instead can take on a range of values. The chance of each value is governed by a probability distribution\nFor example, X is a random variable. A particular value for X is y_i\ny_i &lt;- -0.2513233\nA random number is associated with a probability of it taking that value\n\\(P(X = y_i) = p_i\\)\n\\(p_i\\) is defined from a probability distribution (\\(P\\)). We may have an idea of the shape (i.e., normally distributed), but not the specific distribution (i.e. a normal with a mean = 0, and sd = 1). For example, Figure 13.1 is a normal distribution with a mean = 0 and sd = 1. (I created it by randomly drawing 10000 samples from a normal distribution using the rnorm function)\nplot(density(rnorm(10000, mean = 0, sd = 1)), xlab = \"y\", main = \"mean = 0, sd = 1\")\n\n\n\n\n\n\n\nFigure 13.1: Normal distribution with mean = 0 and sd = 1\nFor our random variable, if it is governed by the distribution above, a single sample (i.e., a measurement of X) will likely be close to 0 but sometimes it will be quite far away.\nImagine that we are measuring the same thing with the same sensor but the sensor has an error that is normally distributed. We think that the measurement should be zero (mean = 0) but with the sensor uncertainty (sd = 1).\nThe question is “What is the probability that X = y_i given a normal distribution with mean = 0 and sd = 1?”. In Figure 13.2, the red line is a hypothetical observation (y_i).\nplot(density(rnorm(10000, mean = 0, sd = 1)), xlab = \"y\", main = \"mean = 0, sd = 1\")\nabline(v = y_i, col = \"red\")\n\n\n\n\n\n\n\nFigure 13.2: Normal distribution with mean = 0 and sd = 1. The red line is an ‘observation’\nWe could also ask “What is the probability that X = y_i given a normal distribution with mean = 2 and sd = 1?” In Figure 13.3 you can see that the observation is at a lower probability in the distribution.\nplot(density(rnorm(10000, mean = 2, sd = 1)), xlab = \"y\", main = \"mean = 2, sd = 1\")\nabline(v = y_i, col = \"red\")\n\n\n\n\n\n\n\nFigure 13.3: Normal distribution with mean = 2 and sd = 1. The red line is an ‘observation’\nWe can numerically compare the answer to the two questions using the “Probability Density Function”. In R the PDF is represented by the dnorm function (d representing “density”). The dnorm function allows us to answer the question: Which mean is more likely given the data point?\nmean of 0\n\n\n[1] 0.3865399\n\n\nmean of 2\n\n\n[1] 0.03164526\nNow, we can collect more data that represents more samples from the random variable with an unknown mean. Here are two data points.\ny &lt;- c(-0.2513233, -2.662035)\nWhat is the probability of both events occurring given the mean = 0? They are independent so we can multiply them.\nIMPORTANT: If two random events are independent, then the probability that they both occur is the multiplication of the two individual probabilities. For example, the probability of getting two heads in two coin flips is 0.5 * 0.5 = 0.25. If the events are not independent then we can’t just multiply (we will get to this later).\nThe two calls to the dnorm function are the evaluation of the two data points.\ndnorm(x = y[1], mean = 0, sd = 1, log = FALSE) * dnorm(x = y[2], mean = 0, sd = 1, log = FALSE)\n\n[1] 0.004459699\nWe can repeat the calculation but with a different value for the mean to ask the question: What about with a mean of 2?\ndnorm(x = y[1], mean = 2, sd = 1, log = FALSE) * dnorm(x = y[2], mean = 2, sd = 1, log = FALSE)\n\n[1] 2.40778e-07\nUsing the values above, which mean (mean = 0 or mean = 2) is more likely given the two data points?\nIMPORTANT: Since multiplying a lot of small numbers can result in very small numbers, we add the log of the densities. This is shown below by using log = TRUE.\nmessage(\"log likelihood: mean of 0\")\n\nlog likelihood: mean of 0\n\ndnorm(x = y[1], mean = 0, sd = 1, log = TRUE) + dnorm(x = y[2], mean = 0, sd = 1, log = TRUE)\n\n[1] -5.412674\n\nmessage(\"log likelihood: mean of 2\")\n\nlog likelihood: mean of 2\n\ndnorm(x = y[1], mean = 2, sd = 1, log = TRUE) + dnorm(x = y[2], mean = 2, sd = 1, log = TRUE)\n\n[1] -15.23939\nThis logically extends to three data points or more. We will get two log-likelihood values because we are examining two “models” with three data points. In the code below, what are the two “models”?\ny &lt;- c(-0.2513233, -2.662035, 4.060981)\n\nLL1 &lt;- dnorm(x = y[1], mean = 0, sd = 1, log = TRUE) + \n       dnorm(x = y[2], mean = 0, sd = 1, log = TRUE) + \n       dnorm(x = y[3], mean = 0, sd = 1, log = TRUE)\n\nLL2 &lt;- dnorm(x = y[1], mean = 2, sd = 1, log = TRUE) + \n       dnorm(x = y[2], mean = 2, sd = 1, log = TRUE) + \n       dnorm(x = y[3], mean = 2, sd = 1, log = TRUE)\n\nmessage(\"log likelihood: mean = 0\")\n\nlog likelihood: mean = 0\n\nLL1\n\n[1] -14.5774\n\nmessage(\"log likelihood: mean = 2\")\n\nlog likelihood: mean = 2\n\nLL2\n\n[1] -18.28215\nThe separate calls to dnorm can be combined into one call by using the vectorization capacities of R. If a vector of observation is passed to the function, it will return a vector that was evaluated as each value of the vector. The vector can be used in a call to the sum function to get a single value.\nmessage(\"log likelihood: mean = 0\")\n\nlog likelihood: mean = 0\n\nsum(dnorm(x = y, mean = 0, sd = 1, log = TRUE))\n\n[1] -14.5774\n\nmessage(\"log likelihood: mean = 2\")\n\nlog likelihood: mean = 2\n\nsum(dnorm(x = y, mean = 2, sd = 1, log = TRUE))\n\n[1] -18.28215\nThe data (i.e., the three values in y) are more likely for a PDF with a mean = 0 than a mean = 2. Clearly, we can compare two different guesses for the mean (0 vs. 2) but how do we find the most likely value for the mean?\nFirst, let’s create a vector of different mean values we want to test\n#Values for mean that we are going to explore\nmean_values &lt;- seq(from = -4, to = 7, by = 0.1)\nmean_values\n\n  [1] -4.0 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6\n [16] -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1\n [31] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4\n [46]  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9\n [61]  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4\n [76]  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9\n [91]  5.0  5.1  5.2  5.3  5.4  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4\n[106]  6.5  6.6  6.7  6.8  6.9  7.0\nNow we will calculate the log-likelihood of the data for each potential value of the mean. The code below loops over the different values of the mean that we want to test.\nLL &lt;- rep(NA, length(mean_values))\n\nfor(i in 1:length(mean_values)){\n  LL[i] &lt;- sum(dnorm(y, mean = mean_values[i], sd = 1, log = TRUE))\n}\nLL\n\n  [1] -43.16789 -41.86812 -40.59836 -39.35860 -38.14884 -36.96908 -35.81931\n  [8] -34.69955 -33.60979 -32.55003 -31.52026 -30.52050 -29.55074 -28.61098\n [15] -27.70121 -26.82145 -25.97169 -25.15193 -24.36217 -23.60240 -22.87264\n [22] -22.17288 -21.50312 -20.86335 -20.25359 -19.67383 -19.12407 -18.60431\n [29] -18.11454 -17.65478 -17.22502 -16.82526 -16.45549 -16.11573 -15.80597\n [36] -15.52621 -15.27644 -15.05668 -14.86692 -14.70716 -14.57740 -14.47763\n [43] -14.40787 -14.36811 -14.35835 -14.37858 -14.42882 -14.50906 -14.61930\n [50] -14.75954 -14.92977 -15.13001 -15.36025 -15.62049 -15.91072 -16.23096\n [57] -16.58120 -16.96144 -17.37167 -17.81191 -18.28215 -18.78239 -19.31263\n [64] -19.87286 -20.46310 -21.08334 -21.73358 -22.41381 -23.12405 -23.86429\n [71] -24.63453 -25.43477 -26.26500 -27.12524 -28.01548 -28.93572 -29.88595\n [78] -30.86619 -31.87643 -32.91667 -33.98691 -35.08714 -36.21738 -37.37762\n [85] -38.56786 -39.78809 -41.03833 -42.31857 -43.62881 -44.96904 -46.33928\n [92] -47.73952 -49.16976 -50.63000 -52.12023 -53.64047 -55.19071 -56.77095\n [99] -58.38118 -60.02142 -61.69166 -63.39190 -65.12214 -66.88237 -68.67261\n[106] -70.49285 -72.34309 -74.22332 -76.13356 -78.07380 -80.04404\nThe relationship between the different mean values and the likelihood is a curve (Figure 13.4)\nIMPORTANT: This curve may look like a probability density function (PDF) but it is not! Why? Because the area under the curve does not sum to 1. As a result 1) we use the term likelihood rather than density and 2) while the curve gives us the most likely estimate for the parameter, it does not give us the PDF of the parameter (even if we want the PDF, Bayesian statistics are needed for that)\nplot(x = mean_values, y = LL, type = \"o\", xlab = \"mean\", ylab = \"log(likelihood)\")\n\n\n\n\n\n\n\nFigure 13.4: The likelihood curve over different values of the mean\nThe most likely value is the highest value on the curve. Which is:\nindex_of_max &lt;- which.max(LL)\nmean_values[index_of_max]\n\n[1] 0.4\nthat compares well to directly calculating the mean\nmean(y)\n\n[1] 0.3825409\nWe just calculated the likelihood of the data GIVEN our model where our model was that “the data are normally distributed with a specific mean”. We found the most likely value for a parameter in the model (the parameter being the mean of the PDF).\nFor use in R where we want to find the minimum of a function, you can find the minimum of the negative log-likelihood curve (Figure 13.5)\nplot(x = mean_values, y = -LL, type = \"o\", xlab = \"mean\", ylab = \"-1 * log(likelihood)\") #Notice negative sign\nabline(v = mean(y), col = \"red\")\n\n\n\n\n\n\n\nFigure 13.5: The likelihood curve over different values of the mean with minimum shown\nTo use the optimizer function in R, we first need to create a function that defines our likelihood (this is the same as above with the mean being `par[1]`). The likelihood is also called the cost function where worse values for the parameter have higher cost scores.\nLL_fn &lt;- function(y, par){\n  #NEED NEGATIVE BECAUSE OPTIM MINIMIZES\n  -sum(dnorm(y, mean = par[1], sd = 1, log = TRUE))\n}\nThis likelihood function is then used in an optimization function that finds the minimum (don’t worry about the details).\n#@par are the starting values for the search\n#@fn is the function that is minimized (find parameters that yield the lowest -log(LL)) \n#@ anything else, like y, are arguments needed by the LL_fn\nfit &lt;- optim(par = 4, fn = LL_fn, method = \"BFGS\", y = y)\nfit\n\n$par\n[1] 0.3825409\n\n$value\n[1] 14.35789\n\n$counts\nfunction gradient \n       4        3 \n\n$convergence\n[1] 0\n\n$message\nNULL\nThe optimal value from fit is similar to our manual calculation and equal to the actual mean.\nmessage(\"From optim\")\n\nFrom optim\n\nfit$par\n\n[1] 0.3825409\n\nmessage(\"From manual calcuation\")\n\nFrom manual calcuation\n\nmean_values[which.max(LL)]\n\n[1] 0.4\n\nmessage(\"From the mean function\")\n\nFrom the mean function\n\nmean(y)\n\n[1] 0.3825409\nSweet, it works! I know you are thinking “Wow a complicated way to calculate a mean…great thanks!”. But the power is that we can replace the direct estimation of the mean with a “process” model that predicts the mean.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html#applying-likelihood-to-linear-regression",
    "href": "likelihood-methods.html#applying-likelihood-to-linear-regression",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "13.2 Applying likelihood to linear regression",
    "text": "13.2 Applying likelihood to linear regression\nHere is a simple model that predicts y using a regression y = par[1] + par[2] * x\nFirst we are going to create some simulated data from the regression and add normally distributed noise.\n\nset.seed(100)\npar_true &lt;- c(3, 1.2) #Slope and intercept\nsd_true &lt;- 0.9 #noise term\nx &lt;- runif(100, 3, 10)\ny_true &lt;- par_true[1] + par_true[2] * x \ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_true) #This is the \"fake data\"\n\nThe fake data are shown in Figure 13.6.\n\nplot(x, y)\n\n\n\n\n\n\n\nFigure 13.6: The “fake” data used in the following analysis\n\n\n\n\n\nNow we need a function that does the regression calculation from the parameters and inputs (x)\n\npred_linear &lt;- function(x, par){\n  y &lt;- par[1] + par[2] * x\n}\n\nNow our likelihood function uses the result of pred_linear as the mean of the PDF. This asks how likely each data point is given the parameters and inputs (x). Unlike our example above where the mean is constant, the mean is allowed to be different for each data point because the value of the input (x) is different for each point.\n\nLL_fn &lt;- function(par, x, y){\n  #NEED NEGATIVE BECAUSE OPTIM MINIMIZES\n  -sum(dnorm(y, mean = pred_linear(x, par), sd = 1, log = TRUE))\n}\n\nAs an example, for a value of par[1] = 0.2, and par[2] = 1.4 we can calculate the likelihood for the first two data points with the following\n\ndnorm(y[1], mean = pred_linear(x[1], c(0.2, 1.4)), sd = 1, log = TRUE) + dnorm(y[2], mean = pred_linear(x[2], c(0.2, 1.4)), sd = 1, log = TRUE) # + ...\n\n[1] -2.80959\n\n\nNow use optim to solve\n\npred_linear &lt;- function(x, par){\n  y &lt;- par[1] + par[2] * x\n}\nLL_fn &lt;- function(par, x, y){\n  #NEED NEGATIVE BECAUSE OPTIM MINIMIZES\n  -sum(dnorm(y, mean = pred_linear(x, par), sd = 1, log = TRUE))\n}\nfit &lt;- optim(par = c(0.2, 1.5), fn = LL_fn, method = \"BFGS\", x = x, y = y)\n\nHow do the estimated parameters compare to the true values?\n\nmessage(\"From optim\")\n\nFrom optim\n\nfit$par\n\n[1] 2.976254 1.191683\n\nmessage(\"True values (those we used to generate the data)\")\n\nTrue values (those we used to generate the data)\n\npar_true\n\n[1] 3.0 1.2",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html#key-steps-in-likelihood-analysis",
    "href": "likelihood-methods.html#key-steps-in-likelihood-analysis",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "13.3 Key steps in Likelihood analysis",
    "text": "13.3 Key steps in Likelihood analysis\nA key takehome is that defining the likelihood function requires two steps\n\nWrite down your process model. This is a deterministic model (meaning that it doesn’t have any random component)\n\n\n#Process model\npred_linear &lt;- function(x, par){\n  y &lt;- par[1] + par[2] * x\n}\n\n\nWrite down your probability model. You can think of this as your data model because it represents how you think the underlying process is converted into data through the random processes of data collection, sampling, sensor uncertainty, etc. In R this will use a “d” function: dnorm, dexp, dlnorm, dgamma, etc.\n\n\n#Probability model\nLL_fn &lt;- function(par, x, y){\n  -sum(dnorm(y, mean = pred_linear(x, par), sd = 1, log = TRUE))\n}",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html#fitting-parameters-in-the-probability-model",
    "href": "likelihood-methods.html#fitting-parameters-in-the-probability-model",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "13.4 Fitting parameters in the probability model",
    "text": "13.4 Fitting parameters in the probability model\nThere is one more assumption in our likelihood calculation that we have not addressed: we are assuming the standard deviation in the data is known (sd = 1). In reality, we don’t know what the sd is. This assumption is simple to fix. All we need to do is make the sd a parameter that we estimate.\nNow there is a par[3] in in the likelihood function (but it isn’t in the process model function)\n\n#Probability model\nLL_fn &lt;- function(par, x, y){\n  -sum(dnorm(y, mean = pred_linear(x, par), sd = par[3], log = TRUE))\n}\n\nFitting the sd is not necessarily clear. Think about it this way: if the sd is too low, there will be a very low likelihood for the data near the mean. In this case, you can increase the likelihood just by increasing the sd parameter (not changing other parameters).\nHow we can estimate all three parameters\n\nfit &lt;- optim(par = c(0.2, 1.5, 2), fn = LL_fn, method = \"BFGS\", x = x, y = y)\n\nHow well do they compare to the true values used to generate the simulated data?\n\nmessage(\"From optim\")\n\nFrom optim\n\nfit$par\n\n[1] 2.9762510 1.1916835 0.9022436\n\nmessage(\"True values (those we used to generate the data)\")\n\nTrue values (those we used to generate the data)\n\nc(par_true, sd_true)\n\n[1] 3.0 1.2 0.9\n\n\nSweet it works! I know you are thinking “Wow a complicated way to calculate a linear regression…thanks…I can just use the lm() function in R to do this!”.\n\nfit2 &lt;- lm(y ~ x)\nsummary(fit2)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.96876 -0.57147 -0.03937  0.48181  2.39566 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.97625    0.34386   8.655 9.95e-14 ***\nx            1.19168    0.04994  23.862  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9114 on 98 degrees of freedom\nMultiple R-squared:  0.8532,    Adjusted R-squared:  0.8517 \nF-statistic: 569.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nNotice how the “Residual standard error” is similar to our sd?\nHowever, the power is that we are not restricted to the assumptions of a linear regression model. We can use more complicated and non-linear “process” models (not restricted to y = mx + b) and we can use more complicated “data” models than the normal distribution with constant variance (i.e., a single value for sd that is used for all data point)\nFor example, we can simulate new data that assumes the standard deviation increases as the prediction increases (larger predictions have more variation). This might be due to sensor error increasing with the size of the measurement. In this case, we multiply y times a scalar (sd_scalar_true)\n\nset.seed(100)\nx &lt;- runif(100, 3, 10)\npar_true &lt;- c(3, 1.2)\nsd_scalar_true &lt;- 0.1\ny_true &lt;- par_true[1] + par_true[2] * x\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_scalar_true * y) #This is the \"fake data\"\n\nThe fake data are shown in Figure 13.7.\n\nplot(x, y)\n\n\n\n\n\n\n\nFigure 13.7: The “fake” data used in the following analysis\n\n\n\n\n\nWe use the same “process model” because we have not changed how we think the underlying process works\n\n#Process model\npred_linear &lt;- function(x, par){\n  y &lt;- par[1] + par[2] * x\n}\n\nHowever, we do have to change our “data” or “probability” model. See how sd in the dnorm is now sd = par[3] * pred_linear(x, par) so that the sd increases with the size of the prediction\n\n#Probability model\nLL_fn &lt;- function(par, x, y){\n  -sum(dnorm(y, mean = pred_linear(x, par), sd = par[3] * pred_linear(x, par), log = TRUE))\n}\n\nWe can now fit it\n\nfit &lt;- optim(par = c(0.2, 1.5, 1), fn = LL_fn, method = \"BFGS\", x = x, y = y)\nfit$par\n\n[1] 3.2107510 1.1663474 0.1013275\n\n\nAs a result, we now better represent how data is generated from the underlying process. This will often change and improve parameter estimates for the process model.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html#applying-likelihood-to-mechanistic-equations",
    "href": "likelihood-methods.html#applying-likelihood-to-mechanistic-equations",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "13.5 Applying likelihood to mechanistic equations",
    "text": "13.5 Applying likelihood to mechanistic equations\nAs another example, you can easily change the process model to be non-linear. For example, we will generate simulated data from a saturating function (a Michaelis-Menten function)\n\nset.seed(100)\nx &lt;- runif(100, 0.1, 10)\npar_true &lt;- c(2, 0.5)\nsd_true &lt;- 0.1\ny &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y), mean = y, sd = sd_true)\n\nThe fake data are shown in Figure 13.8.\n\nplot(x, y)\n\n\n\n\n\n\n\nFigure 13.8: The “fake” data used in the following analysis\n\n\n\n\n\nWe certainly don’t want to fit a linear regression to these data (though we could but it would not fit well). Fortunately, fitting it using likelihood is as simple as changing the process model. Here is our new process model.\n\n#Process model\npred_mm &lt;- function(x, par){\n  y &lt;- par[1] * (x / (par[2] + x))\n}\n\nThe “data” or “probability” model stays the same\n\n#Probability model\nLL_fn &lt;- function(par, x, y){\n  -sum(dnorm(y, mean = pred_mm(x, par), sd = par[3], log = TRUE))\n}\n\nAnd we fit it\n\nfit &lt;- optim(par = c(0.2, 1.5, 1), fn = LL_fn, method = \"BFGS\", x = x, y = y)\n\nAnd it fits well\n\nfit$par\n\n[1] 1.97737266 0.47210121 0.09996482\n\nc(par_true, sd_true)\n\n[1] 2.0 0.5 0.1\n\n\nFigure 13.9 shows the curve from the optimized parameters with the observations\n\nplot(x, y)\ncurve(pred_mm(x, par = fit$par), from = 0.1, to = 10, add = TRUE)\n\n\n\n\n\n\n\nFigure 13.9: the curve from the optimized parameters with the observations\n\n\n\n\n\nFinally, we don’t have to assume that the data model is a normal distribution. Here is an example assuming that the data model follows a gamma distribution (Figure 13.10). Learn more about the gamma here\n\nLL_fn &lt;- function(par, x, y){\n    -sum(dgamma(y, shape = (pred_mm(x, par)^2/par[3]), rate =  pred_mm(x, par)/par[3], log = TRUE))\n}\n\nfit &lt;- optim(par = c(0.2, 1.5, 1), fn = LL_fn, x = x, y = y)\n\n\n\n\n\n\n\n\n\nFigure 13.10: the curve from the optimized parameters with the observations for a gamma distribution data model.\n\n\n\n\n\n\n13.5.1 Take home messages\n\nThink about what process produces your data. Consider deterministic mechanisms and random processes.\n\nWrite your process model as a function of inputs and parameters. Your process model should be deterministic (not have any random component). The process model can be a simple empirical model or a process model like Chapter 16.\nWrite down your data model. What random processes converts the output of your process model to data? This could include variance in observations, variance in the process models (it doesn’t represent the real world perfectly even if we could observe it perfectly), and variation among sample units (like individuals). In a likelihood analysis, all of these forms are typically combined together.\nEmbed your process model as the mean for your data model and solve with an optimizing algorithm. In the normal distribution, the mean is one of the parameters of the PDF so you can use the process model prediction directory in the data model as the mean. In other distributions the parameters of the PDF are not the mean, instead the mean is a function of the parameters. Therefore you will need to convert the mean to the parameters of the distribution. This is called “moment matching.”\n\n\n\n13.5.2 Brief introduction to other components of a likelihood analysis\n\nEstimating uncertainty in the parameters using the shape of the likelihood curve (you can use the shape of the likelihood curve to estimate confidence (steeper = more confidence). In the Figure 13.11, the blue line has more data in the calculation of the mean than the black line. The dashed lines are the 2-unit likelihood level (2 log(LL) units were subtracted from the highest likelihood to determine the dashed line). The values of the dashed lines where they cross the likelihood curve with the same color are approximately the 95% confidence intervals for the parameter.\n\n\n#Create a dataset\ny_more_data &lt;- rnorm(20, mean = 0, sd = 1)\n#Select only 5 of the values from the larger dataset\ny_small_data &lt;- y_more_data[1:5]\n\n#Calculate the likelihood surface for each of the datasets\nmean_values &lt;- seq(from = -4, to = 7, by = 0.1)\n\nLL_more &lt;- rep(NA, length(mean_values))\nfor(i in 1:length(mean_values)){\n  LL_more[i] &lt;- sum(dnorm(y_more_data, mean = mean_values[i], sd = 1, log = TRUE))\n}\n\nLL_small &lt;- rep(NA, length(mean_values))\nfor(i in 1:length(mean_values)){\n  LL_small[i] &lt;- sum(dnorm(y_small_data, mean = mean_values[i], sd = 1, log = TRUE))\n}\n\n#Find the highest likelihood\nmle_LL_small &lt;- LL_small[which.max(LL_small)]\nmle_LL_more &lt;- LL_more[which.max(LL_more)]\n\n\n\n\n\n\n\n\n\nFigure 13.11: Likelihood curve with the two-unit support intervals shown\n\n\n\n\n\n\nComparing different models to select for the best model (uses the likelihood of the different models + a penalty for having more parameters). AIC is a common way to compare models (https://en.wikipedia.org/wiki/Akaike_information_criterion). It is 2 * number of parameters - 2 * ln (likelihood). Increasing the likelihood by adding more parameters won’t necessarily result in the lowest AIC (lower is better) because it includes a penalty for having more parameters.\nThere are more complete packages for doing likelihood analysis that optimize parameters, calculate AIC, estimate uncertainty, etc. For example, the likelihood package and mle function in R do these calculations for you. Here is an example using the mle package (note that the mle package assumes that you have created the objects x and y and only have parameters in the function calls…see how pred_mm and LL_fn don’t have x and y as arguments)\n\n\npred_mm &lt;- function(par1, par2){\n  par1 * (x / (par2 + x))\n}\nLL_fn &lt;- function(par1, par2, par3){\n  mu &lt;- pred_mm(par1, par2)\n  -sum(dnorm(y, mean = mu, sd = par3, log = TRUE))\n}\nlibrary(stats4)\nfit &lt;- mle(minuslogl = LL_fn, start = list(par1 = 0.2, par2 = 1.5, par3 = 1), method = \"BFGS\", nobs = length(y)) \nAIC(fit)\n\n[1] -170.8059\n\nsummary(fit)\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LL_fn, start = list(par1 = 0.2, par2 = 1.5, par3 = 1), \n    method = \"BFGS\", nobs = length(y))\n\nCoefficients:\n       Estimate  Std. Error\npar1 1.97737266 0.021219232\npar2 0.47210121 0.043616015\npar3 0.09996482 0.007065724\n\n-2 log L: -176.8059 \n\nlogLik(fit)\n\n'log Lik.' 88.40293 (df=3)\n\nconfint(fit)\n\nProfiling...\n\n\n          2.5 %    97.5 %\npar1 1.93648687 2.0205486\npar2 0.39017722 0.5630485\npar3 0.08755983 0.1155966\n\n\n\n\n13.5.3 Key point\nThe likelihood approach shown here gives us the likelihood of the particular set of data GIVEN the model. It does not give us the probability of the model given the particular set of data (i.e., the probability distributions of the model parameters). Ultimately, we want the probability distributions of the model parameters so that we can turn them around to forecast the probability of yet-to-be-collected data. For this, we need to use Bayesian statistics in Chapter 14!",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html#reading",
    "href": "likelihood-methods.html#reading",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "13.6 Reading",
    "text": "13.6 Reading",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "likelihood-methods.html#problem-set",
    "href": "likelihood-methods.html#problem-set",
    "title": "13  Parameter calibration using likelihood methods",
    "section": "13.7 Problem set",
    "text": "13.7 Problem set\nThe problem set is located in likelihood_problem_set.qmd in https://github.com/frec-5174/book-problem-sets. You can fork the repository or copy the code into your own quarto document",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Parameter calibration using likelihood methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html",
    "href": "bayesian-methods-intro.html",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "",
    "text": "14.1 Introduction to Bayesian statistics",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#introduction-to-bayesian-statistics",
    "href": "bayesian-methods-intro.html#introduction-to-bayesian-statistics",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "",
    "text": "14.1.1 Starting with likelihood\nHere we start where we left off with the likelihood chapter Chapter 13. Imagine the following dataset with five data points drawn from a normal distribution (Figure 14.1).\n\nnum_data_points &lt;- 5\n\nmean_data &lt;- 3.0\nsd_data &lt;- 1.0\nnew_data &lt;- rnorm(num_data_points, mean = mean_data, sd = sd_data)\n\n\nhist(new_data, main = \"\")\n\n\n\n\n\n\n\nFigure 14.1: Five random samples from a normal distribution with mean = 3 and sd = 1\n\n\n\n\n\nWe can calculate the likelihood for a set of different means using the manual likelihood estimation that we did in Chapter 13.\n\n#DATA MODEL\ndelta_x &lt;- 0.1\nx &lt;- seq(-10,10, delta_x)\nnegative_log_likelihood &lt;- rep(NA,length(x))\n\nfor(i in 1:length(x)){\n  negative_log_likelihood[i] &lt;- -sum(dnorm(new_data, mean = x[i], sd = 1, log = TRUE))\n}\n\nFigure 14.2 is the likelihood surface for different values of the mean given the observation of x = 2.\n\nplot(x, negative_log_likelihood)\n\n\n\n\n\n\n\nFigure 14.2: Negative log-likelihood curve\n\n\n\n\n\nThe negative log-likelihood is useful for finding the maximum likelihood values with an optimizing function. However, here we want to convert back to a density (Figure 14.3)\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\n\n\nplot(x, density_likelihood, type = \"l\")\n\n\n\n\n\n\n\nFigure 14.3: likelihood curve\n\n\n\n\n\nThe y-axis is a tiny number because we multiplied probability densities together. This is OK for our illustrative purposes. To help visualize we can rescale so that the area under the curve is 1. The area is approximated where the density is the height and the width is the distance between points on the x-axis (delta_x) that we evaluated (height x width is the area of a bar under the curve). If we sum the bars together, we get the area under the curve (a value way less than 1). Dividing the likelihood by the area rescales the densities so the area under the curve is 1. Rescaling the likelihood is not a formal part of the analysis - just used here for visualizing.\n\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_x) #Width * Height\n\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\nThe rescaled likelihood looks like this (Figure 14.4)\n\nd &lt;- tibble(x = x,\n            likelihood = density_likelihood_rescaled)\nggplot(data = d, aes(x = x, y = likelihood)) +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 14.4: Rescale likelihood (area under the curve is 1)\n\n\n\n\n\nRemember that this curve is the \\(P(data \\| mean = x)\\) - the probability of the data given the model. We want the \\(P(mean = x \\| data)\\) - probability of the model given the data.\nFollowing Bayes rule where the \\(P(data | model)\\) is the likelihood and \\(P(model)\\) is our guess for the mean before seeing the data (called the prior)\n\\[\nP(model | data) = \\frac{P(data | model) \\cdot P(model)}{P(data)}\n\\]\nwe can multiply the likelihood x the prior.\n\\[\nP(mean | data) \\simeq likelihood \\cdot prior\n\\] Our prior is the following normal distribution\n\nmean_prior &lt;- 0\nsd_prior &lt;- 1.0\n\nWith the density for each value of x in Figure 14.5\n\ndensity_prior &lt;- dnorm(x, mean = mean_prior, sd = sd_prior)\n\n\nd &lt;- tibble(x = x,\n            density_prior = density_prior)\nggplot(d, aes(x = x, y = density_prior)) +\n  geom_line()  +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 14.5: likelihood of the x given the prior distribution\n\n\n\n\n\nPutting the rescaled likelihood on the same plot as the prior results in Figure 14.6\n\ntibble(x = x,\n            prior = density_prior,\n            likelihood_rescaled = density_likelihood_rescaled) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line()  +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 14.6: likelihood of the x given the prior distribution and likelihood of the model.\n\n\n\n\n\nMultiplying the prior x the likelihood (not rescaled) gives Figure 14.7\n\nprior_times_likelihood &lt;- density_prior * density_likelihood\n\n\nd &lt;- tibble(x = x,\n            prior_times_likelihood = prior_times_likelihood)\nggplot(d, aes(x = x, y = prior_times_likelihood)) +\n  geom_line()  +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 14.7: likelihood of the x given the prior distribution and likelihood of the model.\n\n\n\n\n\nBut from Bayes rule, we can rescale the prior * likelihood by the area under the curve to convert to a probability density function.\n\narea_under_curve &lt;- sum(prior_times_likelihood * delta_x)  #sum(Width * Height) = total probability of the data given all possible values of the parameter\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\nThe probability of the data is (i.e., the area under likelihood * prior curve): 2.2325827^{-6}\nNow we can visualize the rescaled likelihood (remember that the un-scaled likelihood was used in the calculations), the prior, and the normalized posterior.\nFigure 14.8 shows how the posterior is a blend of the prior and the likelihood.\n\ntibble(x = x,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line()  +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 14.8: Prior, likelihood, and posterior shown together\n\n\n\n\n\nHere is a function that will allow us to explore how the posterior is sensitive to the likelihood and the prior.\n\nexplore_senstivity &lt;- function(num_data_points, mean_data, sd_data, mean_prior, sd_prior, title){\n\nnew_data &lt;- rnorm(num_data_points, mean_data, sd_data)\ndelta_x &lt;- 0.1\nx &lt;- seq(-15,15, delta_x)\nnegative_log_likelihood &lt;- rep(NA,length(x))\n\n#DATA MODEL\nfor(i in 1:length(x)){\n  #Process model is that mean = x\n  negative_log_likelihood[i] &lt;- -sum(dnorm(new_data, mean = x[i], sd = 1, log = TRUE))\n}\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_x) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\n#Prior\ndensity_prior &lt;- dnorm(x, mean = mean_prior, sd = sd_prior)\n\n#Prior x Likelihood\nprior_times_likelihood &lt;- density_prior * density_likelihood\narea_under_curve &lt;- sum(prior_times_likelihood * delta_x) #Width * Height\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\np &lt;- tibble(x = x,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = x, y = density, color = distribution)) +\n  geom_line() +\n  labs(title = title)  +\n  theme_bw()\nreturn(p)\n}\n\nNow we can explore how the posterior is sensitive to 1) prior sd (i.e., confidence in the prior) 2) the number of data points used in the likelihood (how does increasing the number of data points influence the posterior) the prior mean 3) the mean of the data used in the likelihood (how different is it than the prior?)\n\n#Baseline\np1 &lt;- explore_senstivity(num_data_points = 5,\n                         mean_data = 3,\n                         sd_data = 2,\n                         mean_prior = 0,\n                         sd_prior = 1.0,\n                         title = \"Baseline\\n\")\n\n#Increase confidence in prior\np2 &lt;- explore_senstivity(num_data_points = 5,\n                         mean_data = 3,\n                         sd_data = 2,\n                         mean_prior = 0,\n                         sd_prior = 0.1,\n                         title = \"Increase confidence\\nin prior\")\n\n#Increase the number of data points\np3 &lt;- explore_senstivity(num_data_points = 50,\n                         mean_data = 3,\n                         sd_data = 2,\n                         mean_prior = 0,\n                         sd_prior = 1.0,\n                         title = \"Increase data\\n\")\n\n#Make likelihood mean closer to the prior\np4 &lt;- explore_senstivity(num_data_points = 5,\n                         mean_data = 0,\n                         sd_data = 2,\n                         mean_prior = 0,\n                         sd_prior = 1.0,\n                         title = \"Make likelihood mean\\ncloser to prior\")\n\n\np4 &lt;- explore_senstivity(num_data_points = 1,\n                        mean_data = 10,\n                        sd_data = 1.0,\n                        mean_prior = 10,\n                        sd_prior = 1.0,\n                        title = \"Make likelihood mean\\ncloser to prior\")\n\nFigure 14.9 shows the sensitivity of the posterior to assumptions of the prior and the data.\n\n\n\n\n\n\n\n\nFigure 14.9: Sensitivity of posteriors to different data and prior scenarios\n\n\n\n\n\nJust like we extended the likelihood analysis to the non-linear example, we can do the same for the Bayesian analysis.\nFirst, create a data set using the Michaelis-Menten function from the likelihood exercise. Here, instead of fitting both parameters, we only fit one parameter (the maximum or saturating value) called par1. In the chunk below we set the number of data points, the true value for par1, and the standard deviation of the data.\n\nnum_data_points &lt;- 10\npar1_true &lt;- 3\nsd_data &lt;- 0.5\nx &lt;- runif(num_data_points, 0, 10)\npar_true &lt;- c(par1_true, 0.5)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\n\n\nplot(x, y, ylim = c(0, par1_true + 2))\n\n\n\n\n\n\n\n\nNow we can define the prior. We think the prior is normally distributed with a mean and sd defined below\n\nmean_prior &lt;- 1.0\nsd_prior &lt;- 0.5\n\nHere is the manual calculation of the likelihood and the prior. We combine the results into a data frame for visualization.\n\ndelta_par1 &lt;- 0.1\npar1 &lt;- seq(-3,10, delta_par1)\nnegative_log_likelihood &lt;- rep(NA,length(par1))\nfor(i in 1:length(par1)){\n  #Process model\n  pred &lt;- par1[i] * (x / (x + par_true[2]))\n  #Data model\n  negative_log_likelihood[i] &lt;- -sum(dnorm(y, mean = pred, sd = sd_data, log = TRUE))\n}\n\ndensity_likelihood &lt;- exp(-negative_log_likelihood)\nlikelihood_area_under_curve &lt;- sum(density_likelihood * delta_par1) #Width * Height\ndensity_likelihood_rescaled &lt;- density_likelihood/likelihood_area_under_curve\n\n#Priors\ndensity_prior &lt;- dnorm(par1, mean = mean_prior, sd = sd_prior)\nprior_times_likelihood &lt;- density_prior * density_likelihood\narea_under_curve &lt;- sum(prior_times_likelihood * delta_par1) #Width * Height\nnormalized_posterior &lt;- prior_times_likelihood / area_under_curve\n\n\ntibble(par1 = par1,\n       prior = density_prior,\n       likelihood = density_likelihood_rescaled,\n       normalized_posterior = normalized_posterior) %&gt;% \n  pivot_longer(cols = -par1, names_to = \"distribution\", values_to = \"density\") %&gt;% \n  mutate(distribution = factor(distribution)) %&gt;% \n  ggplot(aes(x = par1, y = density, color = distribution)) +\n  geom_line()  +\n  theme_bw()\n\n\n\n\n\n\n\n\nNow we can look at how our prior and posterior distributions influence the shape of the process model curve (M-M). For illustration, the figure below shows the M-M curve using the most likely value from the prior, the most likely value if we just looked at the likelihood, and the most likely value from the posterior.\n\npar1_mle &lt;- par1[which.max(density_likelihood)]\npar1_post &lt;- par1[which.max(normalized_posterior)]\n\nd &lt;- tibble(x = seq(0,10, 0.1),\n            prior = mean_prior * (x / (x + par_true[2])),\n            likelihood = par1_mle * (x / (x + par_true[2])),\n            posterior = par1_post * (x / (x + par_true[2]))) %&gt;% \n  pivot_longer(cols = -x, names_to = \"distribution\", values_to = \"prediction\") %&gt;% \n  mutate(distribution = factor(distribution))\n\n\nggplot(d, aes(x = x, y = prediction, col = distribution)) +\n  geom_line() +\n  labs(y = \"M-M model prediction (process model)\")  +\n  theme_bw()",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#solving-a-bayesian-model",
    "href": "bayesian-methods-intro.html#solving-a-bayesian-model",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.2 Solving a Bayesian model",
    "text": "14.2 Solving a Bayesian model\nThe example above is designed to build an intuition for how a Bayesian analysis works but is not how the parameters in a Bayesian model are estimated in practice. For one thing, if there are multiple parameters being estimated, it is very hard to estimate the area under the curve. Second, the area is very very small if there are many data points (so small that the computer can’t hold it well).\nThere are two common methods for estimating posterior distributions that involve numerical computation. Both methods involve randomly sampling from an unknown posterior distribution and saving the samples. The sequence of saved samples of the parameters is called a Markov chain Monte Carlo (MCMC). The distribution of parameter values in the MCMC chain is your posterior distribution.\nThe first is called Gibbs sampling and is used when you know the probability distribution type for the posterior but not the parameter values of the distribution. For example, if your prior is normally distributed and your likelihood is normally distributed then you know (from math that others have already done for you) that the posterior is normally distributed. Therefore, you can randomly draw from that distribution to build your MCMC chain that generates your posterior distribution. We will not go over this in detail but know that this method will give you an answer quicker but requires you (or the software you are using) to know that your prior and likelihood are conjunct (i.e., someone else has worked out that the posterior always has a certain PDF if the prior and likelihood are of certain PDF - see pages 86-89 and Table A3 in Hobbs and Hooten).\nThe second is called MCMC Metropolis-Hastings (MCMC-MH) and is a rejection sampling method. In this case, you don’t know the form of the posterior. Basically, the MCMC-MH is the following\n\nCreate a vector that has a length equal to the total number of iterations you want to have in your MCMC chain. Set the first value of the vector to your parameter starting point.\nrandomly choose a new parameter value based on the previous parameter value in the MCMC chain (called a proposal). For example (where i is the current iteration in your MCMC chain): par_proposed &lt;- rnorm(1, mean = par[i - 1], sd = jump), where jump is the standard deviation that governs how far you want the proposed parameter to potentially be from the previous parameter.\nuse this proposed parameter in your likelihood and prior calculations and multiply the likelihood * prior (i.e., the numerator in Bayes formula). Save this as the proposed probability(prob_proposed)\nTake the ratio of the proposed probability to the previous probability (also called the current probability; prob_current). call this prob_proposed\nRandomly select a number between 0 and 1. Call this z\nIf prob_proposed from Step 3 is greater than z from Step 4 then save the proposed parameter for that iteration of MCMC chain. If it is less, then assign the previous parameter value for that iteration of the MCMC chain. As a result of Step 5:\n\n\nAll parameters that improve the probability will have prob_proposed/prob_current &gt; 1. Therefore all improvements will be accepted since z (by definition in Step 4) can never be greater than 1.\nWorse parameters where prob_proposed/prob_current &lt; 1, will be accepted in proportion to how worse they are. For example if prob_proposed/prob_current = 0.9, 90% of the time z will be less the 0.90 so the worse parameters that are worse by 10% will be accepted 90% of the time. If prob_proposed/prob_current = 0.01 (i.e., the new parameters are much worse), only 1% of the time will z be less than 0.01. Therefore it is possible but not common to save these worse parameters. As a result, the MCMC-MH approach explores the full distribution by spending more time at more likely parameter values. This is different than maximum likelihood optimization methods like optim that only save parameters that are better than the previous (thus finding the peak of the mountain rather than the shape of the mountain). The MCMC-HM approach requires taking a lot of samples so that it spends some time at very unlikely values - a necessity for estimating the tails of a distribution.\nYou want to accept ~40% percent of all proposed parameter values. If your jump parameter from #1 is too large, you won’t be able to explore the area around the most likely values (i.e., you won’t get a lot of prob_proposed/prob_current values near 1). If your jump parameter from #1 is too small, you won’t be able to explore the tails of the distribution (i.e., you won’t get a lot of prob_proposed/prob_current values near 0 that have a random chance of being accepted).\n\nNote: There is a step that is ignored here that just confuses at this stage. Your proposal distribution in #1 doesn’t have to be normal, which is symmetric (i.e., your probability of jumping from a value of X to Y is the same as jumping from Y to X). There is an adjustment for non-symmetric proposals that is on page 71 in Dietze and page 158 in Hobbs and Hooten.\nHere is an example of the MCMC-MH method:\n(Note: the example below should use logged probability densities for numerical reasons but use the non-logged densities so that the method is clearer. The example in the assignment uses logged densities).\nSet up data (same as above)\n\nnum_data_points &lt;- 10\npar1_true &lt;- 3\nsd_data &lt;- 0.5\nx &lt;- runif(num_data_points, 0, 10)\npar_true &lt;- c(par1_true, 0.5)\ny_true &lt;- par_true[1] * (x / (x + par_true[2]))\ny &lt;- rnorm(length(y_true), mean = y_true, sd = sd_data)\n\n\nplot(x, y, ylim = c(0, par1_true + 2))\n\n\n\n\n\n\n\n\nRun MCMC-MH\n\n#Initialize chain\nnum_iter &lt;- 1000\npars &lt;- array(NA, dim = c(num_iter))\npars[1] &lt;- 2\nlog_prob_current &lt;- -10000000000\nprob_current &lt;- exp(log_prob_current)\njump &lt;- 0.8\n\nmean_prior &lt;- 1.0\nsd_prior &lt;- 0.5\n\naccept &lt;- rep(NA,num_iter)\naccept[1] &lt;- 1\n\nfor(i in 2:num_iter){\n  \n  #Randomly select new parameter values\n  proposed_pars &lt;- rnorm(1, pars[i - 1], jump)\n  \n    \n  #PRIORS: how likely is the proposed value given the prior distribution?\n  prior &lt;- dnorm(proposed_pars, mean = mean_prior, sd = sd_prior)\n\n  #PROCESS MODEL: Use new parameter values in the process model\n  pred &lt;- proposed_pars * (x / (x + par_true[2]))\n\n  #DATA MODEL: how likely is the data given the proposed parameter?\n  #We are multiplying here\n  likelihood &lt;- prod(dnorm(y, mean = pred, sd = sd_data))\n  \n  #Combine the prior and likelihood\n  #remember that you multiply probabilities which means you can add log(probability)\n  prob_proposed &lt;- prior * likelihood\n  \n  z &lt;- (prob_proposed/prob_current)\n  \n  #Now pick a random number between 0 and 1\n  r &lt;- runif(1, 0, 1)\n  \n  #If z &gt; r then accept the new parameters\n  #Note: this will always happen if the new parameters are more likely than\n  #the old parameters z &gt; 1 means than z is always &gt; r no matter what value of\n  #r is chosen.  However, it will accept worse parameter sets (P_new is less\n  #likely then P_old - i.e., z &lt; 1) in proportion to how much worse it is\n  #For example: if z = 0.9 and then any random number drawn by `runif` that is\n  #less than 0.90 will result in accepting the worse values (i.e., the slightly\n  #worse values will be accepted a lot of the time).  In contrast, if z = 0.01\n  #(i.e., the new parameters are much much worse), then they can still be accepted\n  #but much more rarely because random r values of &lt; 0.1 occur more rarely\n  #print(c(pars[i - 1], proposed_pars, z, r))\n  if(z &gt; r){\n    accept[i] &lt;- 1\n    pars[i] &lt;- proposed_pars\n    prob_current &lt;- prob_proposed\n  }else{\n    accept[i] &lt;- 0\n    pars[i] &lt;- pars[i - 1]\n    prob_current &lt;- prob_current #this calculation isn't necessary but is here to show you the logic\n  }\n}\n\nThe pars variable is our MCMC chain estimating the posterior distribution. We can visualize it in two ways. The first is with the iteration number on the x-axis. The second is a histogram. A chain that is ready for analysis will have a constant mean and variance. The variance is important because it is the exploration of the posterior distribution. The histogram shows the posterior distribution.\n\nd &lt;- tibble(iter = 1:num_iter,\n       par1 = pars)\n\np1 &lt;-  ggplot(d, aes(x = iter, y = par1)) +\n  geom_line()  +\n  theme_bw()\n\np2 &lt;- ggplot(d, aes(x = par1)) +\n  geom_histogram() +\n  theme_bw()\n\np1 | p2\n\n\n\n\n\n\n\n\nYou should notice that the chain starts at 2 before moving to a mean of 3. The starting value of 2 was arbitrary. Since it was far from 3, the proposed new parameter values often resulted in improvements and accepting more likely values. As a result, the chain moves to the part with a mean of 3 and constant variance (i.e., where the chain has converged). This transition from the starting value to the point where the chain has converged should be discarded. We call this the “burn-in”. Here are the same plots with the burn-in removed.\n\nnburn &lt;- 100\nd_burn &lt;- tibble(iter = nburn:num_iter,\n       par1 = pars[nburn:num_iter])\n\n\np1 &lt;-  ggplot(d_burn, aes(x = iter, y = par1)) +\n  geom_line() +\n  theme_bw()\n\np2 &lt;- ggplot(d_burn, aes(x = par1)) +\n  geom_histogram() +\n  theme_bw()\n\np1 | p2\n\n\n\n\n\n\n\n\nNow you can analyze the chain to explore the posterior distribution\n\npar_post_burn &lt;- pars[nburn:num_iter]\n\n#Mean\nmean(par_post_burn)\n\n[1] 2.952403\n\n#sd\nsd(par_post_burn)\n\n[1] 0.1653424\n\n#Quantiles\nquantile(par_post_burn, c(0.025, 0.5,0.975))\n\n    2.5%      50%    97.5% \n2.652697 2.960757 3.281253 \n\n\nFinally, you can sample from the posterior distribution just like you would sample from a random variable using the rnorm, rexp, rlnorm, etc. function. The key is to randomly select an iteration (num_sample = 1) or a set of samples (if num_sample &gt; 0) with replacement (replace = TRUE; i.e., an iteration could be randomly selected multiple times).\n\nnum_samples &lt;- 100\nsample_index &lt;- sample(x = 1:length(par_post_burn), size = num_samples, replace = TRUE)\nrandom_draws &lt;- par_post_burn[sample_index]\n\n\nhist(random_draws)",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#predictive-posterior-distributions",
    "href": "bayesian-methods-intro.html#predictive-posterior-distributions",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.3 Predictive posterior distributions",
    "text": "14.3 Predictive posterior distributions\nFinally, we can use the idea of randomly drawing from the posterior to develop predictions from the posterior. This is just like the Logistic growth module where you randomly sampled from the parameter uncertainty and used the random samples in the logistic equation.\nHere we are calculating two things 1) pred_posterior_mean just has uncertainty in the M-M model parameter. Think about this as generating uncertainty around the mean prediction at each value of x. 2) y_posterior is the prediction of an observation. Therefore, you take the value from #1 and add the uncertainty in the observations from sd_data that was set above. This is your predictive or forecast uncertainty.\n\nnum_samples &lt;- 1000\nx_new &lt;- x\npred_posterior_mean &lt;- matrix(NA, num_samples, length(x_new))   # storage for all simulations\ny_posterior &lt;- matrix(NA, num_samples, length(x_new)) \n\nfor(i in 1:num_samples){\n  sample_index &lt;- sample(x = 1:length(pars), size = 1, replace = TRUE)\n  pred_posterior_mean[i, ] &lt;- pars[sample_index] * (x_new / (x_new + par_true[2]))\n  y_posterior[i, ] &lt;- rnorm(length(x_new), pred_posterior_mean[i, ], sd = sd_data)\n  \n}\nn.stats.y &lt;- apply(y_posterior, 2, quantile, c(0.025, 0.5, 0.975))\nn.stats.y.mean &lt;- apply(y_posterior, 2, mean)\n\nn.stats.mean &lt;- apply(pred_posterior_mean, 2, quantile, c(0.025, 0.5, 0.975))\n\nd &lt;- tibble(x = x_new,\n            median = n.stats.y.mean,\n            lower95_y = n.stats.y[1, ],\n            upper95_y = n.stats.y[3, ],\n            lower95_mean = n.stats.mean[1, ],\n            upper95_mean = n.stats.mean[3, ],\n            obs = y)\n\n\nggplot(d, aes(x = x)) +\n  geom_ribbon(aes(ymin = lower95_y, ymax = upper95_y), fill = \"lightblue\", alpha = 0.5) +\n    geom_ribbon(aes(ymin = lower95_mean, ymax = upper95_mean), fill = \"pink\", alpha = 0.5) +\n  geom_line(aes(y = median)) +\n  geom_point(aes(y = obs)) +\n  labs(y = \"M-M Prediction\")  +\n  theme_bw()",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#important-considerations",
    "href": "bayesian-methods-intro.html#important-considerations",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.4 Important considerations",
    "text": "14.4 Important considerations\n\n14.4.1 Joint vs marginal distributions\nIf you have multiple parameters in your MCMC chain, you randomly draw posterior_sample_indices and use values for all the parameters at that iteration. By doing this you are representing the joint distribution of the parameter - e.g., if parameter 1 is high, parameter 2 is always low. If you select posterior_sample_indices for each parameter, you break the correlations of parameters that the MCMC method estimated and, as a result, overestimate the uncertainty.\n\n\n14.4.2 Identifiability\nImagine you have a model that predicts water temperature as a linear function of air temperature but uses the following equation:\nwater temperature = m + a * b * air temperature\nIn this toy example, the slope has been separated into two parameters (a and b). As a result, you can get the same value for a slope with a large value a and a small value for b as when you have a small value for a and a large value for b. Consequentially it is impossible to find a value for either a or b and these parameters are unidentifiable. If you collapse the a*b down into a single parameter (m) then that parameter is identifiable.\nIn an ecosystem example, we often use net fluxes (like net ecosystem exchange) to calibrate models. Since NEE is the net of respiration and photosynthesis, the same NEE can emerge from a system with large values for respiration and photosynthesis as a system with small values for respiration and photosynthesis. Therefore the parameters that govern respiration and photosynthesis are not identifiable using only NEE data for parameter estimation.\nIdentifiability issues can be addressed by 1) adding additional data sources for parameter estimation. For example, in the next chapter, we use NEE and LAI in the calibration. The LAI data helps separate the low production from the high production system because a lower production system can not be associated with high LAI values in the model. 2) by using “stronger” priors that have distributions with less spread (see below) or 3) by modifying the equation(s) of the model to reduce the trade-off in parameters.\n\n\n14.4.3 Data leakage\nData that contributed to the prior distribution can be me used in the likelihood (data leaking from the prior to posterior). This will result in artificial re-enforcement of the prior and will give artificially large confidence in the posterior. Similarly, data that you collected that will be used in the likelihood can not be used to develop the prior.\nYou can use data that you will use in your likelihood calculation to involve the starting point of your MCMC chain. In some cases, a maximum likelihood fit of your model, like in Chapter 13, will yield starting points that are very close mean of the posterior (just without the uncertainty estimation). By starting your MCMC chain at these maximum likelihood estimates for the parameters, you will reduce the burn-in iterations and increase the computational efficiency of your Bayesian analysis. This is not data leakage.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#priors",
    "href": "bayesian-methods-intro.html#priors",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.5 Priors",
    "text": "14.5 Priors\nInformed, strong vs. uninformed, vague, weak\nThe parameters of the distribution determine how informed the prior is. For example, a normal distribution with a mean = 0 and sd = 0.1 is much more informed than one with the sd = 1000 (the latter has similar probability densities across a large range of parameter values). It is also important to consider the range of support in a distribution. For example, a normal distribution has non-zero densities for parameter values from -infinity to +infinity. Therefore the prior density for a parameter value will never be 0 (thus resulting in the prior * likelihood = 0, and the MCMC never accepting that parameter value). In contrast, a uniform distribution has hard bounds that are user-provided. If the prior is uniform from 0 to 1, then no parameter values less than 0 or greater than 1 will be considered even if the data supports these values. Other probability distributions have embedded bounds. For example, the log-normal distribution can have values less than 0. Overall, hard bounds can be desired because of biophysical constraints on the parameter (e.g., a rate parameter or a proportion parameter can’t be negative).\nIn some cases, they are so informed that you assume they are a fixed value. Justification for the parameter value choice when fixing a parameter or using a strong prior.\nRemember that the priors are for specific parameters in the model. The information used to develop informed priors must come from comparable information about the same parameter.\n\nLiterature review\nDatabases\nExpert options/survey\n\n\n14.5.1 Prior key points\nYou may want to consider informed priors if\n\nThere are known issues with parameter identifiability in your model\nThe parameter is a physical constant (like gravity constant)\nThere is considerable prior research on a parameter\nThere are physical constraints to a parameter (e.g., it can’t be a negative value)\n\nYou may consider using informed priors if there is a history of using a set of parameter values in previous applications of the model. In this case, be aware of zombie parameters, that is parameters whose values appear from the literature to be known because everyone uses it, but are in fact a relic of a modeler’s decision in the past that has carried through many model applications without being closely examined. The parameter may be less certain than the literature indicates.\nYou may want to consider uninformed priors if\n\nYou need a prior for the parameter but do not have any information about it.\nThere is a desire to have the analysis reflect the data only",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#additional-bayesian-topics",
    "href": "bayesian-methods-intro.html#additional-bayesian-topics",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.6 Additional Bayesian topics",
    "text": "14.6 Additional Bayesian topics\nThis chapter is an introduction to the concepts and numerical techniques used in Bayesian analysis. There are more advanced topics that may be of interest but are not covered here. These include:\n\nState-space modeling: state-space models are used to estimate process uncertainty for dynamic models like the forest model in Chapter 16. When estimating process uncertainty using a state-space model, the uncertainty represents the uncertainty that accumulates between time steps in the model.\nHierarchical models: Hierarchical models are used when you want a parameter to vary over time and space but don’t want to estimate unique parameters for every space and time point. Hierarchical models allow for you to estimate a “global” parameter distribution whereby the individual parameters at a site or a point in time are samples from the global distribution. Hierarchical models allow information from one site to help inform parameter values for another site while considering them independent.\nPackages for posterior estimation: Many packages exist to estimate posteriors in Bayesian models that differ in their algorithms, capacities for complex applications, and available documentation. These include Stan, JAGS, Nimble, and greta. All packages can solve simple Bayesian problems like the M-M example above. Packages have different strengths for solving more complex problems. Some are also more well-suited for use with custom functions and dynamic models like the forest model in this book.\nMore robust algorithms for estimating posteriors and the use of conjecture priors to speed posterior estimation.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#reading",
    "href": "bayesian-methods-intro.html#reading",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.7 Reading",
    "text": "14.7 Reading",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "bayesian-methods-intro.html#problem-set",
    "href": "bayesian-methods-intro.html#problem-set",
    "title": "14  Parameter calibration using Bayesian methods",
    "section": "14.8 Problem Set",
    "text": "14.8 Problem Set\nThe problem set is located in bayesian_problem_set.qmd in https://github.com/frec-5174/book-problem-sets. You can fork the repository or copy the code into your own quarto document",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameter calibration using Bayesian methods</span>"
    ]
  },
  {
    "objectID": "particle-filter.html",
    "href": "particle-filter.html",
    "title": "15  Data assimilation using the particle filter",
    "section": "",
    "text": "15.1 Review of Batch vs Sequential Methods\nBefore beginning the introduction of the particle filter it is important to revisit the MCMC-MH approach that we used to estimate the parameters of a Bayesian model. Here is the general algorithm again:\nAbove, you see that the outermost for-loop is looping over the number of iterations (num_iterations). The inner loop is looping over the length of the time series (length_of_time_series). Therefore, this approach tests each parameter value choice in an iteration (and latent state if using a state space model) over ALL time points in the time series. As a result, we call this a batch method because it considers all data as a single “batch” of data.\nStrengths of the batch method are:\n- The parameters are consistent with all data\n- Straightforward to estimate parameters, uncertainty parameters, and latent states\nWeaknesses:\n- Can be computationally slow\n- Require re-fitting model if there is even a single new data point\nAlternatively, sequential methods only analyze data one time point at a time. Here is general code for a sequential method - notice that the for-loop order is reversed\nIn the sequential method, we can restart at any time point, as long as we have the values for the states and parameters that are associated with each particle. When doing an iterative forecast, these values are what you would save to wait for new data to arrive.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#review-of-batch-vs-sequential-methods",
    "href": "particle-filter.html#review-of-batch-vs-sequential-methods",
    "title": "15  Data assimilation using the particle filter",
    "section": "",
    "text": "for(i in 1:num_iterations){\n\nChoose new parameters based on previous parameters\n\nfor(t in 1:length_of_time_series){\n\nMake predictions over the complete time series\n\n}\n\nCalculate the likelihood of data given model and current parameters\nCalculate the probability of priors given current parameters\n\nAccept or reject parameters\n\n}\n\n\n\n\n\nCalculate the prior distribution of parameters\n\nfor(t in 1:length_of_time_series){\n\nfor(i in 1:num_of_particles){\n\nMake predictions for each particle based on the previous value for the particle\n\n}\n\nCompare particle to data (likelihood or other technique)\n\nWeight particles based on the comparison to the data\n\nAdjust particles based on weights\n\n}",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#introduction-to-particle-filter",
    "href": "particle-filter.html#introduction-to-particle-filter",
    "title": "15  Data assimilation using the particle filter",
    "section": "15.2 Introduction to Particle Filter",
    "text": "15.2 Introduction to Particle Filter\nMany sequential data assimilation methods make different assumptions about data and model distributions to simplify the analysis. Many of the methods emerged before our massive computational resources or were developed for HUGE problems like assimilating terabytes of data into a global weather model that simulates the physics of the atmosphere at 25 km vertical resolution. Examples include the Kalman Filter, Extended Kalman Filter, Ensemble Kalman Filter, and 4D-var. These methods all heavily use matrix algebra which I have not introduced in this class. Since these methods commonly assume that the data and model errors are normally distributed, the numerical version (Ensemble Kalman Filter) can run fewer particles (in this case: ensemble members) because it doesn’t take as many samples to estimate the mean and variance of a distribution than it does to estimate the full distribution\nHere, I introduce the particle filter. The particle filter is a sequential method that will be more familiar to those who have learned the Bayesian methods that we have covered in the earlier chapters (Chapter 14). It has the concept of a likelihood, of which you are well versed (Chapter 13). The particle filter does not assume a specific distribution of the data and model error so it requires more particles to estimate the full distributions. As a result, it is more appropriate for “smaller” problems like the ones we are tackling in this book\nThe particle filter is quite simple\n\nInitialize a set of particles: Set the initial distribution of the states for each particle and, if estimating parameters, the initial distribution of parameters that you want to estimate. Think of this as your initial priors.\nPredict the next time step using your process model for each particle. Add process uncertainty to each particle (i.e., rnorm)\nIf there are observations at the time step, calculate the likelihood of the data given the particle just like we calculated the likelihood in the likelihood and Bayesian exercises. For example: LL &lt;- dnorm(obs, mean = pred, sd = sd_obs). You will typically use the uncertainty of the observations in the likelihood because you have already included the process uncertainty in #2. If observations are not available at the time step, then continue to the time step.\nIf there are observations at the time step, resample the particles using the likelihood as the weights (don’t forget to exponentiate the likelihood if you logged it in #3, the weights must be probabilities rather than log probabilities). The weighted sampling, with replacement, will randomly pick the more likely particles more often. You will keep the same number of particles but the values for each particle will change. Less likely particles will be replaced with more likely particles (though the less likely particles can still be selected). Be sure to resample all states together and, if also estimating parameters, the parameters as well. The key to resampling is the following:\n\n## Calculate likelihood (weight) for one state with an observation at that time-step\n## The dimensiosn of x\nwt &lt;- dnorm(obs[t], mean = x[t, ], sd = sd_data)\n\n# Normalize the weights\nwt_norm &lt;- wt / sum(wt)\n\n## resample ensemble members in proportion to their weight.  \n## Since the total number of samples = the number of particles then you will preserve the \n## Same number of particles\nresample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n\n##Use the index to resample\nx[t, ] &lt;- x[t, resample_index]\n\nContinue to the next time step.\n\nFundamentally, the particle filter depends on two concepts that you have already been exposed to: likelihood and sampling from a set of particles (you sampled iterations from MCMC in previous exercises).\nSpecifically, we refer to the particle filter described above as a bootstrap particle filter.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#example-of-particle-filter",
    "href": "particle-filter.html#example-of-particle-filter",
    "title": "15  Data assimilation using the particle filter",
    "section": "15.3 Example of Particle Filter",
    "text": "15.3 Example of Particle Filter\nHere is an example of the particle filter applied to canopy greenness data at a NEON site. Canopy greenness is the relative greenness of a photograph taken by a camera located above the vegetation canopy (called a phenocam). Greenness is qualified using the 90% percentile of the daily GCC (GCC = green / (green + red + blue)), called the gcc_90. gcc_90 measures spring changes in vegetation.\nFirst load in data. We are only going to focus on the 13 weeks of the data at a single site (CPER).\n\nlibrary(tidyverse)\ndf &lt;- read_csv(\"https://sdsc.osn.xsede.org/bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=P1D/phenology-targets.csv.gz\", show_col_types = FALSE) |&gt;\n  filter(variable == \"gcc_90\" & site_id == \"CPER\",\n         datetime &gt; as_date(\"2023-04-01\") & datetime &lt; as_date(\"2023-06-30\"),\n         wday(datetime) == 1)\n\nFigure 15.1 shows the data for CPER\n\nggplot(df, aes(x = datetime, y = observation)) +\n  geom_point() +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.1: Greeness from phenocam data at CPER\n\n\n\n\n\n\n15.3.1 Particle Filter with no-observations\nFirst, we are going to run the particle filter with all data missing. This is equal to the random walk.\nThe sd_add is the model process uncertainty.\nThe sd_obs is the observation uncertainty. It is also used to set the initial condition uncertainty since we start the particle filter on a day with an observation.\nThe key decision is the num_particles. More is always better but it comes at a computational and computer memory cost.\n\nnum_particles &lt;- 25\n\nobs &lt;- df$observation\n\nnum_time_steps &lt;- length(obs)\n\n#This sets all the observations to NA after the first\nobs[2:num_time_steps] &lt;- NA\n\nsd_add &lt;- 0.005\nsd_obs &lt;- 0.01\n\nx &lt;- array(NA, dim = c(num_time_steps, num_particles))\nx[1, ] &lt;- rnorm(num_particles, mean = obs[1], sd = sd_obs)\n\nx_prior &lt;- x \nx_prior[1, ] &lt;- x[1, ]\n\nfor(t in 2:num_time_steps){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    x[t, m ] &lt;- x[t - 1, m  ] + rnorm(1, mean = 0, sd = sd_add)\n  }\n  \n  x_prior[t, ] &lt;- x[t, ]\n  \n  ## analysis step\n  if(!is.na(obs[t])){ \n    \n    ## calculate Likelihood (weights)\n    wt &lt;- dnorm(obs[t], mean = x[t, ], sd = sd_obs)    \n    ## calculate likelihood (weight)\n    \n    wt_norm &lt;- wt / sum(wt)\n    \n    ## Resample ensemble members in proportion to their weight\n    resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n    \n    x[t, ] &lt;- x[t, resample_index]  ## update state\n  }\n}\n\nFigure 15.2 shows each particle.\n\ntibble(time = df$datetime,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -time, names_to = \"ensemble\", values_to = \"x\") |&gt; \n  #mutate(x = exp(x)) |&gt; \n  ggplot(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_line() +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.2: Results of the particle filter with each particle shown\n\n\n\n\n\nLet’s save the output as a different name so we can compare to a PF with observations.\n\nx_no_obs &lt;- x\n\n\n\n15.3.2 PF with observations\nNow we can examine how the PF uses observations to update the model and modify the trajectory. The following is the same as above except that there are data at weeks 1, 5, and 10.\n\nobs[c(5, 10)] &lt;- df$observation[c(5, 10)]\n\nx &lt;- array(NA, dim = c(num_time_steps, num_particles))\nx[1, ] &lt;- rnorm(num_particles, mean = obs[1], sd = sd_obs)\n\nx_prior &lt;- x \nx_prior[1, ] &lt;- x[1, ]\n\nfor(t in 2:num_time_steps){\n  \n  ## forward step\n  for(m in 1:num_particles){\n    x[t, m ] &lt;- x[t - 1, m  ] + rnorm(1, mean = 0, sd = sd_add)\n  }\n  \n  x_prior[t, ] &lt;- x[t, ]\n  \n  ## analysis step\n  if(!is.na(obs[t])){ \n    \n    ## calculate Likelihood (weights)\n    wt &lt;- dnorm(obs[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n    \n    wt_norm &lt;- wt / sum(wt)\n    \n    ## resample ensemble members in proportion to their weight\n    resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n    \n    x[t, ] &lt;- x[t, resample_index]  ## update state\n    \n  }\n}\n\nFigure 15.3 shows the particles with the observations. You can see how the particles are adjusted when data are present.\n\ntibble(time = df$datetime,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -time, names_to = \"ensemble\", values_to = \"x\") |&gt; \n  ggplot(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_line() +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.3: Results of the particle filter with each particle shown. Observations are included\n\n\n\n\n\nSave the output as a different object to compare to other PF simulations\n\nx_with_obs &lt;- x\n\nNow we can compare the influence of data assimilation on the last 3 weeks of the time series (think of this as a 3-week forecast)\n\nno_obs &lt;- tibble(time = df$datetime,\n                 as_tibble(x_no_obs)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"no obs\")\n\nwith_obs &lt;- tibble(time = df$datetime,\n                   as_tibble(x_with_obs)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"with obs\")\n\ncombined &lt;- bind_rows(no_obs, with_obs)\n\ndf$obs_in_fit &lt;- obs\n\ncombined %&gt;% \n  group_by(time, type) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = df, aes(x = datetime, y = observation), color = \"red\") +\n  geom_point(data = df, aes(x = datetime, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n15.3.3 PF through a Bayesian lens\nThe particle filter is fundamentally a Bayesian analysis.\n\nPrior: The ensemble predictions before data assimilation.\nLikelihood: the distribution of the data with uncertainty.\nPosterior: The ensemble and weights after data assimilation.\n\nThe figure below illustrates these Bayesian concepts. In the figure, the ensemble predictions are converted to a smoothed histogram and the observations are converted to normal distributions using the observation standard deviation. Data are assimilated at time steps 5 and 10. The prior, likelihood, and posterior are shown on the days that data are assimilated. Only prior is shown on the other days because no data are available to convert to a posterior.\n\ncombined |&gt; \nggplot(aes(x = x, y = y, color = type, group = date)) + \n  geom_point(size = 0.4) + \n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.4: Particle filter through a Bayesian lens\n\n\n\n\n\n\n\n15.3.4 PF with parameter estimation\nThe estimate of parameters using a PF is also straightforward. Here we model the change in gcc_90 using an equation with a growth term included. The growth rate is highest for a defined day of the year (doy) with a bell shape around that doy.\nx[t-1, m] + r[t, m] * exp(-(1 / (2 * b)) * (doy[t] - a)^2)\nThe values for a (the peak doy) and b (the width of the bell shape) are provided.\nWe are estimating r, (the maximum growth rate). Just like we need to initialize the states at the first time step, we will initialize the distribution of the b1 at the first time step using a normal distribution with a mean = r_mean and sd = r_sd.\n\nnum_particles &lt;- 25\nobs &lt;- rep(NA, length(df$observation))\nobs[1] &lt;- df$observation[1]\nobs[c(5, 7, 10)] &lt;- df$observation[c(5, 7, 10)]\n\nsd_add &lt;- 0.005\nsd_obs &lt;- 0.01\n\na &lt;- 141\nb &lt;- 30\nr_mean &lt;- 0.04 #0.025\nr_sd &lt;- 0.01\n\nx &lt;- array(NA, dim = c(num_time_steps, num_particles))\nx[1, ] &lt;- rnorm(num_particles, obs[1], sd = sd_obs)\n\nr &lt;- array(NA, dim = c(num_time_steps, num_particles))\nr[1, ] &lt;- rnorm(num_particles, mean = r_mean, sd = r_sd)\n\ndoy &lt;- yday(df$datetime)\n\nNow run the PF with the model. You need to also carry through the values for r.\nImportantly, the distribution of r carries through from the previous time step. When there is an observation, r is resampled using the same index that the states are resampled. This ensures that the parameters match the states from the same particle.\n\nfor(t in 2:num_time_steps){\n  \n  for(m in 1:num_particles){\n    \n    r[t, m] &lt;- r[t-1, m]\n    pred &lt;- x[t-1, m] + r[t, m] * exp(-(1 / (2 * b)) * (doy[t] - a)^2)\n    x[t, m ] &lt;- pred + rnorm(1, mean = 0, sd = sd_add)\n    \n  }\n  \n  ## analysis step\n  if(!is.na(obs[t])){\n    \n    ## calculate Likelihood (weights)\n    wt &lt;- dnorm(obs[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n    \n    wt_norm &lt;- wt / sum(wt)\n    \n    ## resample ensemble members in proportion to their weight\n    resample_index &lt;- sample(1:num_particles, \n                             num_particles, \n                             replace = TRUE, \n                             prob = wt_norm) \n    \n    x[t, ] &lt;- x[t, resample_index]  ## update state\n    r[t, ] &lt;- r[t, resample_index] ## Parameter update\n  }\n}\n\nFigure 15.5 shows the states from the PF when parameter fitting is included\n\ntibble(time = df$datetime,\n       obs = obs,\n       as_tibble(x)) %&gt;% \n  pivot_longer(cols = -c(time, obs), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  ggplot() +\n  geom_line(aes(x = time, y = x, group = factor(ensemble))) +\n  geom_point(aes(x = time, y = obs), color = \"red\") +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.5: Results of the particle filter when parameter fitting is included\n\n\n\n\n\nAnd visualize the time evolution of the parameter (Figure 15.6). There are two new concepts illustrated below:\n\nParameter distributions evolve through time. As a result, the distribution of parameters is strongly influenced by the most recent observations. The distributions produced by a PF are not the same as the distributions produced by the MCMC chain.\n\nparticles can have degeneracy, whereby the values of the parameters collapse down to one or a few values. This occurs because the PF does not propose new parameter values, it only selects (through resampling) parameter values from the initial set that you started with. Over the time-step, the PF weeds out bad parameters and only a few ones are left. This is a major issue with a PF. Degeneracy can also occur in the states but since we are adding process uncertainty (sd_add) the particles can separate through time. There are a couple of ways to solve degeneracy - increase the number of particles so you sample more initial parameter values or add random noise to each parameter between times-steps, similar to adding process noise to model states. It is important to find the right account of random noise to allow parameters to evolve through time but not too much where the spread becomes unreasonable if data is not available to assimilate and reduce the spread.\n\n\n\n\n\n\n\n\n\nFigure 15.6: Timing evolution of the parameters\n\n\n\n\n\n\n\n15.3.5 Sensitivity to the number of particles\nThe number of particles is a key decision when using a PF. To explore the sensitivity of the PF to the number of particles, here is a function that can be reused with different numbers of particles. It is the same as the dynamic model above and returns the r and r for the particles in a list.\n\nbootstrap_pf &lt;- function(num_particles, sd_add = 0.005, sd_obs = 0.01){\n  \n  x &lt;- array(NA, dim = c(num_time_steps, num_particles))\n  x[1, ] &lt;- rnorm(num_particles, obs[1], sd = sd_obs)\n  \n  r &lt;- array(NA, dim = c(num_time_steps, num_particles))\n  r[1, ] &lt;- rnorm(num_particles, mean = r_mean, sd = r_sd)\n  \n  ### resampling bootstrap particle filter\n  \n  for(t in 2:num_time_steps){\n    \n    ## The new \n    for(m in 1:num_particles){\n      \n      r[t, m] &lt;- r[t-1, m]\n      pred &lt;- x[t-1, m] + r[t, m] * exp(-(1 / (2 * b)) * (doy[t] - a)^2)\n      x[t, m ] &lt;- pred + rnorm(1, mean = 0, sd = sd_add)\n      \n    }\n    \n    ## analysis step\n    if(!is.na(obs[t])){\n      \n      ## calculate Likelihood (weights)\n      wt &lt;- dnorm(obs[t], mean = x[t, ], sd = sd_obs)    ## calculate likelihood (weight)\n      \n      wt_norm &lt;- wt / sum(wt)\n      \n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:num_particles, num_particles, replace = TRUE, prob = wt_norm) \n      \n      x[t, ] &lt;- x[t, resample_index]  ## update state\n      r[t, ] &lt;- r[t, resample_index] ## Parameter update\n    }\n  }\n  return(list(x = x, r = r))\n}\n\nFirst, run the PF using 10, 100, and 1000 particles\n\npf_10 &lt;- bootstrap_pf(10)\npf_100 &lt;- bootstrap_pf(100)\npf_1000 &lt;- bootstrap_pf(1000)\n\np10 &lt;- tibble(time = df$datetime,\n              as_tibble(pf_10$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_10\")\n\np100 &lt;- tibble(time = df$datetime,\n               as_tibble(pf_100$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_100\")\n\np1000 &lt;- tibble(time = df$datetime,\n                as_tibble(pf_1000$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"pf_1000\")\n\n\ndf$obs_in_fit &lt;- obs\n\nAnd combine into a single plot (Figure 15.7). You see the width of the 95% confidence interval increases substantially from 10 to 1000 particles but then is similar from 1000 to 10000. This reflects what we learned from the very first exercise where you drew random samples from a distribution and found that between 1000 and 10000 random samples were required to approximate the distribution well.\n\nbind_rows(p10, p100, p1000) %&gt;% \n  group_by(time, type) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = df, aes(x = datetime, y = observation), color = \"red\") +\n  geom_point(data = df, aes(x = datetime, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.7: Sensitivity of predictions to different numbers of particles\n\n\n\n\n\n\n\n15.3.6 Sensitivity to observation uncertainty\nWe can also explore the sensitivity of the PF to observation uncertainty. Intuitively, we should have a stronger update (i.e., the particles are adjusted to be closer to the observation) when there is less uncertainty in the observations. The code below explores whether this intuition is correct.\n\npf_low_obs &lt;- bootstrap_pf(5000, sd_obs = 0.001)\n\npf_high_obs &lt;- bootstrap_pf(5000, sd_obs = 0.1)\n\npf_low &lt;- tibble(time = df$datetime,\n                 as_tibble(pf_low_obs$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"low obs uncertainity\")\n\npf_high &lt;- tibble(time = df$datetime,\n                  as_tibble(pf_high_obs$x)) %&gt;% \n  pivot_longer(cols = -c(\"time\"), names_to = \"ensemble\", values_to = \"x\") %&gt;% \n  mutate(type = \"high obs uncertainity\")\n\ndf$obs_in_fit &lt;- obs\n\nDoes Figure 15.8 match your intuition?\n\nbind_rows(pf_low, pf_high) %&gt;% \n  group_by(time, type) %&gt;% \n  summarise(mean = mean(x),\n            upper = quantile(x, 0.975),\n            lower = quantile(x, 0.025),.groups = \"drop\") %&gt;% \n  ggplot(aes(x = time, y = mean)) +\n  geom_line(aes(color = type)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, color = type, fill = type), alpha = 0.2) +\n  geom_point(data = df, aes(x = datetime, y = observation), color = \"red\") +\n  geom_point(data = df, aes(x = datetime, y = obs_in_fit), color = \"black\") +\n  labs(x = \"Datetime\", y = \"gcc 90\", title = \"Canopy greenness\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 15.8: Sensitivity of predictions to observation uncertainty",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#key-points",
    "href": "particle-filter.html#key-points",
    "title": "15  Data assimilation using the particle filter",
    "section": "15.4 Key points",
    "text": "15.4 Key points\nIt is important to consider the relative balance of model uncertainty and observation uncertainty. If the Observation uncertainty &gt; Model uncertainty, the data assimilation ignores the observations. In this case, the “posteriors” are the prior (the model predictions are favored). If model uncertainty &gt; observation uncertainty, the posterior follows the data and ignores the model predictions when the predictions are different from the observations. In this case, the data distribution (likelihood) is the posterior. Evenly balanced uncertainty will result in posteriors that are a blend of the model ensemble distribution and data distribution.\nAs another way to view the balance of uncertainty, you are only able to use the set of particle ensemble members (priors) for resampling (or re-weighting) to generate the posteriors. Therefore, it is important to consider how the spread in your ensemble is generated. If your ensemble lacks any particles near the data then it can’t sample particles near the data. Therefore, if it is critical for your predictions to follow the data, then you need to generate more spread in your particle ensemble so that there are at least a few particles that are near observations.",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#reading",
    "href": "particle-filter.html#reading",
    "title": "15  Data assimilation using the particle filter",
    "section": "15.5 Reading",
    "text": "15.5 Reading",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "particle-filter.html#problem-set",
    "href": "particle-filter.html#problem-set",
    "title": "15  Data assimilation using the particle filter",
    "section": "15.6 Problem set",
    "text": "15.6 Problem set\nThe problem set is located in particle_filter_problem_set.qmd in https://github.com/frec-5174/book-problem-sets. You can fork the repository or copy the code into your quarto document",
    "crumbs": [
      "Advanced forecasting techniques",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data assimilation using the particle filter</span>"
    ]
  },
  {
    "objectID": "process-model.html",
    "href": "process-model.html",
    "title": "16  Process model",
    "section": "",
    "text": "16.1 Overview of model\nWe use a simple representation of a forest ecosystem that simulates carbon using three pools (leaves, wood, and soil). The model in Figure 16.1 is a 0-D model (no horizontal and vertical dimensions) that runs at a 1-day timestep. It models carbon using the units of MgC/ha.\nPhotosynthesis (gross primary productivity; gpp) is modeled using a light-use efficiency (LUE) approach. The goal of an LUE model is to convert light energy into carbon. First, Photosynthetically Active Radiation (PAR) is converted to absorbed PAR. Since more PAR is absorbed when more leaves are present, the absorbed PAR increases with LAI. However, due to self-shading, there is a diminishing return on PAR absorbed as LAI increases. The function used looks like the following:\nThe absorbed PAR is multiplied by a light-use efficiency parameter (alpha) that converts the absorbed PAR to carbon. LAI is determined by multiplying leaf carbon by the specific leaf area parameter (SLA).\nAutotrophic respiration (ra) is modeled as a constant proportion of GPP using the Ra_frac parameter. Net primary productivity (npp) is the carbon that is not used for autotrophic respiration (gpp - ra).\nNPP is allocated to the two vegetation carbon states (wood and leaves) based on a constant proportion parameter. leaf_frac is the proportion that is allocated to leaves and 1-leaf_frac is the allocation to wood.\nLitterfall occurs between the starting day of the year (litterfall_start) and the ending day of the year (litterfall_start + litterfall_length). The annual proportion of leaves (litterfall_rate) that are dropped is removed during this period.\nTree mortality is modeled as a constant proportion of wood (mortality) that is lost each day.\nHeterotrophic respiration is modeled as a constant proportion of soil carbon (Rbasal) that is adjusted by the temperature using a Q10 function that is governed by a Q10 parameter (Q10). The Q10 function represents how much respiration increases per 10-degree C increase in temperature. A Q10 parameter (Q10) of 2 corresponds to a doubling of respiration per 10-degree C increase in temperature. The Q10 function requires a base temperature (here, 20 degrees C) where the function evaluates to 1 and the represent rate equals the rate specified by Rbasal. The Q10 function looks like the following (using a Q10 of 2):\nThe change in leaves at each time step is modeled as leaf allocation - litterfall. The change in wood in each timestep is modeled as wood allocation - mortality. The change in soil carbon in each timestep is modeled as litterfall + mortality - heterotrophic respiration.\nFinally, net ecosystem exchange (nee) is modeled as ra + rh - gpp with negative values corresponding to a net gain in carbon by the ecosystem.\nThe model requires inputs of photosynthetic active radiation (PAR), air temperature (temp), and day-of-year (doy).",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#overview-of-model",
    "href": "process-model.html#overview-of-model",
    "title": "16  Process model",
    "section": "",
    "text": "Figure 16.1: Conceptual diagram of the forest model",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#uncertainty",
    "href": "process-model.html#uncertainty",
    "title": "16  Process model",
    "section": "16.2 Uncertainty",
    "text": "16.2 Uncertainty\nUncertainty can be represented in the model through its use of multiple ensemble members and the addition of random noise to each model state at each time step. The ensemble members can differ in their initial starting point, their parameters, and their meteorological inputs. The ensembles members are represented in the model code using the vector capacities in R. For example states[, 1] represents all ensemble members (the rows of the state matrix) for state number 3 (the columns of the state matrix). A similar concept applies to the drivers (inputs[, 1]). The parameters (e.g. parms$alpha) are vectors with one value for each ensemble member.\nNormally distributed random noise can be added to each state at each time step to represent “process uncertainty” (e.g., uncertainty due to the capacity of the model structure to reproduce the system). A unique value is drawn for each ensemble member following\nrnorm(ens_members, states[, 1] , parms$sigma.leaf)\nwhere the mean of the normal distribution is the predicted value for the state at that time step and the standard deviation (e.g., parms$sigma.leaf) controls how much noise is added. parms$sigma.leaf, parms$sigma.wood, and parms$sigma.soil are parameters that are set by the user or estimated from a model calibration. Setting the sigma parameters to 0 removes the random noise (as done below). See Chapter 17 for more details on how uncertainty is represented and propagated through time in the model.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#solving-the-model",
    "href": "process-model.html#solving-the-model",
    "title": "16  Process model",
    "section": "16.3 Solving the model",
    "text": "16.3 Solving the model\nAt each time step, the processes that result in a change in the model states are calculated (i.e., photosynthesis, mortality, respiration, etc.). The process rates are then combined to determine the net rate of change (i.e., the derivative) for each model state.\n  dleaves_dt &lt;- leaf_alloc  - litterfall\n  dwood_dt &lt;- wood_alloc  - mortality\n  dSOM_dt &lt;- litterfall + mortality - rh\nThe net rate of change for each time step is added to the value of the state at the previous time step to calculate the update state.\n  states[, 1] &lt;- states[, 1] + dleaves_dt\n  states[, 2] &lt;- states[, 2] + dwood_dt\n  states[, 3] &lt;- states[, 3] + dSOM_dt\nThis process is repeated at each time step to create a time series of model output. This method is called the Euler method for solving ordinary differential equations and assumes a constant change in time (dt = 1 day) at each time step.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#model-as-a-function",
    "href": "process-model.html#model-as-a-function",
    "title": "16  Process model",
    "section": "16.4 Model as a function",
    "text": "16.4 Model as a function\nThe code below describes the model as an R function. The function predicts one-time step at a time. The same function is found in the R/forecast_model.R script in the book repository. The inputs to the function are:\n\nt: the index of the current model time-step\nstates: a matrix of the states (rows = ensemble members, columns = states)\nparms: a data frame of parameters (rows = ensemble members, columns = parameters)\ninputs: a matrix of drivers (rows = ensemble members, columns = drivers)\n\n\nforest_model &lt;- function(t, states, parms, inputs){\n  \n  ens_members &lt;- nrow(states)\n  \n  inputs_temp &lt;- inputs[, 1]\n  inputs_PAR &lt;- inputs[, 2]\n  inputs_doy &lt;- inputs[, 3]\n  \n  # Unit Conversion: umol/m2/sec to Mg/ha/timestep\n  k &lt;- 1e-6 * 12 * 1e-6 * 10000 * 86400 #mol/umol*gC/mol*Mg/g*m2/ha*sec/timestep\n\n  # Convert leaf carbon to leaf area index\n  lai &lt;- states[, 1] * parms$SLA * 0.1  #0.1 is conversion from Mg/ha to kg/m2\n  \n  # photosynthesis\n  #Calculate gross primary productivity as a function of LAI and PAR (convert to daily)\n  ## pmax ensures GPP never goes negative\n  gpp &lt;- pmax(0, k * parms$alpha * (1 - exp(-0.5 * lai)) * inputs_PAR)\n\n  ## Autotropic respiration & allocation to leaves and wood\n  ra &lt;- gpp * parms$Ra_frac\n  npp &lt;- gpp - ra\n  \n  leaf_alloc &lt;- npp * parms$leaf_frac\n  wood_alloc &lt;- npp * (1 - parms$leaf_frac)\n\n  # Calculate soil respiration using a base rate and a Q10 temperature modifier \n  #(more soil = more respiration, hotter = more respiration)\n  ## pmax ensures SOM never goes negative\n  rh &lt;- pmax(k * parms$Rbasal * states[, 3] * parms$Q10 ^ (inputs_temp / 10), 0) \n\n  ## turnover\n\n  #calculate the daily rate of leaf drop\n  litterfall &lt;- states[ , 1] * (parms$litterfall_rate * (365/ (params$litterfall_length)))\n  #Not leaf drop if outside the day of the year window\n  litterfall[!(inputs_doy &gt; params$litterfall_start & \n              inputs_doy[1] &lt; (params$litterfall_start + params$litterfall_length))] &lt;- 0.0\n  \n  #kill trees\n  mortality &lt;- states[ , 2] * parms$mortality\n  \n  #Change in states\n  dleaves_dt &lt;- leaf_alloc  - litterfall\n  dwood_dt &lt;- wood_alloc  - mortality\n  dSOM_dt &lt;- litterfall + mortality - rh\n  \n  #Update states by adding the change\n  states[, 1] &lt;- states[, 1] + dleaves_dt\n  states[, 2] &lt;- states[, 2] + dwood_dt\n  states[, 3] &lt;- states[, 3] + dSOM_dt\n  \n  ## Add normally distributed random noise to states\n  ## pmax ensures states never go negative\n  states[, 1] &lt;- pmax(rnorm(ens_members, states[, 1], parms$sigma.leaf), 0)\n  states[, 2] &lt;- pmax(rnorm(ens_members, states[, 2], parms$sigma.wood), 0)\n  states[, 3] &lt;- pmax(rnorm(ens_members, states[, 3], parms$sigma.soil), 0)\n  \n  #Derived variables (LAI and net ecosystem exchange)\n  lai &lt;- states[, 1]  * parms$SLA * 0.1\n  nee &lt;- ra + rh - gpp\n  \n  return(cbind(state1 = states[, 1],\n              state2 = states[, 2],\n              state3 = states[, 3],\n              lai = lai,\n               gpp = gpp ,\n               nee = nee,\n               ra =  ra,\n               npp_w = wood_alloc,\n               npp_l = leaf_alloc,\n               rh = rh,\n               litterfall = litterfall,\n               mortality = mortality))\n}",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#set-up-model-run",
    "href": "process-model.html#set-up-model-run",
    "title": "16  Process model",
    "section": "16.5 Set up model run",
    "text": "16.5 Set up model run\n\n16.5.1 Time frame, site ID, and number of ensembles\nIn this chapter, we are running a deterministic simulation (no uncertainty) so we only have one ensemble member. The simulation is for a single NEON site (Ordway-Swisher Biological Station; OSBS), which is a longleaf pine ecosystem located in Florida, USA. The simulation will run from 2020-09-30 to two days ago.\n\nsite &lt;- \"OSBS\"\nens_members &lt;- 1\nsim_dates &lt;- seq(as_date(\"2020-09-30\"), Sys.Date() - lubridate::days(2), by = \"1 day\")\n\n\n\n16.5.2 Set drivers\nI have provided a custom function to access the weather drivers. You can learn more about the function in Chapter 24. My choice to provide a custom function here rather than writing out the complete code is intentional, to avoid unnecessary distraction from the focus on the process model.\n\ninputs &lt;- get_historical_met(site = site, sim_dates, use_mean = TRUE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\nHere is a plot of the meteorological input data.\n\n\n\n\n\n\n\n\n\n\n\n16.5.3 Set parameters\nThe model has 10 process parameters and 3 parameters representing the process uncertainty. Each of the parameters is set below to reasonable values. The process uncertainty standard deviations are set to 0.0 since our simulation is deterministic.\n\n#Set parameters\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(60, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) # leaf carbon process uncertainty\nparams$sigma.wood &lt;- rep(0.0, ens_members) # wood carbon process uncertainty\nparams$sigma.soil &lt;- rep(0.0, ens_members) #soil carbon process uncertainty\nparams &lt;- as.data.frame(params)\n\n\n\n16.5.4 Set initial condition\nNext, we set the starting point of the simulation. All three states need a starting point. To do this, we create a three-dimensional array to hold the model output, with the dimensions corresponding to 1) the number of simulated time steps; 2) the number of ensemble members (which in our case is 1 because this simulation is deterministic); and 3) the number of output variables. We then populate the first time step in the output array with the initial condition for each state.\n\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) \n#12 is the number of output variables (states + fluxes + derived variables)\noutput[1, , 1] &lt;- 5 # air temperature; degrees C\noutput[1, , 2] &lt;- 140 # PAR; umol/m2/sec\noutput[1, , 3] &lt;- 140 # day of year",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#run-model",
    "href": "process-model.html#run-model",
    "title": "16  Process model",
    "section": "16.6 Run model",
    "text": "16.6 Run model\nLoop over the sim_dates vector that includes the dates of the simulation. The loop starts on the index of 2 because the index of 1 was set as the initial conditions above. The forest_model function is the process model above. At each time step, the output is saved to the output array.\n\nfor(t in 2:length(sim_dates)){\n  output[t, , ]  &lt;- forest_model(t, \n                                 states = matrix(output[t-1 , , 1:3], nrow = ens_members),\n                                 parms = params,\n                                 inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#plot-output",
    "href": "process-model.html#plot-output",
    "title": "16  Process model",
    "section": "16.7 Plot output",
    "text": "16.7 Plot output\nThe output_to_df converts the output into a data frame and is located in the R/helpers.R script in the book repository. The sim_name is the name of your simulation.\n\noutput_df &lt;- output_to_df(output, sim_dates, sim_name = \"baseline\")\n\nFigure 16.2 is a plot of the output data frame.\n\noutput_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_point(aes(y = prediction, group = ensemble)) +\n  facet_wrap(~variable, scale = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 16.2: Output without uncertainty from the process model",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model.html#problem-set",
    "href": "process-model.html#problem-set",
    "title": "16  Process model",
    "section": "16.8 Problem set",
    "text": "16.8 Problem set\nUse the visualization of the simulation above to answer the following questions:\n\nWhich carbon state is changing the most over the simulation (wood or som)?\nWhy does wood carbon increase in the summer and decrease over the winter?\nIs carbon stored or released by the ecosystem when NEE is negative? What time of year is NEE positive?\nAll models are simplifications of the system: What important ecosystem processes are missing from this forest ecosystem model?",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html",
    "href": "process-model-prop-uncertainty.html",
    "title": "17  Propagating uncertainty in the process model",
    "section": "",
    "text": "17.1 Setting up simulations\nSet simulation dates.\nsim_dates &lt;- seq(as_date(\"2023-11-15\"),length.out  = 34, by = \"1 day\")\nSet site identifier.\nsite &lt;- \"OSBS\"",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html#setting-up-simulations",
    "href": "process-model-prop-uncertainty.html#setting-up-simulations",
    "title": "17  Propagating uncertainty in the process model",
    "section": "",
    "text": "17.1.1 Baseline parameters\nThese are the parameters that will be used for all the simulations except for the simulation where parameter uncertainty is propagated. Note that now, we are setting the number of ensemble members (ens_members) to 100 instead of 1 as we did in the previous chapter. This requires us to create a vector of values for each parameter so that each of the 100 ensemble members can be assigned parameter values.\n\nens_members &lt;- 100\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(70, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.wood &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n\n\n17.1.2 Baseline initial conditions\nThese are the initial conditions that will be used for all the simulations except for the simulation where initial conditions uncertainty is propagated. Note that we also create our output array here, as described in Chapter 13.\n\n#Set initial conditions\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- 5 # air temperature; degrees C\noutput[1, , 2] &lt;- 140 # PAR; umol/m2/sec\noutput[1, , 3] &lt;- 140 # day of year\n\n\n\n17.1.3 Baseline drivers\nThese are the driver data that will be used for all the simulations except for the simulation where driver data uncertainty is propagated. Here, we use the mean of the weather forecast ensemble.\n\ninputs &lt;- get_forecast_met(site = site, sim_dates, use_mean = TRUE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html#sec-prop-unc-pars",
    "href": "process-model-prop-uncertainty.html#sec-prop-unc-pars",
    "title": "17  Propagating uncertainty in the process model",
    "section": "17.2 Parameter uncertainty",
    "text": "17.2 Parameter uncertainty\nOur model has 13 parameters. Each of them requires a value that is likely not known with perfect certainty. Representing parameter uncertainty involves replacing the single value for each parameter with a distribution. The distribution can be from literature reviews, a best guess, or the outcome of a calibration exercise. If the calibration exercise is Bayesian (see Chapter 14), the distribution of the parameter before calibration can be referred to as a prior, and after calibration as a posterior. Sampling from the parameter distribution provides values for the parameters that are assigned to each ensemble member.\nIn many cases, a sensitivity analysis can be used to determine which parameters to focus on for uncertainty estimation. Briefly, a sensitivity analysis runs a model many times using different combinations of parameter values to identify the parameters for which changes in value lead to large changes in model predictions. If a model is not particularly sensitive to a parameter (i.e., changes in that parameter value do not strongly affect model predictions), then the overall prediction uncertainty is less likely to be strongly influenced by the uncertainty in that parameter. In practice, the values for less sensitive parameters are often held at a single value to avoid unnecessary computation. Other parameters are so well known that they are also held at a single value (e.g., the gravitation constant).\nIn this example, I focus only on propagating the uncertainty associated with one parameter (alpha) that represents the light-use efficiency of photosynthesis. All other parameters are held at their baseline values.\n\nnew_params &lt;- params\nnew_params$alpha &lt;- rnorm(ens_members, params$alpha, sd = 0.005)\n\nThis results in alpha having the following distribution Figure 17.1:\n\n\n\n\n\n\n\n\nFigure 17.1: Histogram showing the distribution of the parameter alpha\n\n\n\n\n\nNow, we use the new_params as the parameters in the simulation.\n\nfor(t in 2:length(sim_dates)){\n\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = new_params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\nparameter_df &lt;- output_to_df(output, sim_dates, sim_name = \"parameter_unc\")\n\nFigure 17.2 shows the forecast that only includes parameter uncertainty.\n\nparameter_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 17.2: Forecast with parameter uncertainty",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html#process-uncertainty",
    "href": "process-model-prop-uncertainty.html#process-uncertainty",
    "title": "17  Propagating uncertainty in the process model",
    "section": "17.3 Process uncertainty",
    "text": "17.3 Process uncertainty\nProcess uncertainty arises because all models are simplifications of reality. We can use random noise to capture the dynamics that are missing from the model. The random noise is added to each state at the end of the model time step. The random noise is normally distributed with a mean equal to the model prediction for that time step and the standard deviation equal to the parameters sigma.leaf (or leaves), sigma.wood (for wood), and sigma.soil (SOM). The result is a random walk  that is guided by the mean prediction of the process model.\nProcess uncertainty can be removed by setting the standard deviations equal to 0. Here we add in process uncertainty by setting the standard deviation to a non-zero value. The standard deviations (i.e., the values of sigma.leaf, sigma.wood, and sigma.soil) can be determined using state-space calibration of the ecosystem model. You can learn more about state-space modeling in Dietze (2017)\n\nnew_params &lt;- params\nnew_params$sigma.leaf &lt;- rep(0.1, ens_members)\nnew_params$sigma.wood &lt;- rep(1, ens_members) #0.01 ## wood biomass\nnew_params$sigma.soil &lt;- rep(1, ens_members)# 0.01\n\nAs an example, Figure 17.3 shows the distribution in the noise that is added to the leaf state at each time step.\n\n\n\n\n\n\n\n\nFigure 17.3: Histogram of the distribution of process uncertainty added to leaf carbon\n\n\n\n\n\nNow, we run the model with process uncertainty.\n\nfor(t in 2:length(sim_dates)){\n\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = new_params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\nprocess_df &lt;- output_to_df(output, sim_dates, sim_name = \"process_unc\")\n\nFigure 17.4 shows the forecast that only includes process uncertainty.\n\nprocess_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\")\n\n\n\n\n\n\n\nFigure 17.4: Forecast with process uncertainty",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html#initial-conditions-uncertainty",
    "href": "process-model-prop-uncertainty.html#initial-conditions-uncertainty",
    "title": "17  Propagating uncertainty in the process model",
    "section": "17.4 Initial conditions uncertainty",
    "text": "17.4 Initial conditions uncertainty\nInitial conditions uncertainty is the spread in the model states at the first time-step of a forecast. This spread would be due to a lack of measurements (and thus no direct knowledge of the state) or uncertainty in measurements (and thus a spread in the state values because we can’t perfectly observe them). Here we represent initial condition uncertainty by generating a normal distribution with a mean equal to the observed value (or our best guess) and a standard deviation that represents measurement uncertainty. We update the initial starting point in the forecast with this distribution.\n\n#Set initial conditions\nnew_output &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\nnew_output[1, , 1] &lt;- rnorm(ens_members, 5, 0.5)\nnew_output[1, , 2] &lt;- rnorm(ens_members, 140, 10)\nnew_output[1, , 3] &lt;- rnorm(ens_members, 140, 20)\n\nAs an example, Figure 17.5 shows the distribution in the noise that is added the initial leaf state.\n\n\n\n\n\n\n\n\nFigure 17.5: Distribution of initial condition uncertainty for leaf carbon (Mg/ha)\n\n\n\n\n\nNow, we run the model with initial conditions uncertainty.\n\nfor(t in 2:length(sim_dates)){\n\n  new_output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(new_output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = params, \n                               inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\ninitial_conditions_df &lt;- output_to_df(new_output, sim_dates, sim_name = \"initial_unc\")\n\nFigure 17.6 shows the forecast that only includes initial conditions uncertainty.\n\ninitial_conditions_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\")  +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 17.6: Forecast with initial conditions uncertainty",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html#driver-uncertainty",
    "href": "process-model-prop-uncertainty.html#driver-uncertainty",
    "title": "17  Propagating uncertainty in the process model",
    "section": "17.5 Driver uncertainty",
    "text": "17.5 Driver uncertainty\nThe uncertainty in the weather forecasts comes directly from the 31 ensemble members provided by the NOAA Global Ensemble Forecasting System (GEFS). The ensemble is generated by slightly changing (perturbing) the initial states in the weather model before starting the forecast. Due to the chaotic nature of the atmosphere, these small differences get amplified over time, resulting in a spread that increases further in the future.\n\nnew_inputs &lt;- get_forecast_met(site = site, sim_dates, use_mean = FALSE)\nnew_inputs_ensemble &lt;- assign_met_ensembles(new_inputs, ens_members)\n\nAs an example, Figure 17.7 shows 31 ensemble members from a single 35-day forecast generated by NOAA GEFS.\n\n\n\n\n\n\n\n\nFigure 17.7: 35-day ahead forecasts from NOAA GEFS of the two variables used by the process model.\n\n\n\n\n\nNow, we run the model with driver uncertainty.\n\nfor(t in 2:length(sim_dates)){\n\n  output[t, , ]  &lt;- forest_model(t, \n                               states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                               parms = params, \n                               inputs = matrix(new_inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\ndrivers_df &lt;- output_to_df(output, sim_dates, sim_name = \"driver_unc\")\n\nFigure 17.8 shows the forecast that only includes driver uncertainty.\n\ndrivers_df |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 17.8: Forecast with driver uncertainty",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "process-model-prop-uncertainty.html#problem-set",
    "href": "process-model-prop-uncertainty.html#problem-set",
    "title": "17  Propagating uncertainty in the process model",
    "section": "17.6 Problem set",
    "text": "17.6 Problem set\nThe problem set is located in process-model-uncert.qmd at https://github.com/frec-5174/book-problem-sets. You can fork the repository or copy the code into your quarto document\n\n\n\n\nDietze, Michael C. 2017. Ecological Forecasting. Princeton: Princeton University Press.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Propagating uncertainty in the process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html",
    "href": "neon-data.html",
    "title": "18  Data to constrain process model",
    "section": "",
    "text": "18.1 NEON Project",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#neon-terrestrial-sites",
    "href": "neon-data.html#neon-terrestrial-sites",
    "title": "18  Data to constrain process model",
    "section": "18.2 NEON Terrestrial sites",
    "text": "18.2 NEON Terrestrial sites",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#download-data",
    "href": "neon-data.html#download-data",
    "title": "18  Data to constrain process model",
    "section": "18.3 Download data",
    "text": "18.3 Download data\nFirst, we define the site ID. The four letter site code denotes individual NEON sites. You can learn more about NEON sites here: https://www.neonscience.org/field-sites/explore-field-sites.\nThe elevation, latitude, and longitude are needed to convert the tree diameter measurements to biomass and are found on the NEON page describing the site.\n\nsite &lt;- \"OSBS\"\nelevation &lt;- 46 \nlatitude &lt;- 29.689282 \nlongitude &lt;- -81.993431",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#wood-carbon",
    "href": "neon-data.html#wood-carbon",
    "title": "18  Data to constrain process model",
    "section": "18.4 Wood carbon",
    "text": "18.4 Wood carbon\nIn this section, we will be calculating carbon in live and dead trees at a NEON site. The carbon in live trees represents the wood carbon stock in Figure 16.1 and the dead trees represent a component of the soil organic matter stock in Figure 16.1. In the end, we will have a site-level mean carbon stock in live trees and dead trees for each year that was sampled from the plots that represent the ecosystem under the flux tower (e.g., tower plots). We use the tower plots so that they correspond to the same ecosystem as the NEON nee data.\nWe will select the key variables in each table (thus only downloading those variables).\nThe code below reads the data directly from NEON’s cloud storage.\n\n## Mapping and tagging table\n\nmap_tag_table &lt;- neon_cloud(\"mappingandtagging\",\n                            product = \"DP1.10098.001\",\n                            site = site) |&gt; \n  select(individualID, scientificName) |&gt; \n  collect() |&gt; \n  group_by(individualID) |&gt; \n  slice(1) |&gt; #This is needed because some individuals change species IDs\n  ungroup()\n\n## Individual table\nind_table &lt;- neon_cloud(\"apparentindividual\",\n                        product = \"DP1.10098.001\",\n                        site = site) |&gt;\n  select(individualID, eventID, plotID, date, stemDiameter,plantStatus, measurementHeight) |&gt; \n  distinct() |&gt; \n  collect()\n\n## Plot table\nplot_table &lt;- neon_cloud(\"perplotperyear\",\n                        product = \"DP1.10098.001\",\n                        site = site) |&gt; \n  select(plotID,totalSampledAreaTrees,plotType) |&gt;  \n  distinct(plotID, .keep_all = TRUE) |&gt; \n  collect() \n\nThe equations that convert diameter at breast height (DBH), where breast height is defined at 130 cm above the base of the tree, differ by species and location. Therefore the scientific name (both genus and species components) is needed. The species names in the mapping and tagging table need to be separated into the genus and species so that we can calculate the biomass using an R function that expects them to be separate.\n\ngenus_species &lt;- unlist(str_split_fixed(map_tag_table$scientificName, \" \", 3))\n\nmap_tag_table &lt;- map_tag_table |&gt; \n  mutate(GENUS = genus_species[,1], \n         SPECIES = genus_species[,2]) \n\nNow we will join the tables by the key variables to build our dataset for the site.\n\ncombined_table &lt;- left_join(ind_table, map_tag_table, by = \"individualID\") |&gt; \n  arrange(plotID,individualID)\n\ncombined_table &lt;- inner_join(combined_table, plot_table, by = \"plotID\") |&gt; \n  arrange(individualID)\n\ncombined_table_dbh &lt;- combined_table |&gt; \n  filter(measurementHeight == 130,\n         !is.na(stemDiameter))\n\n\n18.4.1 Calculate carbon in live trees\nTidy up the individual tree data to include only live trees from the tower plots. Also, create a variable that is the year of the sample date. We will filter the data based on measurement heights of 130 cm to only include data that had diameter at breast height (dbh) measurements.\n\ncombined_table_live_tower &lt;- combined_table_dbh |&gt;  \n  filter(str_detect(plantStatus,\"Live\"),\n         plotType == \"tower\",\n         measurementHeight == 130) |&gt; \n  mutate(stemDiameter = as.numeric(stemDiameter))\n\nTo calculate the biomass of each tree in the table, we will use the get_biomass function from the allodb package (Gonzalex-Akre https://doi.org/10.1111/2041-210X.13756), which is a package that converts DBH measurements to tree biomass estimates. This function takes as arguments: dbh, genus, species, and coords. We have already extracted genera and species and filtered them to dbh measurements. (note allodb is not on CRAN but can be downloaded using remotes::install_github(\"ropensci/allodb\"))\nIn this next section, as well as a future one where we calculate dead tree carbon, we are going to make a simplifying assumption. We will assume that the below-ground biomass of a tree is some fixed proportion of its above-ground biomass. In our analysis, we will assume this value is \\(0.3\\) (ag_bg_propr), but it is a parameter that can be changed. We also assume that carbon is \\(0.5\\) of biomass.\nThe get_biomass function is within the allodb package and returns the biomass of each tree in units of kg.\n\nlibrary(allodb)\n\nag_bg_prop &lt;- 0.3\n\ntree_live_carbon &lt;- combined_table_live_tower |&gt;\n  mutate(ag_tree_kg = get_biomass(dbh = combined_table_live_tower$stemDiameter,\n                                  genus = combined_table_live_tower$GENUS,\n                                  species = combined_table_live_tower$SPECIES,\n                                  coords = c(longitude, latitude)),\n  bg_tree_kg = ag_tree_kg * ag_bg_prop, ## assumption about ag to bg biomass\n  tree_kgC = (ag_tree_kg + bg_tree_kg) * 0.5) ## convert biomass to carbon\n\nCalculate the plot level biomass by summing up the tree biomass in a plot and dividing by the area of plot.\n\nmeasurement_dates &lt;- tree_live_carbon |&gt; \n  summarise(measure_date = max(date), .by = eventID)\n\n plot_live_carbon &lt;-  tree_live_carbon |&gt; \n   left_join(measurement_dates, by = \"eventID\") |&gt; \n    mutate(treeC_kgCm2 = (tree_kgC)/(totalSampledAreaTrees)) |&gt; \n    summarise(plot_kgCm2 = sum(treeC_kgCm2, na.rm = TRUE), .by = c(\"plotID\", \"measure_date\"))\n\nFigure 18.1 plot level carbon in living trees\n\nggplot(plot_live_carbon, aes(x = measure_date, y = plot_kgCm2, color = plotID)) + \n  geom_point() +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.1: Plot level carbon in living trees for the focal NEON site\n\n\n\n\n\nOnly a subset of plots are measured each year and we only want the plots that have annual measurements. This code determines the set of plots that are measured each year (a subset, n = 5), while all the other plots are measured every 5 years.\n\nlast_plots &lt;- plot_live_carbon |&gt; \n  filter(measure_date == max(measure_date)) |&gt; \n  pull(plotID)\n\nsite_live_carbon &lt;- plot_live_carbon |&gt; \n  filter(plotID %in% last_plots) |&gt; \n  pivot_wider(names_from = plotID, values_from = plot_kgCm2) |&gt; \n  na.omit() |&gt; \n  pivot_longer(-measure_date, names_to = \"plotID\", values_to = \"plot_kgCm2\") |&gt; \n    group_by(measure_date) |&gt;\n    summarize(mean_kgCperm2 = mean(plot_kgCm2, na.rm = TRUE),\n              sd_kgCperm2 = sd(plot_kgCm2))\n\nFigure 18.2 is the site-level carbon calculated by taking the mean only of the plots that were measured each year.\n\nggplot(site_live_carbon, aes(x = measure_date, y = mean_kgCperm2)) + \n  geom_point() + \n  geom_errorbar(aes(ymin=mean_kgCperm2-sd_kgCperm2, ymax=mean_kgCperm2+sd_kgCperm2), width=.2,\n                 position=position_dodge(0.05)) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.2: Site level carbon in living trees for the focal NEON site\n\n\n\n\n\n\n\n18.4.2 Calculate carbon in dead trees\nWe will now use the allodb package to extract the carbon in dead trees. This is exactly like the steps above except for using the trees with a dead status.\n\ncombined_table_dead_tower &lt;- combined_table_dbh |&gt; \n  filter(str_detect(\"Standing dead\",plantStatus),\n         plotType == \"tower\") |&gt; \n  mutate(stemDiameter = as.numeric(stemDiameter))\n\nCalculate the biomass of each tree in the table. This assumes that standing dead trees have the same carbon as a live tree (which is an incorrect assumption).\n\ntree_dead_carbon &lt;- combined_table_dead_tower |&gt;\n    mutate(ag_tree_kg = get_biomass(\n          dbh = combined_table_dead_tower$stemDiameter,\n          genus = combined_table_dead_tower$GENUS,\n          species = combined_table_dead_tower$SPECIES,\n          coords = c(longitude, latitude)\n          ),\n         bg_tree_kg = ag_tree_kg * ag_bg_prop,\n         tree_kgC = (ag_tree_kg + bg_tree_kg) * 0.5)\n\nCalculate the plot level carbon.\n\nmeasurement_dates &lt;- tree_dead_carbon |&gt; \n  summarise(measure_date = max(date), .by = eventID)\n\n plot_dead_carbon &lt;-  tree_dead_carbon |&gt; \n   left_join(measurement_dates, by = \"eventID\") |&gt; \n    mutate(treeC_kgCm2 = (tree_kgC)/(totalSampledAreaTrees)) |&gt; \n    summarise(plot_kgCm2 = sum(treeC_kgCm2, na.rm = TRUE), .by = c(\"plotID\", \"measure_date\"))\n\nFigure 18.3 plot level carbon in dead trees.\n\nggplot(plot_dead_carbon, aes(x = measure_date, y = plot_kgCm2, color = plotID)) + \n  geom_point() +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.3: Plot level carbon in dead trees for the focal NEON site\n\n\n\n\n\nCalculate site level carbon in dead trees from the plots measured each year.\n\n site_dead_carbon &lt;- plot_dead_carbon |&gt;\n    filter(plotID %in% last_plots) |&gt; \n    group_by(measure_date) |&gt;\n    summarize(mean_kgCperm2 = mean(plot_kgCm2, na.rm = TRUE),\n              sd_kgCperm2 = sd(plot_kgCm2))\n\nFigure 18.4 is the site-level carbon.\n\nggplot(site_dead_carbon, aes(x = measure_date, y = mean_kgCperm2)) + \n  geom_point() +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.4: Site level carbon in dead trees for the focal NEON site",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-trees-on-the-ground-coarse-woody-debris",
    "href": "neon-data.html#calculate-carbon-in-trees-on-the-ground-coarse-woody-debris",
    "title": "18  Data to constrain process model",
    "section": "18.5 Calculate carbon in trees on the ground (coarse woody debris)",
    "text": "18.5 Calculate carbon in trees on the ground (coarse woody debris)\nWhile the code above calculates the carbon in standing dead trees, it misses the carbon in dead trees that are no longer standing (called coarse woody debris). The coarse woody debris is another component of SOM in our simple forest model.\nThe data needed to calculate carbon in trees that are lying on the ground are in two NEON data products.\n\ncdw_density &lt;- neon_cloud(\"cdw_densitydisk\",\n                        product = \"DP1.10014.001\",\n                        site = site) |&gt; \n  collect()\n\nlog_table &lt;- neon_cloud(\"cdw_densitylog\",\n                        product = \"DP1.10014.001\",\n                        site = site,\n                        unify_schemas = TRUE) |&gt; \n  collect()\n \ncdw_tally &lt;- neon_cloud(\"cdw_fieldtally\",\n                        product = \"DP1.10010.001\",\n                        site = site) |&gt; \n  collect()\n\nWe will go through the same steps to calculate carbon in the coarse woody debris.\n\n## Filter by tower plot for log table\nlog_table_filter &lt;- log_table |&gt; \n  filter(plotType == \"tower\",\n         plotID %in% last_plots)\n\n## Filter by tower plot for cdw table\ncdw_tally &lt;- cdw_tally |&gt;\n  filter(plotType == 'tower',\n         plotID %in% last_plots)\n\n## create \nlog_table_filter$gcm3 &lt;- rep(NA, nrow(log_table_filter))\n\n## Set site specific volume factor\nsite_volume_factor &lt;- 8\n\nfor (i in 1:nrow(log_table_filter)){\n  ## Match log table sampleID to cdw density table sample ID\n  ind &lt;- which(cdw_density$sampleID == log_table_filter$sampleID[i])\n  ## Produce g/cm^3 by multiplying the bulk density of the disk by the site volume factor\n  log_table_filter$gcm3[i] &lt;- mean(cdw_density$bulkDensDisk[ind]) * site_volume_factor\n}\n\nyear_measurement &lt;- min(log_table_filter$yearBoutBegan)\n\n## Table of coarse wood\nsite_cwd_carbon &lt;- log_table_filter |&gt;\n  summarize(mean_kgCperm2 = .5 * sum(gcm3, na.rm = TRUE) * .1) |&gt; \n  mutate(year = year_measurement)",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-fine-roots",
    "href": "neon-data.html#calculate-carbon-in-fine-roots",
    "title": "18  Data to constrain process model",
    "section": "18.6 Calculate carbon in fine roots",
    "text": "18.6 Calculate carbon in fine roots\nWe lump fine root carbon into the wood stem stock in the simple forest model. Here we are going to calculate the carbon stored in fine roots using the root chemistry data product. We will calculate the carbon in both dead and alive roots. Though we are interested mostly in live roots, at the time of writing this, the 2021 NEON data for our site does not have rootStatus data available. Thus we will use historical data to compute an estimate of the ratio so that we don’t have to throw away perfectly good information.\n\n## root chemistry data product\nbbc_percore &lt;- neon_cloud(\"bbc_percore\",\n                          product = \"DP1.10067.001\",\n                          site = site) |&gt; \n  collect()\n\nrootmass &lt;- neon_cloud(\"bbc_rootmass\",\n                        product = \"DP1.10067.001\",\n                        site = site) |&gt; \n  collect()\n\n\nrootmass$year = year(rootmass$collectDate)\n\n## set variables for liveDryMass, deadDryMass, unkDryMass, area\nrootmass$liveDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$deadDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$unkDryMass &lt;- rep(0, nrow(rootmass))\nrootmass$area &lt;- rep(NA, nrow(rootmass))\n\nfor (i in 1:nrow(rootmass)){\n  ## match by sample ID\n  ind &lt;- which(bbc_percore$sampleID == rootmass$sampleID[i])\n  ## extract core sample area\n  rootmass$area[i] &lt;- bbc_percore$rootSampleArea[ind]\n  ## categorize mass as live, dead, or unknown\n  if (is.na(rootmass$rootStatus[i])){\n    rootmass$unkDryMass[i] &lt;- rootmass$dryMass[i]\n  } else if (rootmass$rootStatus[i] == 'live'){\n    rootmass$liveDryMass[i] &lt;- rootmass$dryMass[i]\n  } else if (rootmass$rootStatus[i] == 'dead'){\n    rootmass$deadDryMass[i] &lt;- rootmass$dryMass[i]\n  } else{\n    rootmass$unkDryMass[i] &lt;- rootmass$dryMass[i]\n  }\n}\n\n##\nsite_roots &lt;- rootmass |&gt;\n  ## Filter plotID to only our plots of interest\n  filter(plotID %in% last_plots) |&gt;\n  ## group by year\n  group_by(year) |&gt;\n  ## sum live, dead, unknown root masses. multiply by\n  ## .5 for conversion to kgC/m^2\n  summarize(mean_kgCperm2_live = .5*sum(liveDryMass/area, na.rm = TRUE)/1000,\n            mean_kgCperm2_dead = .5*sum(deadDryMass/area, na.rm = TRUE)/1000,\n            mean_kgCperm2_unk = .5*sum(unkDryMass/area, na.rm = TRUE)/1000,\n            year_total = sum(c(mean_kgCperm2_dead, mean_kgCperm2_live, mean_kgCperm2_unk)) / length(unique(plotID)),\n            med_date = median(collectDate)) |&gt; \n  rename(mean_kgCperm2 = year_total) |&gt; \n  select(year, mean_kgCperm2)",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#calculate-carbon-in-soils",
    "href": "neon-data.html#calculate-carbon-in-soils",
    "title": "18  Data to constrain process model",
    "section": "18.7 Calculate carbon in soils",
    "text": "18.7 Calculate carbon in soils\nThe video below provides an introduction to the science of soil carbon and methods for measuring it.\n\nSoil carbon data is contained in two NEON data products: one that describes the physical characteristics of the soil (depth and density) and another that describes the carbon concentration of the soil. Ultimately multiplying the density by the carbon concentration gives the total carbon.\n\n#Download bieogeochemistry soil data to get carbon concentration\n#data_product1 &lt;- \"DP1.00097.001\"\n#Download physical soil data to get the bulk density\n\nmgc_perbiogeosample &lt;- neon_cloud(\"mgp_perbiogeosample\",\n                        product = \"DP1.00096.001\",\n                        site = site) |&gt; \n  collect()\n\nmgp_perbulksample &lt;- neon_cloud(\"mgp_perbulksample\",\n                        product = \"DP1.00096.001\",\n                        site = site) |&gt; \n  collect()\n\nThis code pulls out the relevant columns from the data that were read in above.\n\nbulk_density &lt;- mgp_perbulksample |&gt; \n    filter(bulkDensSampleType == \"Regular\") |&gt; \n    select(horizonName,bulkDensExclCoarseFrag) \n\n  #gramsPerCubicCentimeter\nhorizon_carbon &lt;- mgc_perbiogeosample |&gt; \n    filter(biogeoSampleType == \"Regular\") |&gt; \n    select(horizonName,biogeoTopDepth,biogeoBottomDepth,carbonTot) \n\nyear &lt;- year(as_date(mgp_perbulksample$collectDate[1]))\n\nThe code below\n\njoins the bulk density table into the table with the carbon concentration\nDetermines the height of the horizon (biogeoBottomDepth - biogeoTopDepth) and converts to total mass of soil in the horizon using the bulk density.\nMultiply the carbon concentration (carbonTot) by the mass of soil (along with unit conversion) to get the soil carbon in kg C / m2.\n\n\n  #Unit notes\n  #bulkDensExclCoarseFrag = gramsPerCubicCentimeter\n  #carbonTot = gramsPerKilogram\n  \n  #Combine and calculate the carbon of each horizon\nhorizon_combined &lt;- inner_join(horizon_carbon,bulk_density, by = \"horizonName\") |&gt;\n    #Convert volume in g per cm3 to mass per area in g per cm2 by multiplying by layer thickness\n    mutate(horizon_soil_g_per_cm2 = (biogeoBottomDepth - biogeoTopDepth) * bulkDensExclCoarseFrag) |&gt; \n    #Units of carbon are g per Kg soil but we have bulk density in g per cm2 so convert Kg soil to g soil\n    mutate(CTot_g_per_g_soil = carbonTot*(1/1000),  #Units are g C per g soil\n           horizon_C_g_percm2 = CTot_g_per_g_soil*horizon_soil_g_per_cm2, #Units are g C per cm2\n           horizon_C_kg_per_m2 = horizon_C_g_percm2 * 10000 / 1000) |&gt; #Units are g C per m2\n    select(-CTot_g_per_g_soil,-horizon_C_g_percm2) |&gt;\n    arrange(biogeoTopDepth)\n\nThe soil carbon can be visualized by depth Figure 18.5.\n\nggplot(horizon_combined, map = aes(-biogeoTopDepth,horizon_C_kg_per_m2)) +\n  geom_line() +\n  geom_point() +\n  labs(y = \"Carbon\", x = \"Depth\", title = \"Soil carbon by depth\") +\n  coord_flip()  +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.5: Soil carbon by depth for the site from the megapit.\n\n\n\n\n\nTotal soil carbon is the sum across the depths.\n\nsite_soil_carbon &lt;- horizon_combined |&gt; \n    summarize(soilC_gC_m2 = sum(horizon_C_kg_per_m2))",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#combine",
    "href": "neon-data.html#combine",
    "title": "18  Data to constrain process model",
    "section": "18.8 Combine",
    "text": "18.8 Combine\nNext, we will combine our estimates of carbon in each component for visualization and to aggregate into the wood and som stocks below.\n\nsite_live_carbon &lt;- site_live_carbon |&gt; \n  mutate(variable = \"live_tree\") |&gt; \n  rename(datetime = measure_date) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_dead_carbon &lt;- site_dead_carbon |&gt; \n  mutate(variable = \"dead_trees\") |&gt; \n  rename(datetime = measure_date) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_cwd_carbon &lt;- site_cwd_carbon |&gt; \n  mutate(variable = \"down_wood\") |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_roots &lt;- site_roots |&gt; \n  mutate(variable = \"fine_roots\") |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\nsite_soil_carbon &lt;- site_soil_carbon |&gt; \n  mutate(variable = \"soil_carbon\") |&gt; \n  rename(mean_kgCperm2 = soilC_gC_m2) |&gt; \n  mutate(datetime = as_date(paste(year, \"01-01\"))) |&gt; \n  select(datetime, variable, mean_kgCperm2)\n\ntotal_carbon_components &lt;- bind_rows(site_live_carbon, site_dead_carbon, site_cwd_carbon, site_roots, site_soil_carbon)\n\nThe different pools of carbon can be plotted on the same figure to compare the magnitudes Figure 18.6.\n\ntotal_carbon_components |&gt; \n  ggplot(aes(x = datetime, y = mean_kgCperm2, color = variable)) + \n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.6: Site-level carbon stocks at the focal NEON site\n\n\n\n\n\nCombine pools of carbon to match the stocks used in our simple process model. - wood = live trees (stem and coarse roots) + fine roots - som = dead trees + down wood + soil carbon\n\ntotal_carbon_simple &lt;- total_carbon_components |&gt; \n  pivot_wider(names_from = variable, values_from = mean_kgCperm2) |&gt; \n  mutate(wood = live_tree + mean(fine_roots, na.rm = TRUE),\n         som = mean(dead_trees, na.rm = TRUE) + mean(down_wood, na.rm = TRUE) + mean(soil_carbon, na.rm = TRUE),\n         som = ifelse(datetime != min(datetime), NA, som)) |&gt; \n  select(datetime, wood, som) |&gt; \n  pivot_longer(-datetime, names_to = \"variable\", values_to = \"observation\")",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#modis-lai",
    "href": "neon-data.html#modis-lai",
    "title": "18  Data to constrain process model",
    "section": "18.9 MODIS LAI",
    "text": "18.9 MODIS LAI\nThe leaf area index can be used as a proxy for leaf carbon. The forest model converts leaf carbon into LAI using a leaf mass to area parameter. As a result, we can use the leaf area index (LAI) from the MODIS satellite sensor to constrain and evaluate LAI predictions. MODIS LAI product is an 8-day mean for a 500m grid cell.\n\nDownload the leaf area index for the focal NEON site using the MODISTools package.\n\nlai &lt;- MODISTools::mt_subset(product = \"MCD15A2H\",\n                  lat = latitude,\n                  lon =  longitude,\n                  band = c(\"Lai_500m\", \"FparLai_QC\"),\n                  start = as_date(min(total_carbon_simple$datetime)),\n                  end = Sys.Date(),\n                  site_name = site,\n                  progress = FALSE)\n\n\nlai_cleaned &lt;- lai |&gt; \n  mutate(scale = ifelse(band == \"FparLai_QC\", 1, scale),\n         scale = as.numeric(scale),\n         value = scale * value,\n         datetime = lubridate::as_date(calendar_date)) |&gt; \n  select(band, value, datetime) |&gt; \n  pivot_wider(names_from = band, values_from = value) |&gt; \n  filter(FparLai_QC == 0) |&gt; \n  rename(observation = Lai_500m) |&gt; \n  mutate(variable = \"lai\") |&gt; \n  select(datetime, variable, observation)\n\nFigure 18.7 is the LAI for the focal NEON site.\n\nlai_cleaned |&gt; \n  ggplot(aes(x = datetime, y = observation)) +\n  geom_point() +\n  geom_smooth(span = 0.12) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.7: MODIS LAI for the 500m grid cell that includes the flux tower",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#flux-data",
    "href": "neon-data.html#flux-data",
    "title": "18  Data to constrain process model",
    "section": "18.10 Flux data",
    "text": "18.10 Flux data\nNEE flux data is used to help constrain the net of photosynthesis and respiration in the simple forest model. It is already processed for use in the NEON Ecological Forecasting Challenge. Here we read in that data.\nLearn about flux data here:\n\n\nurl &lt;- \"https://sdsc.osn.xsede.org/bio230014-bucket01/challenges/targets/project_id=neon4cast/duration=P1D/terrestrial_daily-targets.csv.gz\"\n\nflux &lt;- read_csv(url, show_col_types = FALSE) |&gt; \n  filter(site_id %in% site, \n         variable == \"nee\") |&gt; \n  mutate(datetime = as_date(datetime)) |&gt; \n  select(datetime, variable, observation)\n\nFigure 18.8 is the daily mean NEE for the focal NEON site.\n\n\n\n\n\n\n\n\nFigure 18.8: Daily mean NEE from the flux tower at the focal NEON site",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "neon-data.html#combine-to-create-data-constraints",
    "href": "neon-data.html#combine-to-create-data-constraints",
    "title": "18  Data to constrain process model",
    "section": "18.11 Combine to create data constraints",
    "text": "18.11 Combine to create data constraints\nThe units of the carbon stocks and nee need to be converted to the units of the forest process model. The carbon stocks are converted from kgC/m2 to MgC/ha and nee is converted from gC/m2/day to MgC/ha/day.\n\nobs &lt;- total_carbon_simple |&gt; \n  bind_rows(lai_cleaned, flux) |&gt; \n  mutate(site_id = site) |&gt; \n  #convert from kgC/m2 to MgC/ha\n  mutate(observation = ifelse(variable %in% c(\"wood\", \"som\") , observation * 10, observation),\n         observation = ifelse(variable %in% c(\"nee\") , observation * 0.01, observation))\n\nThe combined data with the variable names converted to the names used in the forest process model Figure 18.9.\n\nobs |&gt; \n  ggplot(aes(x = datetime, y = observation)) + \n  geom_point() +\n  facet_wrap(~variable, scale = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 18.9: The data available to constrain the forest process model.\n\n\n\n\n\nSave the observations to a CSV file.\n\nwrite_csv(obs, \"data/site_carbon_data.csv\")\n\nNow, we have a complete, up-to-date carbon budget file that is stored in a format compatible with our simple forest process model. This will allow us to calibrate parameters, assimilate data, and evaluate forecasts. We will use this file in the subsequent chapters.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data to constrain process model</span>"
    ]
  },
  {
    "objectID": "bayesian-process-models.html",
    "href": "bayesian-process-models.html",
    "title": "19  Applying Bayesian calibration methods to a process model",
    "section": "",
    "text": "19.1 Running MCMC on the forest process model\nlibrary(tidyverse)\nlibrary(patchwork)\nsource(\"R/helpers.R\")\nsource(\"R/forest_model.R\")\nset.seed(100)\nsite &lt;- \"OSBS\"\nRead in observations\nobs &lt;- read_csv(\"data/site_carbon_data.csv\", show_col_types = FALSE)\nSet up dates of simulation, parameters, initial conditions, and meteorology inputs\nsim_dates &lt;- seq(as_date(\"2022-01-01\"), Sys.Date() - 2, by = \"1 day\")\n\nens_members &lt;- 1\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(250, ens_members)\nparams$litterfall_length&lt;- rep(60, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.wood &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n\nstate_init &lt;- rep(NA, 3)\n\nstate_init[1] &lt;- obs |&gt; \n  filter(datetime %in% sim_dates,\n         variable == \"lai\") |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  mutate(observation = observation / (mean(params$SLA) * 0.1)) |&gt; \n  pull(observation)\n\nstate_init[2] &lt;- obs |&gt; \n  filter(variable == \"wood\",\n         datetime %in% sim_dates) |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  pull(observation)\n\nstate_init[3] &lt;- obs |&gt; \n  filter(variable == \"som\") |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  pull(observation)\n\ninputs &lt;- get_historical_met(site = site, sim_dates, use_mean = TRUE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\nSet up MCMC configuration\n#Set MCMC Configuration\nnum_iter &lt;- 1500\nlog_likelihood_prior_current &lt;- -10000000000\naccept &lt;- 0\n\n\n#Initialize chain\nnum_pars &lt;- 3\njump_params &lt;- c(0.001, 0.0002, 1)\nfit_params &lt;- array(NA, dim = c(num_pars, num_iter))\nfit_params[1, 1] &lt;- params$alpha\nfit_params[2, 1] &lt;- params$Rbasal\nfit_params[3, 1] &lt;- params$litterfall_start\nprior_mean &lt;- c(0.029, 0.002, 200)\nprior_sd &lt;- c(0.005, 0.0005, 10)\nRun MCMC\nfor(iter in 2:num_iter){\n  \n  #Loop through parameter value\n  \n  for(j in 1:num_pars){\n    \n    proposed_pars &lt;- fit_params[, iter - 1]\n    proposed_pars[j] &lt;- rnorm(1, mean = fit_params[j, iter - 1], sd = jump_params[j])\n    \n    log_prior &lt;- dnorm(proposed_pars[1], mean = prior_mean[1], sd = prior_sd[1], log = TRUE) + \n      dnorm(proposed_pars[2], mean = prior_mean[2], sd = prior_sd[2], log = TRUE) +\n      dnorm(proposed_pars[3], mean = prior_mean[3], sd = prior_sd[3], log = TRUE)\n    \n    params$alpha  &lt;- proposed_pars[1]\n    params$Rbasal  &lt;- proposed_pars[2]\n    params$litterfall_start &lt;- proposed_pars[3]\n    \n    #Set initial conditions\n    output &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\n    output[1, , 1] &lt;- state_init[1]\n    output[1, , 2] &lt;- state_init[2]\n    output[1, , 3] &lt;- state_init[3]\n    \n    for(t in 2:length(sim_dates)){\n      output[t, , ]  &lt;- forest_model(t, \n                                     states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                                     parms = params, \n                                     inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n    }\n    \n    output_df &lt;- output_to_df(output, sim_dates, sim_name = \"fitting\")\n    \n    combined_output_obs &lt;- combine_model_obs(output_df, \n                                             obs,\n                                             variables = c(\"lai\", \"wood\", \"som\", \"nee\"), \n                                             sds = c(0.1, 1, 20, 0.01))\n    \n    log_likelihood &lt;- sum(dnorm(x =  combined_output_obs$observation, \n                                mean = combined_output_obs$prediction, \n                                sd = combined_output_obs$sds, log = TRUE))\n    \n    log_likelihood_prior_proposed &lt;- log_prior + log_likelihood\n    \n    z &lt;- exp(log_likelihood_prior_proposed - log_likelihood_prior_current)\n    \n    r &lt;- runif(1, min = 0, max = 1)\n    \n    if(z &gt;  r){\n      fit_params[j, iter] &lt;- proposed_pars[j]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_proposed\n      accept &lt;- accept + 1\n    }else{\n      fit_params[j, iter] &lt;- fit_params[j, iter - 1]\n      log_likelihood_prior_current &lt;- log_likelihood_prior_current #this calculation isn't necessary but is here to show you the logic\n    }\n  }\n}\nExamine acceptance rate (goal is 23-45%)\naccept / (num_iter * num_pars)\n\n[1] 0.3548889\nProcess MCMC chain by removing the first 500 iterations and pivoting to a long format\nnburn &lt;- 500\nparameter_MCMC &lt;- tibble(iter = nburn:num_iter,\n            alpha = fit_params[1, nburn:num_iter],\n            Rbasal = fit_params[2, nburn:num_iter],\n            litterfall_start = fit_params[3, nburn:num_iter]) %&gt;%\n  pivot_longer(-iter, values_to = \"value\", names_to = \"parameter\")\nFigure 19.1\np1 &lt;- ggplot(parameter_MCMC, aes(x = iter, y = value)) +\n  geom_line() +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_bw()\n\np2 &lt;- ggplot(parameter_MCMC, aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~parameter, scales = \"free\") +\n  theme_bw()\n\np1 / p2\n\n\n\n\n\n\n\nFigure 19.1: The MCMC chains and posterior distributions for the calibrated parameters",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Applying Bayesian calibration methods to a process model</span>"
    ]
  },
  {
    "objectID": "bayesian-process-models.html#examining-the-influence-of-parameter-optimization-on-model-predictions",
    "href": "bayesian-process-models.html#examining-the-influence-of-parameter-optimization-on-model-predictions",
    "title": "19  Applying Bayesian calibration methods to a process model",
    "section": "19.2 Examining the influence of parameter optimization on model predictions",
    "text": "19.2 Examining the influence of parameter optimization on model predictions\n\n19.2.1 Simulation with prior parameter distributions\n\nens_members &lt;- 100\n\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\n#Set initial conditions\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- state_init[1]\noutput[1, , 2] &lt;- state_init[2]\noutput[1, , 3] &lt;- state_init[3]\n\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(70, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.0, ens_members) #0.01 \nparams$sigma.wood &lt;- rep(0.0, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.0, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\n\n#Replace parameters with prior distribution\nparams$alpha &lt;- rnorm(ens_members, mean = prior_mean[1], sd = prior_sd[1])\nparams$Rbasal &lt;- rnorm(ens_members, mean = prior_mean[2], sd = prior_sd[2])\nparams$litterfall_start &lt;- rnorm(ens_members, mean = prior_mean[3], sd = prior_sd[3])\n\nfor(t in 2:length(sim_dates)){\n  \n  output[t, , ]  &lt;- forest_model(t, \n                                 states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                                 parms = params, \n                                 inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\noutput_df_no_optim &lt;- output_to_df(output, sim_dates, sim_name = \"using prior\")\n\n\n\n19.2.2 Simulation with posterior parameter distributions\n\n#Set initial conditions\noutput &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\noutput[1, , 1] &lt;- state_init[1]\noutput[1, , 2] &lt;- state_init[2]\noutput[1, , 3] &lt;- state_init[3]\n\n# Sample from posterior distributions.  Use the same index for each\nindex &lt;- sample(nburn:num_iter, ens_members, replace = TRUE)\n\nparams$alpha &lt;- fit_params[1, index]\nparams$Rbasal &lt;- fit_params[2, index]\nparams$litterfall_start &lt;- fit_params[3, index]\n\nfor(t in 2:length(sim_dates)){\n  \n  output[t, , ]  &lt;- forest_model(t, \n                                 states = matrix(output[t-1 , , 1:3], nrow = ens_members) , \n                                 parms = params, \n                                 inputs = matrix(inputs_ensemble[t ,, ], nrow = ens_members))\n}\n\noutput_df_optim &lt;- output_to_df(output, sim_dates, sim_name = \"using posteriors\")\n\n\n\n19.2.3 Visualize the influence of optimization\nFigure 19.2 shows a simulation that uses the prior distribution and a simulation that uses the posterior distribution. This highlights the constraint provided by the data on the model predictions.\n\nobs_filtered &lt;- obs |&gt; \n  filter(datetime &gt; min(output_df_no_optim$datetime))\n\n\nbind_rows(output_df_no_optim, output_df_optim) |&gt; \n   summarise(median = median(prediction, na.rm = TRUE), \n            upper90 = quantile(prediction, 0.95, na.rm = TRUE),\n            lower90 = quantile(prediction, 0.05, na.rm = TRUE),\n            .by = c(\"datetime\", \"variable\", \"model_id\")) |&gt; \n  filter(variable %in% c(\"lai\", \"wood\", \"som\", \"nee\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = lower90, ymax = upper90, fill = model_id), alpha = 0.7) +\n  geom_line(aes(y = median, color = model_id)) +\n  geom_point(data = obs_filtered, aes(x = datetime, y = observation), color = \"blue\", alpha = 0.5) +\n  facet_wrap(~variable, scale = \"free\", ncol = 1) +\n  theme_bw()\n\n\n\n\n\n\n\nFigure 19.2: Predictions using the priors and posterior distribution of parameters\n\n\n\n\n\n\n\n19.2.4 Save posteriors for future use\nThe calibrated parameter chains will be used in later chapters.\n\n  write_csv(parameter_MCMC, \"data/saved_parameter_chain.csv\")\n\n\n\n19.2.5",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Applying Bayesian calibration methods to a process model</span>"
    ]
  },
  {
    "objectID": "particle-filter-process-model.html",
    "href": "particle-filter-process-model.html",
    "title": "20  Applying to data assimilation to process model",
    "section": "",
    "text": "20.1 Prep data\nGet the historical weather for the site\ninputs &lt;- get_historical_met(site = site, sim_dates, use_mean = FALSE)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\nSet the constant parameters (we will fit two parameters using the particle filter below)\nparams &lt;- list()\nparams$alpha &lt;- rep(0.02, ens_members)\nparams$SLA &lt;- rep(4.74, ens_members)\nparams$leaf_frac &lt;- rep(0.315, ens_members)\nparams$Ra_frac &lt;- rep(0.5, ens_members)\nparams$Rbasal &lt;- rep(0.002, ens_members)\nparams$Q10 &lt;- rep(2.1, ens_members)\nparams$litterfall_rate &lt;- rep(1/(2.0*365), ens_members) #Two year leaf lifespan\nparams$litterfall_start &lt;- rep(200, ens_members)\nparams$litterfall_length&lt;- rep(70, ens_members)\nparams$mortality &lt;- rep(0.00015, ens_members) #Wood lives about 18 years on average (all trees, branches, roots, course roots)\nparams$sigma.leaf &lt;- rep(0.1, ens_members) #0.01 \nparams$sigma.wood &lt;- rep(0.1, ens_members) #0.01 ## wood biomass\nparams$sigma.soil &lt;- rep(0.1, ens_members)# 0.01\nparams &lt;- as.data.frame(params)\nRead in the observational data and use the first data in the simulation period as the initial condition.\nobs &lt;- read_csv(\"data/site_carbon_data.csv\", show_col_types = FALSE)\nstate_init &lt;- rep(NA, 3)\n\nstate_init[1] &lt;- obs |&gt; \n  filter(variable == \"lai\",\n         datetime %in% sim_dates) |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  mutate(observation = observation / (mean(params$SLA) * 0.1)) |&gt; \n  pull(observation)\n\nstate_init[2] &lt;- obs |&gt; \n  filter(variable == \"wood\",\n         datetime %in% sim_dates) |&gt; \n  na.omit() |&gt; \n  slice(1) |&gt; \n  pull(observation)\n\nstate_init[3] &lt;- obs |&gt; \n  filter(variable == \"som\") |&gt; \n  na.omit() |&gt;  \n  slice(1) |&gt; \n  pull(observation)\nAssign the initial conditions to the first time-step of the simulation\n#Set initial conditions\nforecast &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\nforecast[1, , 1] &lt;- state_init[1]\nforecast[1, , 2] &lt;- state_init[2]\nforecast[1, , 3] &lt;- state_init[3]\n\nwt &lt;- array(1, dim = c(length(sim_dates), ens_members))\nRead in the parameter chain from Chapter 19. We sample from the chain to get the initial starting distribution for the particle filter. Importantly, the parameter has to be sampled together to preserve the correlation between them. In our example, assume the litterfall_start is constant at the calibrated mean.\nfit_params_table &lt;- read_csv(\"data/saved_parameter_chain.csv\", show_col_types = FALSE) |&gt; \n  pivot_wider(names_from = parameter, values_from = value)\n\nnum_pars &lt;- 2\nfit_params &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nsamples &lt;- sample(1:nrow(fit_params_table), size = ens_members, replace = TRUE)\nfit_params[1, , 1] &lt;- fit_params_table$alpha[samples]\nfit_params[1, , 2] &lt;- fit_params_table$Rbasal[samples]\nparams$litterfall_start &lt;- mean(fit_params_table$litterfall_start, na.rm = TRUE)",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Applying to data assimilation to process model</span>"
    ]
  },
  {
    "objectID": "particle-filter-process-model.html#run-the-particle-filter",
    "href": "particle-filter-process-model.html#run-the-particle-filter",
    "title": "20  Applying to data assimilation to process model",
    "section": "20.2 Run the particle filter",
    "text": "20.2 Run the particle filter\nIn the particle filter, we perturb the parameter values slightly to prevent them from collapsing to a single value.\n\nfor(t in 2:length(sim_dates)){\n  \n  fit_params[t, , 1] &lt;- rnorm(ens_members, fit_params[t-1, ,1], sd = 0.0005)\n  fit_params[t, , 2] &lt;- rnorm(ens_members, fit_params[t-1, ,2], sd = 0.00005)\n  \n  params$alpha  &lt;- fit_params[t, , 1]\n  params$Rbasal  &lt;- fit_params[t, , 2]\n  \n  forecast[t, , ]  &lt;- forest_model(t, \n                                   states = matrix(forecast[t-1 , , 1:3], nrow = ens_members) , \n                                   parms = params, \n                                   inputs = matrix(inputs_ensemble[t , , ], nrow = ens_members))\n  \n  curr_obs &lt;- obs |&gt; \n    filter(datetime == sim_dates[t],\n           variable %in% c(\"nee\",\"lai\",\"wood\"))\n  \n  if(nrow(curr_obs) &gt; 0){\n    \n    forecast_df &lt;- output_to_df(forecast[t, , ], sim_dates[t], sim_name = \"particle_filter\")\n    \n    \n    combined_output_obs &lt;- combine_model_obs(forecast_df, \n                                             obs = curr_obs,\n                                             variables = c(\"lai\", \"wood\", \"som\", \"nee\"), \n                                             sds = c(0.1, 1, 20, 0.005))\n    \n    likelihood &lt;- rep(0, ens_members)\n    for(ens in 1:ens_members){\n      \n      curr_ens &lt;- combined_output_obs |&gt; \n        filter(ensemble == ens)\n      \n      likelihood[ens] &lt;- exp(sum(dnorm(x =  curr_ens$observation, \n                                       mean = curr_ens$prediction, \n                                       sd = curr_ens$sds, log = TRUE)))\n    }\n    \n    wt[t, ] &lt;- likelihood * wt[t-1, ]\n    \n    wt_norm &lt;-  wt[t, ]/sum(wt[t, ])\n    Neff = 1/sum(wt_norm^2)\n    \n    if(Neff &lt; ens_members/2){\n      ## resample ensemble members in proportion to their weight\n      resample_index &lt;- sample(1:ens_members, ens_members, replace = TRUE, prob = wt_norm ) \n      forecast[t, , ] &lt;- as.matrix(forecast[t, resample_index, 1:12])  ## update state\n      fit_params[t, , ] &lt;- as.matrix(fit_params[t, resample_index, ])\n      wt[t, ] &lt;- rep(1, ens_members)\n    }\n  }\n}\n\nProcess the output and parameters into a data frame using the particle weights.\n\nforecast_weighted &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12))\nparams_weighted &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nfor(t in 1:length(sim_dates)){\n  wt_norm &lt;-  wt[t, ]/sum(wt[t, ])\n  resample_index &lt;- sample(1:ens_members, ens_members, replace = TRUE, prob = wt_norm ) \n  forecast_weighted[t, , ] &lt;- forecast[t, resample_index, 1:12] \n  params_weighted[t, , ] &lt;- fit_params[t,resample_index, ] \n}\noutput_df &lt;- output_to_df(forecast_weighted, sim_dates, sim_name = \"particle_filter\")\nparameter_df &lt;- parameters_to_df(params_weighted, sim_dates, sim_name = \"particle_filter\", param_names = c(\"alpha\",\"Rbasal\"))\n\nVisualize the simulation using the particle filter\n\nobs_dates &lt;- obs |&gt; \n  filter(variable == c(\"nee\",\"lai\", \"wood\")) |&gt; \n  pull(datetime)\n\noutput_df |&gt; \n  summarise(median = median(prediction),\n            upper = quantile(prediction, 0.95, na.rm = TRUE),\n            lower = quantile(prediction, 0.05, na.rm = TRUE), .by = c(\"datetime\", \"variable\")) |&gt; \n  left_join(obs, by = c(\"datetime\", \"variable\")) |&gt; \n  filter(variable %in% c(\"nee\",\"wood\",\"som\",\"lai\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = upper, ymax = lower), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  geom_point(aes(y = observation), color = \"red\") +\n  facet_wrap(~variable, scale = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nVisualize how the parameters changed over the simulation. You can see the distributions widening between observations and narrowing when an observation is assimilated.\n\nparameter_df |&gt; \n  summarise(median = median(prediction),\n            upper = quantile(prediction, 0.95, na.rm = TRUE),\n            lower = quantile(prediction, 0.05, na.rm = TRUE), .by = c(\"datetime\", \"variable\")) |&gt; \n  left_join(obs, by = c(\"datetime\", \"variable\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = upper, ymax = lower), alpha = 0.7) +\n  geom_line(aes(y = median)) +\n  facet_wrap(~variable, scale = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nCombine the forecast, parameters, weights, simulation dates, and constant parameters into an object called analysis and save for use in Chapter 21\n\nanalysis &lt;- list(forecast = forecast, fit_params = fit_params, wt = wt, sim_dates = sim_dates, params = params)\nsave(analysis, file = \"data/PF_analysis_0.Rdata\")",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Applying to data assimilation to process model</span>"
    ]
  },
  {
    "objectID": "process-model-forecast.html",
    "href": "process-model-forecast.html",
    "title": "21  Forecast - analysis cycle",
    "section": "",
    "text": "21.1 Step 1: Set initial conditions and parameter distributions\nWe will start our forecast-analysis cycle by running the model with a particle filter over some time in the past. This is designed to spin up the model and set the initial states and parameters for a forecast.\nNow we are going to generate a forecast that started from our last forecast that was saved. First, load the forecast into memory so that we can access the initial conditions and parameters in it.\nif(file.exists(\"data/PF_analysis_1.Rdata\")){\n  load(\"data/PF_analysis_1.Rdata\")\n}else{\n  load(\"data/PF_analysis_0.Rdata\")\n}\nens_members &lt;- dim(analysis$forecast)[2]",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Forecast - analysis cycle</span>"
    ]
  },
  {
    "objectID": "process-model-forecast.html#step-2-get-latest-data",
    "href": "process-model-forecast.html#step-2-get-latest-data",
    "title": "21  Forecast - analysis cycle",
    "section": "21.2 Step 2: Get latest data",
    "text": "21.2 Step 2: Get latest data\nThe latest observation data is available by running Chapter 18 and reading in the CSV. You will need to re-run the code in Chapter 18 if you want to update the observations outside the automatic build of this book.\n\nobs &lt;- read_csv(\"data/site_carbon_data.csv\", show_col_types = FALSE)",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Forecast - analysis cycle</span>"
    ]
  },
  {
    "objectID": "process-model-forecast.html#step-3-generate-forecast",
    "href": "process-model-forecast.html#step-3-generate-forecast",
    "title": "21  Forecast - analysis cycle",
    "section": "21.3 Step 3: Generate forecast",
    "text": "21.3 Step 3: Generate forecast\nThis example introduces the concept of a “look back”. This is stepping back in the past to restart the particle filter so that it can assimilate any data that has been made available since the last time that period was run through the particle filter. The look back is important because many observations have delays before becoming available (called latency). NEON flux data can have a 5-day to 1.5-month latency. MODIS LAI is an 8-day average so has an 8-day latency.\nHere we use a look back of 30 days. Since our forecast horizon is 30 days in the future, the total days of the simulation are 30 + 30. Our reference_datetime is today.\n\nsite &lt;- \"OSBS\"\nlook_back &lt;- 30\nhorizon &lt;- 30\nreference_datetime &lt;- Sys.Date()\nsim_dates &lt;- seq(reference_datetime - look_back, length.out = look_back + horizon, by = \"1 day\")\n\nSince yesterday’s forecast also had a 30-day look-back, we can find the first day of our simulation (30 days ago) in the last saved analysis and use this for initial conditions.\n\nindex &lt;- which(analysis$sim_dates == sim_dates[1])\n\nThe use of the look-back requires combining the “historical” weather and future weather into a single input data frame.\n\ninputs_past &lt;- get_historical_met(site = site, sim_dates[1:(look_back-2)], use_mean = FALSE)\ninputs_future &lt;- get_forecast_met(site = site, sim_dates[(look_back-1):length(sim_dates)], use_mean = FALSE)\ninputs &lt;- bind_rows(inputs_past, inputs_future)\ninputs_ensemble &lt;- assign_met_ensembles(inputs, ens_members)\n\nThe combined weather drivers with the concept of the look back are shown in the figure below. The vertical line is where the look-back period transitions to the future.\n\n\n\n\n\nNOAA Global Ensemble Forecasting System 31 ensemble members for the two drivers in the forest model. The look-back and forecast periods are included with the transition denoted with a vertical line.\n\n\n\n\nThe object with yesterday’s forecast also has the parameter values. Load the fixed parameter values and initialize the parameters that are being fit by the particle filter with the values from the analysis.\n\nparams &lt;- analysis$params\n\nnum_pars &lt;- 2\nfit_params &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nfit_params[1, , 1] &lt;- analysis$fit_params[index, ,1]\nfit_params[1, , 2] &lt;- analysis$fit_params[index, ,2]\n\nInitialize the states with the states from the analysis that occurred on the index simulation date in the previous run through the particle filter.\n\n#Set initial conditions\nforecast &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12)) #12 is the number of outputs\nforecast[1, , 1] &lt;- analysis$forecast[index, ,1]\nforecast[1, , 2] &lt;- analysis$forecast[index, ,2]\nforecast[1, , 3] &lt;- analysis$forecast[index, ,3]\n\nwt &lt;- array(1, dim = c(length(sim_dates), ens_members))\nwt[1, ] &lt;- analysis$wt[index, ]\n\nRun the particle filter (over the look-back days) that transitions to a forecast (which is the same as the particle filter without data)\n\nfor(t in 2:length(sim_dates)){\n  \n  fit_params[t, , 1] &lt;- rnorm(ens_members, fit_params[t-1, ,1], sd = 0.0005)\n  fit_params[t, , 2] &lt;- rnorm(ens_members, fit_params[t-1, ,2], sd = 0.00005)\n  \n  params$alpha  &lt;- fit_params[t, , 1]\n  params$Rbasal  &lt;- fit_params[t, , 2]\n  \n  if(t &gt; 1){\n  \n  forecast[t, , ]  &lt;- forest_model(t, \n                                   states = matrix(forecast[t-1 , , 1:3], nrow = ens_members) , \n                                   parms = params, \n                                   inputs = matrix(inputs_ensemble[t , , ], nrow = ens_members))\n  }\n  \n  analysis &lt;- particle_filter(t, forecast, obs, sim_dates, wt, fit_params,\n                              variables = c(\"lai\", \"wood\", \"som\", \"nee\"), \n                              sds = c(0.1, 1, 20, 0.005))\n  \n  forecast &lt;- analysis$forecast\n  fit_params &lt;- analysis$fit_params\n  wt &lt;- analysis$wt\n}",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Forecast - analysis cycle</span>"
    ]
  },
  {
    "objectID": "process-model-forecast.html#step-4-save-forecast-and-data-assimilation-output",
    "href": "process-model-forecast.html#step-4-save-forecast-and-data-assimilation-output",
    "title": "21  Forecast - analysis cycle",
    "section": "21.4 Step 4: Save forecast and data assimilation output",
    "text": "21.4 Step 4: Save forecast and data assimilation output\nConvert forecast to a dataframe.\n\noutput_df &lt;- output_to_df(forecast, sim_dates, sim_name = \"simple_forest_model\")\n\n\nforecast_weighted &lt;- array(NA, dim = c(length(sim_dates), ens_members, 12))\nparams_weighted &lt;- array(NA, dim = c(length(sim_dates) ,ens_members , num_pars))\nfor(t in 1:length(sim_dates)){\n  wt_norm &lt;-  wt[t, ]/sum(wt[t, ])\n  resample_index &lt;- sample(1:ens_members, ens_members, replace = TRUE, prob = wt_norm ) \n  forecast_weighted[t, , ] &lt;- forecast[t, resample_index, 1:12] \n  params_weighted[t, , ] &lt;- fit_params[t,resample_index, ] \n}\noutput_df &lt;- output_to_df(forecast_weighted, sim_dates, sim_name = \"simple_forest_model\")\nparameter_df &lt;- parameters_to_df(params_weighted, sim_dates, sim_name = \"simple_forest_model\", param_names = c(\"alpha\",\"Rbasal\"))\n\nFigure 21.1 is a visualization of the forecast. The vertical line is where the look-back period transitions to the future.\n\n\n\n\n\n\n\n\nFigure 21.1: Forecast median and 90% CI. The look-back and forecast periods are included with the transition denoted with a vertical line.\n\n\n\n\n\nSave the states and weights for use as initial conditions in the next forecast.\n\nanalysis$params &lt;- params\nsave(analysis, file = \"data/PF_analysis_1.Rdata\")\n\nConvert to the format and units required by the NEON Ecological Forecasting Challenge\n\nefi_output &lt;- output_df |&gt; \n  mutate(datetime = as_datetime(datetime),\n         model_id = \"bookcast_forest\",\n         duration = \"P1D\",\n         project_id = \"neon4cast\",\n         site_id = site,\n         family = \"ensemble\",\n         reference_datetime = as_datetime(reference_datetime)) |&gt; \n  rename(parameter = ensemble) |&gt; \n  filter(datetime &gt;= reference_datetime) |&gt; \n  filter(variable == \"nee\") |&gt; \n  mutate(prediction = prediction  / 0.01) #Convert from MgC/ha/Day to gC/m2/day\n\nWrite the forecast to a CSV file\n\nfile_name &lt;- paste0(\"data/terrestrial_daily-\", reference_datetime, \"-bookcast_forest.csv\")\nwrite_csv(efi_output, file_name)\n\nand submit to the Challenge. The submission uses the submit function in the neon4cast package(remotes::install_github(\"eco4cast/neon4cast\"))\n\nneon4cast::submit(file_name, ask = FALSE)\n\nvalidating that file matches required standard\n\n\ndata/terrestrial_daily-2024-09-26-bookcast_forest.csv\n\n\n✔ file has model_id column\n✔ forecasted variables found correct variable + prediction column\n✔ nee is a valid variable name\n✔ file has correct family and parameter columns\n✔ file has site_id column\n✔ file has datetime column\n✔ file has correct datetime column\n✔ file has correct duration column\n✔ file has project_id column\n✔ file has reference_datetime column\nForecast format is valid\n\nThank you for submitting!",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Forecast - analysis cycle</span>"
    ]
  },
  {
    "objectID": "process-model-forecast.html#step-5-repeat-steps-2---4",
    "href": "process-model-forecast.html#step-5-repeat-steps-2---4",
    "title": "21  Forecast - analysis cycle",
    "section": "21.5 Step 5: Repeat Steps 2 - 4",
    "text": "21.5 Step 5: Repeat Steps 2 - 4\nWait for a day to pass and then use yesterday’s analysis today for initial conditions and parameter distributions. This particular forecast uses GitHub actions to automatically render the book each day and rendering this chapter results in generating, saving, and submitting the latest forecast.\n\nload(\"data/PF_analysis_1.Rdata\")",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Forecast - analysis cycle</span>"
    ]
  },
  {
    "objectID": "process-model-forecast.html#additional-example",
    "href": "process-model-forecast.html#additional-example",
    "title": "21  Forecast - analysis cycle",
    "section": "21.6 Additional example",
    "text": "21.6 Additional example\nI recommend looking at a tutorial created by Mike Dietze that has many of the particle filter concepts used here. It is a similar process model that runs at the 30-minute time-step. The tutorial is more advanced in its use of priors on the parameters and shows how to forecast across multiple sites.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Forecast - analysis cycle</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html",
    "href": "process-model-forecast-evaluation.html",
    "title": "22  Analyzing forecasts",
    "section": "",
    "text": "22.1 Analyzing submitted forecasts\nThe code to download all forecasts generated by the model used in this book is:\nmy_results &lt;- arrow::open_dataset(\"s3://anonymous@bio230014-bucket01/challenges/scores/bundled-parquet/project_id=neon4cast/duration=P1D/variable=nee/model_id=bookcast_forest?endpoint_override=sdsc.osn.xsede.org\")\ndf &lt;- my_results |&gt; \n  filter(site_id == \"OSBS\",\n         reference_datetime &gt; as_date(\"2024-03-15\")) |&gt; \n  collect()\nThe code above can be found in the catalog link at the top of the page.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#aggregated-scores",
    "href": "process-model-forecast-evaluation.html#aggregated-scores",
    "title": "22  Analyzing forecasts",
    "section": "22.2 Aggregated scores",
    "text": "22.2 Aggregated scores\nWe can look at the mean score for the process model but this provides very little context for the quality of forecast. It is more informative to compare the score to the score from other models.\n\ndf |&gt; \n  summarise(mean_crps = mean(crps, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_crps\n      &lt;dbl&gt;\n1     0.956",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#comparing-to-baselines",
    "href": "process-model-forecast-evaluation.html#comparing-to-baselines",
    "title": "22  Analyzing forecasts",
    "section": "22.3 Comparing to baselines",
    "text": "22.3 Comparing to baselines\nWe will benchmark our process model forecast against to two “naive” baselines of climatology and persistenceRW.\n\nall_results &lt;- arrow::open_dataset(\"s3://anonymous@bio230014-bucket01/challenges/scores/bundled-parquet/project_id=neon4cast/duration=P1D/variable=nee?endpoint_override=sdsc.osn.xsede.org\")\ndf_with_baselines &lt;- all_results |&gt; \n  filter(site_id == \"OSBS\",\n         reference_datetime &gt; as_date(\"2024-03-15\"), \n         model_id %in% c(\"bookcast_forest\", \"climatology\", \"persistenceRW\")) |&gt; \n  collect()",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#visualization",
    "href": "process-model-forecast-evaluation.html#visualization",
    "title": "22  Analyzing forecasts",
    "section": "22.4 Visualization",
    "text": "22.4 Visualization\nHow do the forecasts look for a single reference_datetime\n\ndf_with_baselines |&gt; \n  filter(as_date(reference_datetime) == as_date(\"2024-04-01\")) |&gt; \n  ggplot(aes(x = datetime)) +\n  geom_ribbon(aes(ymin = quantile02.5, ymax = quantile97.5, fill = model_id), alpha = 0.3) +\n  geom_line(aes(y = median, color = model_id)) +\n  geom_point(aes(y = observation)) +\n  labs(y = \"forecast\") +\n  theme_bw()\n\nWarning: Removed 490 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#aggregated-scores-1",
    "href": "process-model-forecast-evaluation.html#aggregated-scores-1",
    "title": "22  Analyzing forecasts",
    "section": "22.5 Aggregated scores",
    "text": "22.5 Aggregated scores\nWe can first look at the aggregated scores (all reference_datetime and datetime combinations). Importantly, the code below uses pivot_wider and pivot_longer to ensure we only include datetime values where all three models provided forecasts. Otherwise, there would be different periods from the three models in the aggregated score.\n\ndf_with_baselines |&gt; \n  select(model_id, crps, datetime, reference_datetime) |&gt; \n  pivot_wider(names_from = model_id, values_from = crps) |&gt; \n  na.omit() |&gt; \n  pivot_longer(-c(datetime, reference_datetime), names_to = \"model_id\", values_to = \"crps\") |&gt; \n  summarise(mean_crps = mean(crps), .by = c(\"model_id\")) |&gt; \n  ggplot(aes(x = model_id, y = mean_crps)) +\n  geom_bar(stat=\"identity\")\n\nWarning: Values from `crps` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(datetime, reference_datetime,\n  model_id)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nWarning: There were 3 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `mean_crps = mean(crps)`.\nℹ In group 1: `model_id = \"bookcast_forest\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.\n\n\nWarning: Position guide is perpendicular to the intended axis.\nℹ Did you mean to specify a different guide `position`?\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_bar()`).",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#by-horizon",
    "href": "process-model-forecast-evaluation.html#by-horizon",
    "title": "22  Analyzing forecasts",
    "section": "22.6 By horizon",
    "text": "22.6 By horizon\nHow does forecast performance change as forecasts extend farther in the future (increasing horizon), regardless of when the forecast was produced?\n\ndf_with_baselines |&gt; \n  mutate(horizon = as.numeric(datetime - reference_datetime) / 86400) |&gt; \n  select(model_id, horizon, datetime, reference_datetime, crps) |&gt; \n  pivot_wider(names_from = model_id, values_from = crps) |&gt; \n  na.omit() |&gt; \n  pivot_longer(-c(horizon, datetime, reference_datetime), names_to = \"model_id\", values_to = \"crps\") |&gt; \n  summarize(mean_crps = mean(crps), .by = c(\"model_id\", \"horizon\")) |&gt; \n  ggplot(aes(x = horizon, y = mean_crps, color = model_id)) + \n  geom_line()\n\nWarning: Values from `crps` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(horizon, datetime,\n  reference_datetime, model_id)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nWarning: There were 111 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `mean_crps = mean(crps)`.\nℹ In group 1: `model_id = \"bookcast_forest\"` and `horizon = 29`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 110 remaining warnings.\n\n\nWarning: Removed 111 rows containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#by-reference-datetime",
    "href": "process-model-forecast-evaluation.html#by-reference-datetime",
    "title": "22  Analyzing forecasts",
    "section": "22.7 By reference datetime",
    "text": "22.7 By reference datetime\nHow does forecast performance vary across the dates that the forecasts are generated, regardless of horizon?\n\ndf_with_baselines |&gt; \n  select(model_id, datetime, reference_datetime, crps) |&gt; \n  pivot_wider(names_from = model_id, values_from = crps) |&gt; \n  na.omit() |&gt; \n  pivot_longer(-c(datetime, reference_datetime), names_to = \"model_id\", values_to = \"crps\") |&gt; \n  summarize(mean_crps = mean(crps), .by = c(\"model_id\", \"reference_datetime\")) |&gt; \n  ggplot(aes(x = reference_datetime, y = mean_crps, color = model_id)) + \n  geom_line()\n\nWarning: Values from `crps` are not uniquely identified; output will contain list-cols.\n• Use `values_fn = list` to suppress this warning.\n• Use `values_fn = {summary_fun}` to summarise duplicates.\n• Use the following dplyr code to identify duplicates.\n  {data} |&gt;\n  dplyr::summarise(n = dplyr::n(), .by = c(datetime, reference_datetime,\n  model_id)) |&gt;\n  dplyr::filter(n &gt; 1L)\n\n\nWarning: There were 552 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `mean_crps = mean(crps)`.\nℹ In group 1: `model_id = \"bookcast_forest\"` and `reference_datetime =\n  2024-03-20`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 551 remaining warnings.\n\n\nWarning: Removed 552 rows containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#additional-comparisons",
    "href": "process-model-forecast-evaluation.html#additional-comparisons",
    "title": "22  Analyzing forecasts",
    "section": "22.8 Additional comparisons",
    "text": "22.8 Additional comparisons\nForecasts can be compared across site_id (aggregating across all reference_datetime and horizon) if there are multiple sites and datetime (aggregating across all horizon). Since CRPS is in the naive units of the variable, it can not be compared across variables.",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "process-model-forecast-evaluation.html#reading",
    "href": "process-model-forecast-evaluation.html#reading",
    "title": "22  Analyzing forecasts",
    "section": "22.9 Reading",
    "text": "22.9 Reading\nLewis, A. S. L., Woelmer, W. M., Wander, H. L., Howard, D. W., Smith, J. W., McClure, R. P., et al. (2022). Increased adoption of best practices in ecological forecasting enables comparisons of forecastability. Ecological Applications, 32(2), e02500. https://doi.org/10.1002/eap.2500",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Analyzing forecasts</span>"
    ]
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "23  Build your process-based forecast",
    "section": "",
    "text": "Project 2 challenges you to\n\nEither modify the process model to better represent an ecosystem process or include a new process that is missing, or\nApply the process model to a new NEON site.\n\nYou are asked to provide a kitted R markdown (or Quarto) document with text, code, and plots following:\n\nYour forest process model\nParameter estimation either using likelihood, Bayesian batch, or particle filter methods\nA 30-day ahead forecast using your model",
    "crumbs": [
      "Iterative forecasting using process-models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Build your process-based forecast</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dietze, Michael C. 2017. Ecological Forecasting.\nPrinceton: Princeton University Press.\n\n\nGneiting, Tilmann, and Adrian E Raftery. 2007. “Strictly\nProper Scoring Rules, Prediction, and\nEstimation.” Journal of the American Statistical\nAssociation 102 (477): 359–78. https://doi.org/10.1198/016214506000001437.\n\n\nHarris, David J., Shawn D. Taylor, and Ethan P. White. 2018.\n“Forecasting Biodiversity in Breeding Birds Using Best\nPractices.” PeerJ 6. https://doi.org/10.7717/peerj.4278.\n\n\nHobday, Alistair J., Jason R. Hartog, John P. Manderson, Katherine E.\nMills, Matthew J. Oliver, Andrew J. Pershing, and Samantha Siedlecki.\n2019. “Ethical Considerations and Unanticipated Consequences\nAssociated with Ecological Forecasting for Marine Resources.”\nICES Journal of Marine Science. https://doi.org/10.1093/icesjms/fsy210.\n\n\nLewis, Abigail S. L., Whitney M. Woelmer, Heather L. Wander, Dexter W.\nHoward, John W. Smith, Ryan P. McClure, Mary E. Lofton, et al. 2022.\n“Increased Adoption of Best Practices in Ecological Forecasting\nEnables Comparisons of Forecastability.” Ecological\nApplications 32 (2): e02500. https://doi.org/10.1002/eap.2500.\n\n\nMoore, Tadhg N., R. Quinn Thomas, Whitney M. Woelmer, and Cayelan C.\nCarey. 2022. “Integrating Ecological Forecasting into\nUndergraduate Ecology Curricula with an R Shiny\nApplication-Based Teaching Module.” Forecasting 4\n(3): 604–33. https://doi.org/10.3390/forecast4030033.\n\n\nThomas, R Quinn, Carl Boettiger, Cayelan C Carey, Michael C Dietze, Leah\nR Johnson, Melissa A Kenney, Jason S McLachlan, et al. 2023. “The\nNEON Ecological Forecasting Challenge.”\nFrontiers in Ecology and the Environment 21 (3): 112–13. https://doi.org/10.1002/fee.2616.\n\n\nWoelmer, Whitney M., Tadhg N. Moore, Mary E. Lofton, R. Quinn Thomas,\nand Cayelan C. Carey. 2023. “Embedding Communication Concepts in\nForecasting Training Increases Students’ Understanding of Ecological\nUncertainty.” Ecosphere 14 (8): e4628. https://doi.org/10.1002/ecs2.4628.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "weather-drivers.html",
    "href": "weather-drivers.html",
    "title": "24  Accessing weather drivers",
    "section": "",
    "text": "24.1 “Historical weather”\nOur goal is to generate genuine forecasts of the future. Therefore we need to use weather forecasts as driver inputs into our model.\nThis opens a critical issue when calibrating a model that is then used for forecasting. A model could be calibrated using observed weather from a weather station as inputs. As a result, the parameters are tuned to the weather from the station. However, the bias and statistics of the observed weather may be different from the forecasted weather since weather forecasts are for a larger grid (e.g., the weather forecast may be consistently warmer by 2 degrees C). This offset in the meteorology used to calibrate the forecast could result in the forecast being biased. The issue can be fixed by either adjusting the weather forecast to be more consistent with the observed weather or calibrating the model using weather inputs from the weather forecast model. We use the latter in this book.\nThe get_historical_met function gets the first day of all the weather forecasts generated since September 2020. The first day of each forecast is the closest to what was observed because it directly follows data assimilation. Combining the first days together gives a complete time series of weather until the present day.\nThe function is printed out here so that you can modify it if you want to add variables or process differently.\nget_historical_met &lt;- function(site, sim_dates, use_mean = TRUE){\n\n  if(use_mean){\n    groups &lt;- c(\"datetime\", \"variable\")\n  }else{\n    groups &lt;- c(\"datetime\", \"variable\",\"parameter\")\n  }\n  site &lt;- \"TALL\"\n  met_s3 &lt;- arrow::s3_bucket(paste0(\"bio230014-bucket01/neon4cast-drivers/noaa/gefs-v12/stage3/site_id=\", site),\n                             endpoint_override = \"sdsc.osn.xsede.org\",\n                             anonymous = TRUE)\n\n  inputs_all &lt;- arrow::open_dataset(met_s3) |&gt;\n    filter(variable %in% c(\"air_temperature\", \"surface_downwelling_shortwave_flux_in_air\")) |&gt;\n    mutate(datetime = as_date(datetime)) |&gt;\n    mutate(prediction = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", prediction/0.486, prediction),\n           variable = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", \"PAR\", variable),\n           prediction = ifelse(variable == \"air_temperature\", prediction - 273.15, prediction),\n           variable = ifelse(variable == \"air_temperature\", \"temp\", variable)) |&gt;\n    summarise(prediction = mean(prediction, na.rm = TRUE), .by =  all_of(groups)) |&gt;\n    mutate(doy = yday(datetime)) |&gt;\n    filter(datetime %in% sim_dates) |&gt;\n    collect()\n\n  if(use_mean){\n    inputs_all &lt;- inputs_all |&gt;\n      mutate(parameter = \"mean\")\n  }\n\n  return(inputs_all)\n}",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Accessing weather drivers</span>"
    ]
  },
  {
    "objectID": "weather-drivers.html#forecasts",
    "href": "weather-drivers.html#forecasts",
    "title": "24  Accessing weather drivers",
    "section": "24.2 Forecasts",
    "text": "24.2 Forecasts\nAn individual forecast can be accessed using the get_forecast_met function. This provides the full horizon (35 days ahead) for a forecast that was generated on a specific day (reference_datetime).\nThe function is printed out here so that you can modify it if you want to add variables or process differently.\n\nget_forecast_met &lt;- function(site, sim_dates, use_mean = TRUE){\n\n  if(use_mean){\n    groups &lt;- c(\"datetime\", \"variable\")\n  }else{\n    groups &lt;- c(\"datetime\", \"variable\",\"parameter\")\n  }\n\n  met_s3 &lt;- arrow::s3_bucket(paste0(\"bio230014-bucket01/neon4cast-drivers/noaa/gefs-v12/stage2/reference_datetime=\",sim_dates[1],\"/site_id=\",site),\n                             endpoint_override = \"sdsc.osn.xsede.org\",\n                             anonymous = TRUE)\n\n  inputs_all &lt;- arrow::open_dataset(met_s3) |&gt;\n    filter(variable %in% c(\"air_temperature\", \"surface_downwelling_shortwave_flux_in_air\")) |&gt;\n    mutate(datetime = as_date(datetime)) |&gt;\n    mutate(prediction = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", prediction/0.486, prediction),\n           variable = ifelse(variable == \"surface_downwelling_shortwave_flux_in_air\", \"PAR\", variable),\n           prediction = ifelse(variable == \"air_temperature\", prediction- 273.15, prediction),\n           variable = ifelse(variable == \"air_temperature\", \"temp\", variable)) |&gt;\n    summarise(prediction = mean(prediction, na.rm = TRUE), .by = all_of(groups)) |&gt;\n    mutate(doy = yday(datetime)) |&gt;\n    filter(datetime %in% sim_dates) |&gt;\n    collect()\n\n  if(use_mean){\n    inputs_all &lt;- inputs_all |&gt;\n      mutate(parameter = \"mean\")\n  }\n\n  return(inputs_all)\n}",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Accessing weather drivers</span>"
    ]
  },
  {
    "objectID": "example-schedule.html",
    "href": "example-schedule.html",
    "title": "25  Example semester course schedule",
    "section": "",
    "text": "Below is an example schedule for a semester (15-week) graduate-level university course. It is based having two 1 hour and 15 minute periods per week.\n\n\n\nClass #\nChapter\n\n\n\n\n1\nChapter 1, Chapter 2\n\n\n2\nChapter 2\n\n\n3\nChapter 8\n\n\n4\nChapter 8\n\n\n5\nChapter 3\n\n\n6\nChapter 3\n\n\n7\nChapter 9\n\n\n8\nChapter 4\n\n\n9\nChapter 4\n\n\n10\nChapter 10\n\n\n11\nChapter 12\n\n\n12\nChapter 12\n\n\n13\nChapter 12\n\n\n14\nChapter 12\n\n\nSpring Break\n\n\n\nSpring Break\n\n\n\n16\nChapter 5\n\n\n17\nChapter 22\n\n\n18\nChapter 16, Chapter 17\n\n\n19\nChapter 13\n\n\n20\nChapter 13\n\n\n21\nChapter 13\n\n\n22\nChapter 14\n\n\n23\nChapter 14\n\n\n24\nChapter 19\n\n\n25\nChapter 15\n\n\n26\nChapter 15\n\n\n27\nChapter 20\n\n\n28\nChapter 21\n\n\n29\nChapter 23\n\n\n30\nChapter 23",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Example semester course schedule</span>"
    ]
  },
  {
    "objectID": "nimble.html",
    "href": "nimble.html",
    "title": "26  Implimenting process model in NIMBLE",
    "section": "",
    "text": "An implimentation of the process model in nimble code along with the application to Bayesian parameter estimation and particle filtering is pending",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Implimenting process model in NIMBLE</span>"
    ]
  },
  {
    "objectID": "section-introduction.html",
    "href": "section-introduction.html",
    "title": "Introduction to forecasting",
    "section": "",
    "text": "The first section of the book is focused on introducing key concepts in ecological forecasting using modules from the Macrosystems EDDIE project The concepts are taught using Rshiny applications and do not require coding.",
    "crumbs": [
      "Introduction to forecasting"
    ]
  },
  {
    "objectID": "section-emprical-forecasting.html",
    "href": "section-emprical-forecasting.html",
    "title": "Iterative forecasting using empirical models",
    "section": "",
    "text": "This section builds on the forecasting introduction by providing a guided, hands-on activity focused on developing simple empirical forecasts of variables in the NEON Ecological Forecasting Challenge. The final chapter of the section challenges you to transfer the concepts learned into a new forecasting model that is submitted to the NEON Challenge.",
    "crumbs": [
      "Iterative forecasting using empirical models"
    ]
  },
  {
    "objectID": "section-advanced.html",
    "href": "section-advanced.html",
    "title": "Advanced forecasting techniques",
    "section": "",
    "text": "The advanced forecasting section introduces techniques for improving forecasting, quantifying, and reducing uncertainty. First, likelihood methods are used to improve the calibration of ecological models. Better calibrations reduce the uncertainty associated with the capacity of the model to reproduce data (process uncertainty). Second, Bayesian methods are used to calibrate parameters and estimate parameter uncertainty in ecological models. Finally, data assimilation with the particle filter is used to estimate initial conditions and reduce initial condition uncertainty.\nThis section focuses on the use of non-linear ecological models, rather than statistic or machine learning models like in the first section. The concepts in this section are applied to a process-based ecological model in the third section of the book.",
    "crumbs": [
      "Advanced forecasting techniques"
    ]
  },
  {
    "objectID": "section-process-models.html",
    "href": "section-process-models.html",
    "title": "Iterative forecasting using process-models",
    "section": "",
    "text": "This section builds on the concepts in the book to demonstrate how to develop forecasts using process-based ecosystem models. First, the model is introduced, the key components of its uncertainty are examined, and data are obtained for calibration. Second, the model is calibrated using a Bayesian approach and initial conditions are determined using a particle filter. Finally, the forecast is iteratively submitted to the NEON Ecological Forecasting Challenge and past forecasts submitted to the challenge are evaluated. It is designed to be a practical guide for similar forecasting applications using other process models. The final chapter provides an example problem set for applying the skills in different contexts.",
    "crumbs": [
      "Iterative forecasting using process-models"
    ]
  }
]